{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"/data/Vivek/original/HypeNET/dataset/custom_train_0.0_0.05.tsv\"\n",
    "test_file =  \"/data/Vivek/original/HypeNET/dataset/custom_test_0.0_0.05.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/db_files/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use_unbracketed.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = [entity_to_id(word2id_db, elem) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def get_instance_key(tup):\n",
    "    return tuple([\" \".join([tok.text for tok in nlp(elem)]) for elem in tup])\n",
    "\n",
    "def parse_instance(tup):\n",
    "    \n",
    "    paths_x = list(instances_db.get(get_instance_key(tup), {}).items())\n",
    "    paths_y = list(instances_db.get(get_instance_key(tup[::-1]), {}).items())\n",
    "    path_count_dict_x = { path.replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { path.replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_instance_dataset(dataset):\n",
    "    parsed_dicts = [parse_instance(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "    closest_entity = resolved.get(entity, \"\")\n",
    "    if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "        success.append(entity)\n",
    "        return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Instance DB\n",
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path, maxlen):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if maxlen >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks, maxlen):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            filtered = [stringifyFilterPath(path, maxlen) for path in appendingElem]\n",
    "            paths.extend(filtered)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "doc = open(\"../files/dataset/security4.txt\").read()\n",
    "all_nounchunks = list(nlp(doc).noun_chunks).copy()\n",
    "\n",
    "sentences = [list(nlp(nlp(sent.text)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \")).sents)[0]\n",
    "             for sent in nlp(doc).sents]\n",
    "# [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "all_deps = []\n",
    "instances_db = {}\n",
    "for sentence in sentences:\n",
    "    noun_chunks = [n for n in all_nounchunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "    noun_chunks = list(nlp(sentence.text).noun_chunks)\n",
    "    dependencies = getDependencyPaths(sentence, nlp, noun_chunks, 10)\n",
    "    for dep in dependencies:\n",
    "        if not dep:\n",
    "            continue\n",
    "        key = tuple([preprocess_word(nlp(word)) for word in dep[:2]])\n",
    "        path = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in dep[-1].split(\"/\")])\n",
    "        if key not in instances_db:\n",
    "            instances_db[key] = [path]\n",
    "        else:\n",
    "            instances_db[key].append(path)\n",
    "instances_db = {key: Counter(instances_db[key]) for key in instances_db}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "0 35498\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "122 35376\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "148 35350\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "579 34919\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "735 34763\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "896 34602\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1048 34450\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1292 34206\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1515 33983\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1698 33800\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1912 33586\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2139 33359\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2299 33199\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2648 32850\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2917 32581\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3186 32312\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3460 32038\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3871 31627\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4142 31356\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4454 31044\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4730 30768\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "5091 30407\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "5462 30036\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "6663 28835\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7047 28451\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7331 28167\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7564 27934\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7882 27616\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8168 27330\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8464 27034\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8854 26644\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9129 26369\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9384 26114\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9633 25865\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10103 25395\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10506 24992\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10581 24917\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10642 24856\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "12648 22850\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# thresholds = [0.5, 0.59, 0.6, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
    "\n",
    "# for threshold in thresholds:\n",
    "threshold = 0.86\n",
    "    \n",
    "failed, success = [], []\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "paths_instances, counts_instances, targets_instances  = parse_instance_dataset(test_instances)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "print (len(failed), len(success))\n",
    "emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "output_file = \"../Input/data_instances_sample.pkl\"\n",
    "f = open(output_file, \"wb+\")\n",
    "pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "             nodes_test, paths_test, counts_test, targets_test,\n",
    "             nodes_instances, paths_instances, counts_instances, targets_instances,\n",
    "             nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "             emb_indexer, emb_indexer_inv, emb_vals, \n",
    "             pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "for i,file in enumerate(sorted(glob.glob(\"../files/dataset/security*\"))):\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    instances_pairs = [\"\\t\".join(list(pair) + [\"none\"]) for pair in instances_pairs if pair]\n",
    "\n",
    "    open(\"../files/dataset/instances\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vulnerabilities\\treal-time threat intelligence\\tnone',\n",
       " 'actionable recommendations\\treal-time threat intelligence\\tnone',\n",
       " 'vulnerabilities\\tKeysight’s Threat Simulator\\tnone',\n",
       " 'vulnerabilities\\tactionable recommendations\\tnone',\n",
       " 'actionable recommendations\\tKeysight’s Threat Simulator\\tnone',\n",
       " 'real-time threat intelligence\\tKeysight’s Threat Simulator\\tnone',\n",
       " 'operational security effectiveness\\tleading technology company\\tnone',\n",
       " 'service providers\\tSecOps platform\\tnone',\n",
       " 'Breach Defense\\tsecurity operations\\tnone',\n",
       " 'Breach Defense\\tInc NYSE\\tnone',\n",
       " 'world\\tKEYS\\tnone',\n",
       " 'governments\\tleading technology company\\tnone',\n",
       " 'leading technology company\\tKEYS\\tnone',\n",
       " 'Keysight Technologies\\tsecurity operations\\tnone',\n",
       " 'service providers\\tKEYS\\tnone',\n",
       " 'operational security effectiveness\\tworld\\tnone',\n",
       " 'service providers\\tleading technology company\\tnone',\n",
       " 'world\\tSecOps platform\\tnone',\n",
       " 'governments\\tSecOps platform\\tnone',\n",
       " 'enterprises\\tleading technology company\\tnone',\n",
       " 'Keysight Technologies\\toperational security effectiveness\\tnone',\n",
       " 'Keysight Technologies\\tinnovation\\tnone',\n",
       " 'leading technology company\\tsecurity operations\\tnone',\n",
       " 'world\\tleading technology company\\tnone',\n",
       " 'governments\\tenterprises\\tnone',\n",
       " 'operational security effectiveness\\tInc NYSE\\tnone',\n",
       " 'Keysight Technologies\\tInc NYSE\\tnone',\n",
       " 'operational security effectiveness\\tsecurity operations\\tnone',\n",
       " 'Breach Defense\\tleading technology company\\tnone',\n",
       " 'enterprises\\tworld\\tnone',\n",
       " 'innovation\\tsecurity operations\\tnone',\n",
       " 'Inc NYSE\\tSecOps platform\\tnone',\n",
       " 'service providers\\tInc NYSE\\tnone',\n",
       " 'operational security effectiveness\\tinnovation\\tnone',\n",
       " 'service providers\\tsecurity operations\\tnone',\n",
       " 'Keysight Technologies\\tenterprises\\tnone',\n",
       " 'Inc NYSE\\tinnovation\\tnone',\n",
       " 'operational security effectiveness\\tSecOps platform\\tnone',\n",
       " 'enterprises\\tInc NYSE\\tnone',\n",
       " 'enterprises\\tsecurity operations\\tnone',\n",
       " 'service providers\\tKeysight Technologies\\tnone',\n",
       " 'governments\\tKEYS\\tnone',\n",
       " 'service providers\\tinnovation\\tnone',\n",
       " 'Breach Defense\\tenterprises\\tnone',\n",
       " 'operational security effectiveness\\tenterprises\\tnone',\n",
       " 'world\\tsecurity operations\\tnone',\n",
       " 'enterprises\\tinnovation\\tnone',\n",
       " 'KEYS\\tInc NYSE\\tnone',\n",
       " 'KEYS\\tsecurity operations\\tnone',\n",
       " 'enterprises\\tSecOps platform\\tnone',\n",
       " 'Breach Defense\\tworld\\tnone',\n",
       " 'service providers\\tBreach Defense\\tnone',\n",
       " 'Keysight Technologies\\tKEYS\\tnone',\n",
       " 'world\\tinnovation\\tnone',\n",
       " 'security operations\\tSecOps platform\\tnone',\n",
       " 'KEYS\\tinnovation\\tnone',\n",
       " 'governments\\tinnovation\\tnone',\n",
       " 'KEYS\\tSecOps platform\\tnone',\n",
       " 'Keysight Technologies\\tBreach Defense\\tnone',\n",
       " 'Keysight Technologies\\tSecOps platform\\tnone',\n",
       " 'Keysight Technologies\\tworld\\tnone',\n",
       " 'governments\\toperational security effectiveness\\tnone',\n",
       " 'innovation\\tSecOps platform\\tnone',\n",
       " 'enterprises\\tKEYS\\tnone',\n",
       " 'Breach Defense\\tKEYS\\tnone',\n",
       " 'world\\tInc NYSE\\tnone',\n",
       " 'Inc NYSE\\tsecurity operations\\tnone',\n",
       " 'leading technology company\\tInc NYSE\\tnone',\n",
       " 'governments\\tsecurity operations\\tnone',\n",
       " 'governments\\tInc NYSE\\tnone',\n",
       " 'Keysight Technologies\\tleading technology company\\tnone',\n",
       " 'governments\\tworld\\tnone',\n",
       " 'service providers\\tgovernments\\tnone',\n",
       " 'service providers\\toperational security effectiveness\\tnone',\n",
       " 'leading technology company\\tSecOps platform\\tnone',\n",
       " 'leading technology company\\tinnovation\\tnone',\n",
       " 'Breach Defense\\tSecOps platform\\tnone',\n",
       " 'service providers\\tenterprises\\tnone',\n",
       " 'Breach Defense\\tinnovation\\tnone',\n",
       " 'Keysight Technologies\\tgovernments\\tnone',\n",
       " 'Breach Defense\\toperational security effectiveness\\tnone',\n",
       " 'operational security effectiveness\\tKEYS\\tnone',\n",
       " 'governments\\tBreach Defense\\tnone',\n",
       " 'service providers\\tworld\\tnone',\n",
       " 'exploits\\toperational security\\tnone',\n",
       " 'effectiveness\\tnetwork and security operations teams\\tnone',\n",
       " 'live networks\\toperational security\\tnone',\n",
       " 'effectiveness\\tlive networks\\tnone',\n",
       " 'network and security operations teams\\tlive networks\\tnone',\n",
       " 'integral element\\tnew platform\\tnone',\n",
       " 'network and security operations teams\\tintegral element\\tnone',\n",
       " 'effectiveness\\tnew platform\\tnone',\n",
       " 'network and security operations teams\\tnew platform\\tnone',\n",
       " 'latest attacks\\tintegral element\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\tlatest attacks\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\tnew platform\\tnone',\n",
       " 'effectiveness\\toperational security\\tnone',\n",
       " 'network and security operations teams\\toperational security\\tnone',\n",
       " 'latest attacks\\tlive networks\\tnone',\n",
       " 'integral element\\toperational security\\tnone',\n",
       " 'latest attacks\\teffectiveness\\tnone',\n",
       " 'latest attacks\\tnetwork and security operations teams\\tnone',\n",
       " 'new platform\\tlive networks\\tnone',\n",
       " 'effectiveness\\texploits\\tnone',\n",
       " 'network and security operations teams\\texploits\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\texploits\\tnone',\n",
       " 'integral element\\texploits\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\toperational security\\tnone',\n",
       " 'latest attacks\\toperational security\\tnone',\n",
       " 'integral element\\tlive networks\\tnone',\n",
       " 'new platform\\toperational security\\tnone',\n",
       " 'exploits\\tlive networks\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\tlive networks\\tnone',\n",
       " 'latest attacks\\texploits\\tnone',\n",
       " 'exploits\\tnew platform\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\tnetwork and security operations teams\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\teffectiveness\\tnone',\n",
       " 'Threat Simulator breach and attack simulation solution\\tintegral element\\tnone',\n",
       " 'latest attacks\\tnew platform\\tnone',\n",
       " 'effectiveness\\tintegral element\\tnone',\n",
       " 'Security operations teams organizations\\tcomplex network environment\\tnone',\n",
       " 'attack\\tSecurity operations teams\\tnone',\n",
       " 'attack\\tSecurity operations teams organizations\\tnone',\n",
       " 'Security operations teams\\tSecurity operations teams organizations\\tnone',\n",
       " 'cyber threats\\tattack\\tnone',\n",
       " 'cyber threats\\tSecurity operations teams\\tnone',\n",
       " 'attack\\tcomplex network environment\\tnone',\n",
       " 'Security operations teams\\tcomplex network environment\\tnone',\n",
       " 'cyber threats\\tcomplex network environment\\tnone',\n",
       " 'cyber threats\\tSecurity operations teams organizations\\tnone',\n",
       " 'cyber threats\\tflood\\tnone',\n",
       " 'attack\\tflood\\tnone',\n",
       " 'Security operations teams\\tflood\\tnone',\n",
       " 'Security operations teams organizations\\tflood\\tnone',\n",
       " 'complex network environment\\tflood\\tnone',\n",
       " 'breach\\t50%\\tnone',\n",
       " '50%\\tGood security tools\\tnone',\n",
       " 'survey respondents\\tsurvey respondents security solution\\tnone',\n",
       " 'Good security tools\\trecent Keysight Security Operations Effectiveness survey\\tnone',\n",
       " 'survey respondents security solution\\tbreach\\tnone',\n",
       " 'survey respondents\\tbreach\\tnone',\n",
       " 'breach\\tGood security tools\\tnone',\n",
       " 'survey respondents security solution\\trecent Keysight Security Operations Effectiveness survey\\tnone',\n",
       " '50%\\trecent Keysight Security Operations Effectiveness survey\\tnone',\n",
       " 'survey respondents security solution\\t50%\\tnone',\n",
       " 'survey respondents security solution\\tGood security tools\\tnone',\n",
       " 'survey respondents\\t50%\\tnone',\n",
       " 'breach\\trecent Keysight Security Operations Effectiveness survey\\tnone',\n",
       " 'survey respondents\\tGood security tools\\tnone',\n",
       " 'survey respondents\\trecent Keysight Security Operations Effectiveness survey\\tnone',\n",
       " 'Most organizations security\\tsecurity\\tnone',\n",
       " 'test-based evidence\\trespondents\\tnone',\n",
       " 'test-based evidence\\trespondents security products\\tnone',\n",
       " 'test-based evidence\\tMost organizations\\tnone',\n",
       " 'respondents\\trespondents security products\\tnone',\n",
       " 'test-based evidence\\t35%\\tnone',\n",
       " 'Most organizations\\tMost organizations security\\tnone',\n",
       " 'Most organizations security\\trespondents\\tnone',\n",
       " 'Most organizations\\trespondents security products\\tnone',\n",
       " 'Most organizations security\\t35%\\tnone',\n",
       " 'test-based evidence\\tsecurity\\tnone',\n",
       " '35%\\trespondents security products\\tnone',\n",
       " '35%\\tsecurity\\tnone',\n",
       " 'Most organizations\\tsecurity\\tnone',\n",
       " 'test-based evidence\\tMost organizations security\\tnone',\n",
       " 'Most organizations\\t35%\\tnone',\n",
       " '35%\\trespondents\\tnone',\n",
       " 'respondents\\tsecurity\\tnone',\n",
       " 'Most organizations\\trespondents\\tnone',\n",
       " 'security\\trespondents security products\\tnone',\n",
       " 'Most organizations security\\trespondents security products\\tnone',\n",
       " 'solution\\tvulnerabilities\\tnone',\n",
       " 'solution\\t86%\\tnone',\n",
       " \"company's security posture\\t86%\\tnone\",\n",
       " 'value\\tsecurity testing\\tnone',\n",
       " 'respondents\\tsolution\\tnone',\n",
       " \"company's security posture\\tvalue\\tnone\",\n",
       " 'respondents\\tsecurity testing\\tnone',\n",
       " '86%\\tvulnerabilities\\tnone',\n",
       " 'value\\tsolution\\tnone',\n",
       " 'value\\tvulnerabilities\\tnone',\n",
       " 'value\\t86%\\tnone',\n",
       " 'security testing\\tvulnerabilities\\tnone',\n",
       " \"company's security posture\\trespondents\\tnone\",\n",
       " \"company's security posture\\tsolution\\tnone\",\n",
       " 'Most organizations\\tsecurity testing\\tnone',\n",
       " \"company's security posture\\tvulnerabilities\\tnone\",\n",
       " 'respondents\\t86%\\tnone',\n",
       " 'Most organizations\\tvulnerabilities\\tnone',\n",
       " 'Most organizations\\tvalue\\tnone',\n",
       " 'Most organizations\\t86%\\tnone',\n",
       " \"company's security posture\\tsecurity testing\\tnone\",\n",
       " 'security testing\\t86%\\tnone',\n",
       " 'respondents\\tvulnerabilities\\tnone',\n",
       " 'solution\\tsecurity testing\\tnone',\n",
       " 'respondents\\tvalue\\tnone',\n",
       " 'Most organizations\\trespondents\\tnone',\n",
       " \"company's security posture\\tMost organizations\\tnone\",\n",
       " 'Most organizations\\tsolution\\tnone',\n",
       " 'best security products\\tsenior vice president\\tnone',\n",
       " 'senior vice president\\tIT solutions provider Sayers\\tnone',\n",
       " 'Doug Close\\tyou\\tnone',\n",
       " 'you\\tcybersecurity\\tnone',\n",
       " 'cybersecurity\\tIT solutions provider Sayers\\tnone',\n",
       " 'you\\tIT solutions provider Sayers\\tnone',\n",
       " 'best security products\\tDoug Close\\tnone',\n",
       " 'Doug Close\\tcybersecurity\\tnone',\n",
       " 'Doug Close\\tIT solutions provider Sayers\\tnone',\n",
       " 'best security products\\tIT solutions provider Sayers\\tnone',\n",
       " 'you\\tsenior vice president\\tnone',\n",
       " 'best security products\\tcybersecurity\\tnone',\n",
       " 'best security products\\tyou\\tnone',\n",
       " 'Doug Close\\tsenior vice president\\tnone',\n",
       " 'senior vice president\\tcybersecurity\\tnone',\n",
       " 'detailed recommendations\\tgaps\\tnone',\n",
       " 'security gaps\\tgaps\\tnone',\n",
       " 'gaps\\tattacks\\tnone',\n",
       " 'powerful advantage\\tday\\tnone',\n",
       " 'detailed recommendations\\tday\\tnone',\n",
       " 'security gaps\\tcustomers\\tnone',\n",
       " 'customers\\tgaps\\tnone',\n",
       " 'security gaps\\tday\\tnone',\n",
       " 'customers\\tday\\tnone',\n",
       " 'attacks\\tday\\tnone',\n",
       " 'powerful advantage\\tgaps\\tnone',\n",
       " 'powerful advantage\\tdetailed recommendations\\tnone',\n",
       " 'security gaps\\tattacks\\tnone',\n",
       " 'customers\\tattacks\\tnone',\n",
       " 'customers\\tdetailed recommendations\\tnone',\n",
       " 'powerful advantage\\tattacks\\tnone',\n",
       " 'security gaps\\tdetailed recommendations\\tnone',\n",
       " 'powerful advantage\\tcustomers\\tnone',\n",
       " 'detailed recommendations\\tattacks\\tnone',\n",
       " 'gaps\\tday\\tnone',\n",
       " 'security gaps\\tpowerful advantage\\tnone',\n",
       " 'security capabilities\\tPaula Musich\\tnone',\n",
       " 'Paula Musich\\tmarket research firm Enterprise Management Associates\\tnone',\n",
       " 'security capabilities\\tone point\\tnone',\n",
       " 'security capabilities\\tlimited visibility\\tnone',\n",
       " 'security capabilities\\ttime\\tnone',\n",
       " 'limited visibility\\ttime\\tnone',\n",
       " 'Paula Musich\\tlimited visibility\\tnone',\n",
       " 'Paula Musich\\ttime\\tnone',\n",
       " \"Paula Musich\\torganization's ongoing security posture\\tnone\",\n",
       " \"organization's ongoing security posture\\tresearch director\\tnone\",\n",
       " \"security capabilities\\torganization's ongoing security posture\\tnone\",\n",
       " \"organization's ongoing security posture\\tlimited visibility\\tnone\",\n",
       " \"organization's ongoing security posture\\ttime\\tnone\",\n",
       " 'Paula Musich\\tsecurity and risk management\\tnone',\n",
       " 'limited visibility\\tone point\\tnone',\n",
       " 'time\\tone point\\tnone',\n",
       " 'market research firm Enterprise Management Associates\\tresearch director\\tnone',\n",
       " 'security and risk management\\ttime\\tnone',\n",
       " 'security and risk management\\tlimited visibility\\tnone',\n",
       " \"organization's ongoing security posture\\tone point\\tnone\",\n",
       " 'market research firm Enterprise Management Associates\\tone point\\tnone',\n",
       " 'security and risk management\\tresearch director\\tnone',\n",
       " \"market research firm Enterprise Management Associates\\torganization's ongoing security posture\\tnone\",\n",
       " 'Paula Musich\\tone point\\tnone',\n",
       " 'limited visibility\\tresearch director\\tnone',\n",
       " 'time\\tresearch director\\tnone',\n",
       " 'Paula Musich\\tresearch director\\tnone',\n",
       " 'security and risk management\\tmarket research firm Enterprise Management Associates\\tnone',\n",
       " 'market research firm Enterprise Management Associates\\tlimited visibility\\tnone',\n",
       " 'market research firm Enterprise Management Associates\\ttime\\tnone',\n",
       " 'security and risk management\\tone point\\tnone',\n",
       " 'security capabilities\\tsecurity and risk management\\tnone',\n",
       " 'security capabilities\\tresearch director\\tnone',\n",
       " \"security and risk management\\torganization's ongoing security posture\\tnone\",\n",
       " 'one point\\tresearch director\\tnone',\n",
       " 'security capabilities\\tmarket research firm Enterprise Management Associates\\tnone',\n",
       " 'heart\\tsecurity\\tnone',\n",
       " 'processes\\tsecurity\\tnone',\n",
       " 'processes\\tpeople\\tnone',\n",
       " 'processes\\theart\\tnone',\n",
       " 'heart\\tpeople\\tnone',\n",
       " 'security\\tissue\\tnone',\n",
       " 'processes\\tissue\\tnone',\n",
       " 'people\\tissue\\tnone',\n",
       " 'heart\\tissue\\tnone',\n",
       " 'security\\tpeople\\tnone',\n",
       " 'top\\tchanges\\tnone',\n",
       " 'defenses\\tregular basis\\tnone',\n",
       " 'security operations teams\\tgood security hygiene\\tnone',\n",
       " 'defenses\\tattack simulation\\tnone',\n",
       " 'exploitable vulnerability\\ttesting\\tnone',\n",
       " 'changes\\texploitable vulnerability\\tnone',\n",
       " 'attack simulation\\tregular basis\\tnone',\n",
       " 'testing\\tsecurity operations teams\\tnone',\n",
       " 'changes\\tdefenses\\tnone',\n",
       " 'top\\tgood security hygiene\\tnone',\n",
       " 'top\\tdefenses\\tnone',\n",
       " 'exploitable vulnerability\\tdefenses\\tnone',\n",
       " 'testing\\tattack simulation\\tnone',\n",
       " 'changes\\ttesting\\tnone',\n",
       " 'top\\ttesting\\tnone',\n",
       " 'attack simulation\\tgood security hygiene\\tnone',\n",
       " 'security operations teams\\tregular basis\\tnone',\n",
       " 'security operations teams\\tdefenses\\tnone',\n",
       " 'testing\\tregular basis\\tnone',\n",
       " 'top\\tregular basis\\tnone',\n",
       " 'changes\\tregular basis\\tnone',\n",
       " 'defenses\\tgood security hygiene\\tnone',\n",
       " 'security operations teams\\tattack simulation\\tnone',\n",
       " 'exploitable vulnerability\\tregular basis\\tnone',\n",
       " 'top\\tattack simulation\\tnone',\n",
       " 'testing\\tdefenses\\tnone',\n",
       " 'changes\\tsecurity operations teams\\tnone',\n",
       " 'top\\tsecurity operations teams\\tnone',\n",
       " 'changes\\tattack simulation\\tnone',\n",
       " 'exploitable vulnerability\\tsecurity operations teams\\tnone',\n",
       " 'top\\texploitable vulnerability\\tnone',\n",
       " 'exploitable vulnerability\\tattack simulation\\tnone',\n",
       " 'exploitable vulnerability\\tgood security hygiene\\tnone',\n",
       " 'regular basis\\tgood security hygiene\\tnone',\n",
       " 'changes\\tgood security hygiene\\tnone',\n",
       " 'testing\\tgood security hygiene\\tnone',\n",
       " \"Security Tools\\tIntended Keysight's Threat Simulator solution\\tnone\",\n",
       " \"Threat Simulator\\tIntended Keysight's Threat Simulator solution\\tnone\",\n",
       " 'Threat Simulator\\tSecurity Tools\\tnone',\n",
       " 'Confidence\\tSecurity Tools\\tnone',\n",
       " 'Threat Simulator\\tConfidence\\tnone',\n",
       " \"Confidence\\tIntended Keysight's Threat Simulator solution\\tnone\",\n",
       " 'organization\\tsecurity tools effectiveness\\tnone',\n",
       " 'security tools\\tmethod\\tnone',\n",
       " 'method\\tsecurity tools effectiveness\\tnone',\n",
       " 'security tools\\tsecurity tools effectiveness\\tnone',\n",
       " 'enterprise security operations teams\\tsecurity tools effectiveness\\tnone',\n",
       " 'method\\torganization\\tnone',\n",
       " 'enterprise security operations teams\\torganization\\tnone',\n",
       " 'security tools\\torganization\\tnone',\n",
       " 'enterprise security operations teams\\tmethod\\tnone',\n",
       " 'security tools\\tenterprise security operations teams\\tnone',\n",
       " 'continuous, automated security assessment\\torganizations\\tnone',\n",
       " 'gaps\\tend\\tnone',\n",
       " 'environment drift\\tsecurity configurations\\tnone',\n",
       " 'continuous, automated security assessment\\trelated group\\tnone',\n",
       " 'someone\\trelated group\\tnone',\n",
       " 'IT\\tchange\\tnone',\n",
       " 'continuous, automated security assessment\\tsecurity configurations\\tnone',\n",
       " 'malicious intent\\tclear remediation steps\\tnone',\n",
       " 'someone\\tIT\\tnone',\n",
       " 'environment drift\\tend\\tnone',\n",
       " 'security configurations\\tpatented recommendation engine\\tnone',\n",
       " 'IT\\tgaps\\tnone',\n",
       " 'environment drift\\tmalicious intent\\tnone',\n",
       " 'related group\\tpatented recommendation engine\\tnone',\n",
       " 'malicious intent\\tIt\\tnone',\n",
       " 'someone\\tIt\\tnone',\n",
       " 'malicious intent\\torganizations\\tnone',\n",
       " 'clear remediation steps\\tresult\\tnone',\n",
       " 'environment drift\\trelated group\\tnone',\n",
       " 'continuous, automated security assessment\\tchange\\tnone',\n",
       " 'security configurations\\tgaps\\tnone',\n",
       " 'result\\tgaps\\tnone',\n",
       " 'result\\tIt\\tnone',\n",
       " 'security configurations\\tIT\\tnone',\n",
       " 'malicious intent\\tsomeone\\tnone',\n",
       " 'security configurations\\tIt\\tnone',\n",
       " 'IT\\tend\\tnone',\n",
       " 'related group\\tend\\tnone',\n",
       " 'security configurations\\tend\\tnone',\n",
       " 'gaps\\trelated group\\tnone',\n",
       " 'clear remediation steps\\tpatented recommendation engine\\tnone',\n",
       " 'someone\\tgaps\\tnone',\n",
       " 'organizations\\trelated group\\tnone',\n",
       " 'environment drift\\tpatented recommendation engine\\tnone',\n",
       " 'malicious intent\\tgaps\\tnone',\n",
       " 'change\\tend\\tnone',\n",
       " 'environment drift\\tIt\\tnone',\n",
       " 'security configurations\\tsomeone\\tnone',\n",
       " 'IT\\tIt\\tnone',\n",
       " 'security configurations\\tresult\\tnone',\n",
       " 'result\\tend\\tnone',\n",
       " 'malicious intent\\tchange\\tnone',\n",
       " 'gaps\\tpatented recommendation engine\\tnone',\n",
       " 'clear remediation steps\\torganizations\\tnone',\n",
       " 'clear remediation steps\\trelated group\\tnone',\n",
       " 'end\\tIt\\tnone',\n",
       " 'continuous, automated security assessment\\tsomeone\\tnone',\n",
       " 'environment drift\\tclear remediation steps\\tnone',\n",
       " 'malicious intent\\tresult\\tnone',\n",
       " 'someone\\tend\\tnone',\n",
       " 'organizations\\tpatented recommendation engine\\tnone',\n",
       " 'someone\\tresult\\tnone',\n",
       " 'change\\tIt\\tnone',\n",
       " 'malicious intent\\tpatented recommendation engine\\tnone',\n",
       " 'change\\tpatented recommendation engine\\tnone',\n",
       " 'result\\trelated group\\tnone',\n",
       " 'related group\\tIt\\tnone',\n",
       " 'continuous, automated security assessment\\tIt\\tnone',\n",
       " 'clear remediation steps\\tsomeone\\tnone',\n",
       " 'malicious intent\\tIT\\tnone',\n",
       " 'organizations\\tend\\tnone',\n",
       " 'malicious intent\\trelated group\\tnone',\n",
       " 'change\\torganizations\\tnone',\n",
       " 'change\\trelated group\\tnone',\n",
       " 'environment drift\\tresult\\tnone',\n",
       " 'clear remediation steps\\tchange\\tnone',\n",
       " 'result\\tpatented recommendation engine\\tnone',\n",
       " 'environment drift\\tsomeone\\tnone',\n",
       " 'continuous, automated security assessment\\tclear remediation steps\\tnone',\n",
       " 'result\\tIT\\tnone',\n",
       " 'clear remediation steps\\tgaps\\tnone',\n",
       " 'someone\\torganizations\\tnone',\n",
       " 'result\\tchange\\tnone',\n",
       " 'clear remediation steps\\tend\\tnone',\n",
       " 'result\\torganizations\\tnone',\n",
       " 'IT\\tpatented recommendation engine\\tnone',\n",
       " 'continuous, automated security assessment\\tgaps\\tnone',\n",
       " 'gaps\\tIt\\tnone',\n",
       " 'security configurations\\tchange\\tnone',\n",
       " 'continuous, automated security assessment\\tresult\\tnone',\n",
       " 'clear remediation steps\\tIT\\tnone',\n",
       " 'clear remediation steps\\tIt\\tnone',\n",
       " 'environment drift\\torganizations\\tnone',\n",
       " 'security configurations\\torganizations\\tnone',\n",
       " 'change\\tgaps\\tnone',\n",
       " 'organizations\\tIt\\tnone',\n",
       " 'IT\\torganizations\\tnone',\n",
       " 'someone\\tchange\\tnone',\n",
       " 'IT\\trelated group\\tnone',\n",
       " 'patented recommendation engine\\tend\\tnone',\n",
       " 'continuous, automated security assessment\\tpatented recommendation engine\\tnone',\n",
       " 'patented recommendation engine\\tIt\\tnone',\n",
       " 'continuous, automated security assessment\\tend\\tnone',\n",
       " 'gaps\\torganizations\\tnone',\n",
       " 'continuous, automated security assessment\\tmalicious intent\\tnone',\n",
       " 'malicious intent\\tend\\tnone',\n",
       " 'security configurations\\tclear remediation steps\\tnone',\n",
       " 'continuous, automated security assessment\\tIT\\tnone',\n",
       " 'someone\\tpatented recommendation engine\\tnone',\n",
       " 'security configurations\\tmalicious intent\\tnone',\n",
       " 'continuous, automated security assessment\\tenvironment drift\\tnone',\n",
       " 'environment drift\\tgaps\\tnone',\n",
       " 'environment drift\\tIT\\tnone',\n",
       " 'security configurations\\trelated group\\tnone',\n",
       " 'environment drift\\tchange\\tnone',\n",
       " 'Ixia Solutions Group\\tSecurity Group\\tnone',\n",
       " \"Today's network and security teams security solutions\\tcontinuous basis\\tnone\",\n",
       " \"security solutions\\tToday's network and security teams security solutions\\tnone\",\n",
       " \"Keysight Network Applications\\tToday's network and security teams\\tnone\",\n",
       " \"Today's network and security teams\\tIxia Solutions Group\\tnone\",\n",
       " 'security solutions\\tKeysight Network Applications\\tnone',\n",
       " 'Keysight Network Applications\\tcontinuous basis\\tnone',\n",
       " \"Today's network and security teams\\tSecurity Group\\tnone\",\n",
       " \"vice president\\tToday's network and security teams security solutions\\tnone\",\n",
       " \"Today's network and security teams\\tcontinuous basis\\tnone\",\n",
       " 'security solutions\\tIxia Solutions Group\\tnone',\n",
       " \"Ixia Solutions Group\\tToday's network and security teams security solutions\\tnone\",\n",
       " \"Security Group\\tToday's network and security teams security solutions\\tnone\",\n",
       " 'security solutions\\tcontinuous basis\\tnone',\n",
       " \"vice president\\tToday's network and security teams\\tnone\",\n",
       " 'security solutions\\tvice president\\tnone',\n",
       " 'Scott Register\\tvice president\\tnone',\n",
       " 'vice president\\tcontinuous basis\\tnone',\n",
       " 'security solutions\\tSecurity Group\\tnone',\n",
       " 'Ixia Solutions Group\\tcontinuous basis\\tnone',\n",
       " 'Scott Register\\tIxia Solutions Group\\tnone',\n",
       " 'vice president\\tIxia Solutions Group\\tnone',\n",
       " 'Scott Register\\tSecurity Group\\tnone',\n",
       " 'Scott Register\\tKeysight Network Applications\\tnone',\n",
       " 'vice president\\tKeysight Network Applications\\tnone',\n",
       " 'Scott Register\\tsecurity solutions\\tnone',\n",
       " 'Scott Register\\tcontinuous basis\\tnone',\n",
       " 'vice president\\tSecurity Group\\tnone',\n",
       " \"Today's network and security teams\\tToday's network and security teams security solutions\\tnone\",\n",
       " 'Keysight Network Applications\\tIxia Solutions Group\\tnone',\n",
       " \"security solutions\\tToday's network and security teams\\tnone\",\n",
       " \"Scott Register\\tToday's network and security teams security solutions\\tnone\",\n",
       " \"Keysight Network Applications\\tToday's network and security teams security solutions\\tnone\",\n",
       " \"Scott Register\\tToday's network and security teams\\tnone\",\n",
       " 'Keysight Network Applications\\tSecurity Group\\tnone',\n",
       " 'Security Group\\tcontinuous basis\\tnone',\n",
       " 'security skills\\tlack\\tnone',\n",
       " 'security skills\\tmisconfigurations\\tnone',\n",
       " 'lack\\tmisconfigurations\\tnone',\n",
       " 'Security breaches\\tcapable products\\tnone',\n",
       " 'capable products\\tmisconfigurations\\tnone',\n",
       " 'security skills\\tSecurity breaches\\tnone',\n",
       " 'lack\\tSecurity breaches\\tnone',\n",
       " 'security skills\\tcapable products\\tnone',\n",
       " 'lack\\tcapable products\\tnone',\n",
       " 'Security breaches\\tmisconfigurations\\tnone',\n",
       " 'easy task\\tcoverage gaps\\tnone',\n",
       " 'live network\\tcoverage gaps\\tnone',\n",
       " 'easy task\\tlive network\\tnone',\n",
       " 'security operations teams\\tgaps\\tnone',\n",
       " 'gaps security posture\\tgaps\\tnone',\n",
       " 'Threat Simulator\\tsecurity operations teams\\tnone',\n",
       " 'gaps security posture\\tactionable insight\\tnone',\n",
       " 'actionable insight\\tgaps\\tnone',\n",
       " 'gaps security posture\\tsecurity operations teams\\tnone',\n",
       " 'Threat Simulator\\tactionable insight\\tnone',\n",
       " 'Threat Simulator\\tgaps\\tnone',\n",
       " 'gaps security posture\\tThreat Simulator\\tnone',\n",
       " 'security operations teams\\tactionable insight\\tnone',\n",
       " \"Keysight's Breach Defense SecOps platform\\tThreatARMOR\\tnone\",\n",
       " \"threat intelligence gateway\\tKeysight's Breach Defense SecOps platform\\tnone\",\n",
       " \"Threat Simulator\\tKeysight's Breach Defense SecOps platform\\tnone\",\n",
       " 'ThreatARMOR\\taddition\\tnone',\n",
       " 'Threat Simulator\\tthreat intelligence gateway\\tnone',\n",
       " 'threat intelligence gateway\\taddition\\tnone',\n",
       " \"Keysight's Breach Defense SecOps platform\\taddition\\tnone\",\n",
       " 'Threat Simulator\\tThreatARMOR\\tnone',\n",
       " 'Threat Simulator\\taddition\\tnone',\n",
       " 'threat intelligence gateway\\tThreatARMOR\\tnone',\n",
       " 'event management\\tattack surface\\tnone',\n",
       " 'up to 80%\\tsource\\tnone',\n",
       " 'malicious traffic\\tsource\\tnone',\n",
       " 'ThreatARMOR\\tsource\\tnone',\n",
       " 'number\\texisting security infrastructure\\tnone',\n",
       " 'number\\tsecurity information\\tnone',\n",
       " 'attack surface\\tsource\\tnone',\n",
       " 'SIEM alerts\\tsource\\tnone',\n",
       " 'attack surface\\tsecurity information\\tnone',\n",
       " 'number\\tup to 80%\\tnone',\n",
       " 'number\\tattack surface\\tnone',\n",
       " 'SIEM alerts\\tevent management\\tnone',\n",
       " 'SIEM alerts\\texisting security infrastructure\\tnone',\n",
       " 'SIEM alerts\\tmalicious traffic\\tnone',\n",
       " 'SIEM alerts\\tThreatARMOR\\tnone',\n",
       " 'SIEM alerts\\tsecurity information\\tnone',\n",
       " 'existing security infrastructure\\tsource\\tnone',\n",
       " 'security information\\tsource\\tnone',\n",
       " 'up to 80%\\tevent management\\tnone',\n",
       " 'event management\\tsource\\tnone',\n",
       " 'up to 80%\\tattack surface\\tnone',\n",
       " 'malicious traffic\\tup to 80%\\tnone',\n",
       " 'ThreatARMOR\\tup to 80%\\tnone',\n",
       " 'malicious traffic\\tevent management\\tnone',\n",
       " 'ThreatARMOR\\tevent management\\tnone',\n",
       " 'malicious traffic\\tattack surface\\tnone',\n",
       " 'ThreatARMOR\\tattack surface\\tnone',\n",
       " 'up to 80%\\texisting security infrastructure\\tnone',\n",
       " 'up to 80%\\tsecurity information\\tnone',\n",
       " 'malicious traffic\\tnumber\\tnone',\n",
       " 'ThreatARMOR\\tnumber\\tnone',\n",
       " 'malicious traffic\\texisting security infrastructure\\tnone',\n",
       " 'malicious traffic\\tsecurity information\\tnone',\n",
       " 'ThreatARMOR\\texisting security infrastructure\\tnone',\n",
       " 'ThreatARMOR\\tsecurity information\\tnone',\n",
       " 'SIEM alerts\\tup to 80%\\tnone',\n",
       " 'SIEM alerts\\tnumber\\tnone',\n",
       " 'existing security infrastructure\\tsecurity information\\tnone',\n",
       " 'event management\\texisting security infrastructure\\tnone',\n",
       " 'event management\\tsecurity information\\tnone',\n",
       " 'number\\tevent management\\tnone',\n",
       " 'SIEM alerts\\tattack surface\\tnone',\n",
       " 'number\\tsource\\tnone',\n",
       " 'attack surface\\texisting security infrastructure\\tnone',\n",
       " 'malicious traffic\\tThreatARMOR\\tnone',\n",
       " 'unassigned IP addresses\\tline-rate speeds\\tnone',\n",
       " 'unassigned IP addresses\\tSIEM tools\\tnone',\n",
       " 'known bad IP addresses\\tThreatARMOR\\tnone',\n",
       " 'geography\\tnetwork\\tnone',\n",
       " 'traffic\\tSIEM tools\\tnone',\n",
       " 'internal devices\\tmalicious IP addresses\\tnone',\n",
       " 'geography\\tline-rate speeds\\tnone',\n",
       " 'unused IP space\\tdomains\\tnone',\n",
       " 'internal devices\\tunassigned IP addresses\\tnone',\n",
       " 'network\\tline-rate speeds\\tnone',\n",
       " 'known bad IP addresses\\tSIEM tools\\tnone',\n",
       " 'known bad IP addresses\\tline-rate speeds\\tnone',\n",
       " 'known botnet C&C servers\\tSIEM tools\\tnone',\n",
       " 'network\\tunassigned IP addresses\\tnone',\n",
       " 'malicious IP addresses\\ttraffic\\tnone',\n",
       " 'malicious IP addresses\\tunused IP space\\tnone',\n",
       " 'known botnet C&C servers\\ttraffic\\tnone',\n",
       " 'unused IP space\\tunassigned IP addresses\\tnone',\n",
       " 'known botnet C&C servers\\tunused IP space\\tnone',\n",
       " 'domains\\ttraffic\\tnone',\n",
       " 'unused IP space\\tline-rate speeds\\tnone',\n",
       " 'internal devices\\tSIEM tools\\tnone',\n",
       " 'internal devices\\tThreatARMOR\\tnone',\n",
       " 'geography\\tdomains\\tnone',\n",
       " 'known bad IP addresses\\tunused IP space\\tnone',\n",
       " 'ThreatARMOR\\tunassigned IP addresses\\tnone',\n",
       " 'known bad IP addresses\\tnetwork\\tnone',\n",
       " 'known bad IP addresses\\tmalicious IP addresses\\tnone',\n",
       " 'geography\\ttraffic\\tnone',\n",
       " 'network\\ttraffic\\tnone',\n",
       " 'domains\\tThreatARMOR\\tnone',\n",
       " 'malicious IP addresses\\tSIEM tools\\tnone',\n",
       " 'network\\tThreatARMOR\\tnone',\n",
       " 'internal devices\\tdomains\\tnone',\n",
       " 'malicious IP addresses\\tline-rate speeds\\tnone',\n",
       " 'geography\\tThreatARMOR\\tnone',\n",
       " 'geography\\tSIEM tools\\tnone',\n",
       " 'geography\\tmalicious IP addresses\\tnone',\n",
       " 'internal devices\\tnetwork\\tnone',\n",
       " 'known bad IP addresses\\tunassigned IP addresses\\tnone',\n",
       " 'domains\\tSIEM tools\\tnone',\n",
       " 'domains\\tline-rate speeds\\tnone',\n",
       " 'ThreatARMOR\\ttraffic\\tnone',\n",
       " 'internal devices\\ttraffic\\tnone',\n",
       " 'known botnet C&C servers\\tgeography\\tnone',\n",
       " 'known botnet C&C servers\\tinternal devices\\tnone',\n",
       " 'known bad IP addresses\\tknown botnet C&C servers\\tnone',\n",
       " 'malicious IP addresses\\tunassigned IP addresses\\tnone',\n",
       " 'traffic\\tunassigned IP addresses\\tnone',\n",
       " 'unused IP space\\tSIEM tools\\tnone',\n",
       " 'geography\\tunassigned IP addresses\\tnone',\n",
       " 'known botnet C&C servers\\tThreatARMOR\\tnone',\n",
       " 'geography\\tunused IP space\\tnone',\n",
       " 'malicious IP addresses\\tnetwork\\tnone',\n",
       " 'known botnet C&C servers\\tmalicious IP addresses\\tnone',\n",
       " 'known botnet C&C servers\\tunassigned IP addresses\\tnone',\n",
       " 'known bad IP addresses\\tgeography\\tnone',\n",
       " 'malicious IP addresses\\tThreatARMOR\\tnone',\n",
       " 'known botnet C&C servers\\tnetwork\\tnone',\n",
       " 'network\\tSIEM tools\\tnone',\n",
       " 'SIEM tools\\tline-rate speeds\\tnone',\n",
       " 'known bad IP addresses\\tdomains\\tnone',\n",
       " 'known bad IP addresses\\ttraffic\\tnone',\n",
       " 'traffic\\tline-rate speeds\\tnone',\n",
       " 'unused IP space\\tnetwork\\tnone',\n",
       " 'ThreatARMOR\\tSIEM tools\\tnone',\n",
       " 'ThreatARMOR\\tline-rate speeds\\tnone',\n",
       " 'known botnet C&C servers\\tline-rate speeds\\tnone',\n",
       " 'internal devices\\tline-rate speeds\\tnone',\n",
       " 'malicious IP addresses\\tdomains\\tnone',\n",
       " 'internal devices\\tunused IP space\\tnone',\n",
       " 'unused IP space\\tThreatARMOR\\tnone',\n",
       " 'known bad IP addresses\\tinternal devices\\tnone',\n",
       " 'known botnet C&C servers\\tdomains\\tnone',\n",
       " 'unused IP space\\ttraffic\\tnone',\n",
       " 'domains\\tnetwork\\tnone',\n",
       " 'domains\\tunassigned IP addresses\\tnone',\n",
       " 'geography\\tinternal devices\\tnone',\n",
       " 'leading technology company\\tInc NYSE\\tnone',\n",
       " 'governments\\tenterprises\\tnone',\n",
       " 'Keysight Technologies\\tInc NYSE\\tnone',\n",
       " 'governments\\tInc NYSE\\tnone',\n",
       " 'enterprises\\tinnovation\\tnone',\n",
       " 'KEYS\\tInc NYSE\\tnone',\n",
       " 'world\\tKEYS\\tnone',\n",
       " 'Keysight Technologies\\tleading technology company\\tnone',\n",
       " 'governments\\tleading technology company\\tnone',\n",
       " 'governments\\tworld\\tnone',\n",
       " 'leading technology company\\tKEYS\\tnone',\n",
       " 'service providers\\tKEYS\\tnone',\n",
       " 'service providers\\tgovernments\\tnone',\n",
       " 'enterprises\\tworld\\tnone',\n",
       " 'Keysight Technologies\\tKEYS\\tnone',\n",
       " 'leading technology company\\tinnovation\\tnone',\n",
       " 'service providers\\tleading technology company\\tnone',\n",
       " 'world\\tinnovation\\tnone',\n",
       " 'service providers\\tInc NYSE\\tnone',\n",
       " 'Keysight Technologies\\tenterprises\\tnone',\n",
       " 'Inc NYSE\\tinnovation\\tnone',\n",
       " 'KEYS\\tinnovation\\tnone',\n",
       " 'enterprises\\tleading technology company\\tnone',\n",
       " 'service providers\\tenterprises\\tnone',\n",
       " 'Keysight Technologies\\tinnovation\\tnone',\n",
       " 'governments\\tinnovation\\tnone',\n",
       " 'enterprises\\tInc NYSE\\tnone',\n",
       " 'Keysight Technologies\\tgovernments\\tnone',\n",
       " 'service providers\\tKeysight Technologies\\tnone',\n",
       " 'Keysight Technologies\\tworld\\tnone',\n",
       " 'enterprises\\tKEYS\\tnone',\n",
       " 'governments\\tKEYS\\tnone',\n",
       " 'service providers\\tworld\\tnone',\n",
       " 'world\\tInc NYSE\\tnone',\n",
       " 'service providers\\tinnovation\\tnone',\n",
       " 'world\\tleading technology company\\tnone',\n",
       " 'electronic products\\tmanufacturing test\\tnone',\n",
       " 'electronic products\\tofferings\\tnone',\n",
       " 'manufacturing test\\tto optimization\\tnone',\n",
       " 'cloud environments\\tmanufacturing test\\tnone',\n",
       " 'cloud environments\\tofferings\\tnone',\n",
       " 'offerings\\tto optimization\\tnone',\n",
       " 'networks\\telectronic products\\tnone',\n",
       " 'prototype validation\\tofferings\\tnone',\n",
       " 'design simulation\\tto optimization\\tnone',\n",
       " 'prototype validation\\tlower cost\\tnone',\n",
       " 'networks\\tmanufacturing test\\tnone',\n",
       " 'networks\\tofferings\\tnone',\n",
       " 'networks\\tprototype validation\\tnone',\n",
       " 'cloud environments\\tdesign simulation\\tnone',\n",
       " \"design simulation\\tKeysight's solutions\\tnone\",\n",
       " 'manufacturing test\\tlower cost\\tnone',\n",
       " 'offerings\\tlower cost\\tnone',\n",
       " \"electronic products\\tKeysight's solutions\\tnone\",\n",
       " 'design simulation\\tlower cost\\tnone',\n",
       " 'electronic products\\tlower cost\\tnone',\n",
       " 'networks\\tto optimization\\tnone',\n",
       " 'networks\\tdesign simulation\\tnone',\n",
       " \"prototype validation\\tKeysight's solutions\\tnone\",\n",
       " \"networks\\tKeysight's solutions\\tnone\",\n",
       " \"Keysight's solutions\\tlower cost\\tnone\",\n",
       " 'manufacturing test\\tdesign simulation\\tnone',\n",
       " 'to optimization\\tlower cost\\tnone',\n",
       " \"manufacturing test\\tKeysight's solutions\\tnone\",\n",
       " \"offerings\\tKeysight's solutions\\tnone\",\n",
       " 'cloud environments\\tto optimization\\tnone',\n",
       " 'electronic products\\tdesign simulation\\tnone',\n",
       " 'electronic products\\tto optimization\\tnone',\n",
       " 'networks\\tlower cost\\tnone',\n",
       " 'design simulation\\tprototype validation\\tnone',\n",
       " 'design simulation\\tofferings\\tnone',\n",
       " \"cloud environments\\tKeysight's solutions\\tnone\",\n",
       " 'manufacturing test\\tofferings\\tnone',\n",
       " 'prototype validation\\tto optimization\\tnone',\n",
       " 'networks\\tcloud environments\\tnone',\n",
       " 'cloud environments\\telectronic products\\tnone',\n",
       " 'manufacturing test\\tprototype validation\\tnone',\n",
       " \"Keysight's solutions\\tto optimization\\tnone\",\n",
       " 'electronic products\\tprototype validation\\tnone',\n",
       " 'cloud environments\\tlower cost\\tnone',\n",
       " 'cloud environments\\tprototype validation\\tnone',\n",
       " 'end markets\\tgeneral electronics\\tnone',\n",
       " 'semiconductor\\tgeneral electronics\\tnone',\n",
       " 'aerospace\\tgeneral electronics\\tnone',\n",
       " 'semiconductor\\taerospace\\tnone',\n",
       " 'worldwide communications ecosystem\\tCustomers\\tnone',\n",
       " 'end markets\\taerospace\\tnone',\n",
       " 'general electronics\\tCustomers\\tnone',\n",
       " 'worldwide communications ecosystem\\tenergy\\tnone',\n",
       " 'worldwide communications ecosystem\\tsemiconductor\\tnone',\n",
       " 'aerospace\\tenergy\\tnone',\n",
       " 'defense\\tgeneral electronics\\tnone',\n",
       " 'aerospace\\tdefense\\tnone',\n",
       " 'aerospace\\tCustomers\\tnone',\n",
       " 'worldwide communications ecosystem\\tgeneral electronics\\tnone',\n",
       " 'defense\\tCustomers\\tnone',\n",
       " 'end markets\\tsemiconductor\\tnone',\n",
       " 'defense\\tenergy\\tnone',\n",
       " 'worldwide communications ecosystem\\tdefense\\tnone',\n",
       " 'energy\\tCustomers\\tnone',\n",
       " 'end markets\\tworldwide communications ecosystem\\tnone',\n",
       " 'end markets\\tCustomers\\tnone',\n",
       " 'worldwide communications ecosystem\\taerospace\\tnone',\n",
       " 'semiconductor\\tCustomers\\tnone',\n",
       " 'semiconductor\\tdefense\\tnone',\n",
       " 'end markets\\tenergy\\tnone',\n",
       " 'energy\\tgeneral electronics\\tnone',\n",
       " 'end markets\\tdefense\\tnone',\n",
       " 'semiconductor\\tenergy\\tnone',\n",
       " 'Keysight\\tfiscal year\\tnone',\n",
       " 'fiscal year\\trevenues\\tnone',\n",
       " 'Keysight\\trevenues\\tnone']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('vulnerabilities', 'real - time threat intelligence')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-219fac573c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minstances_db\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vulnerabilities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"real - time threat intelligence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ('vulnerabilities', 'real - time threat intelligence')"
     ]
    }
   ],
   "source": [
    "instances_db[(\"vulnerabilities\", \"real - time threat intelligence\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickledb\n",
    "prefix = \"../junk/db_files/\"\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "allkeys = list(word2id_db.getall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  tylgiv\n",
      "Dropping  valtra\n",
      "Dropping  matsika\n",
      "Dropping  frenstrup\n",
      "Dropping  kakkassery\n",
      "Dropping  only martelly\n",
      "Dropping  n700\n",
      "Dropping  mitteldeutschland\n",
      "Dropping  n5348a\n",
      "Dropping  hiramic\n",
      "Dropping  defined fields\n",
      "Dropping  the s j p harvie professor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a terminating binary expansion\n",
      "Dropping  the online canvas design elements\n",
      "Dropping  the instantaneous angular velocity vector\n",
      "Dropping  fitting anorexic illnesses\n",
      "Dropping  a 1920s proposal\n",
      "Dropping  an international non profit and non governmental student society\n",
      "Dropping  william a trimble\n",
      "Dropping  a provincial regiment\n",
      "Dropping  first real studio experience\n",
      "Dropping  a lycoming o 360 a4 m\n",
      "Dropping  other graphics systems\n",
      "Dropping  polish tradition\n",
      "Dropping  a practising teacher\n",
      "Dropping  close diplomatic and economic relationships\n",
      "Dropping  kiley\n",
      "Dropping  original or reconstructed fabric\n",
      "Dropping  scriptural or customary laws\n",
      "Dropping  national economics challenge champions\n",
      "Dropping  a long horizontal jump\n",
      "Dropping  the open bloodstream\n",
      "Dropping  the officer s blooded horses\n",
      "Dropping  classical comedy\n",
      "Dropping  the continental exchanges\n",
      "Dropping  the most frequent uses\n",
      "Dropping  major local developers\n",
      "Dropping  184 restaurants\n",
      "Dropping  maria s young son\n",
      "Dropping  utsu\n",
      "Dropping  archeologist hugo winckler\n",
      "Dropping  zp120\n",
      "Dropping  the early 1950s dubuffet\n",
      "Dropping  merina and betsileo families\n",
      "Dropping  impersonalization\n",
      "Dropping  all necessary activities\n",
      "Dropping  more complex background settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15446:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "Exception in thread Thread-15449:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  periodic recitals\n",
      "Dropping  last weekend s post coup presidential election\n",
      "Dropping  so2 james suh\n",
      "Dropping  silvie iii\n",
      "Dropping  pot au feu\n",
      "Dropping  its operational readiness\n",
      "Dropping  no one reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15472:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  the exterior mirror\n",
      "Dropping  free agent greg holland\n",
      "Dropping  keio university hospital\n",
      "Dropping  negative at skew\n",
      "Dropping  the former coalfield area\n",
      "Dropping  a coherent personality\n",
      "Dropping  intevation\n",
      "Dropping  fgm 148 javelin\n",
      "Dropping  17 august robert ritter von greim s fliegerkorps v\n",
      "Dropping  neither military training\n",
      "Dropping  self service passport control\n",
      "Dropping  sierra s salon\n",
      "Dropping  general no l de castelnau\n",
      "Dropping  debra delee\n",
      "Dropping  davis second term\n",
      "Dropping  the oldest literary account\n",
      "Dropping  each wall inlet\n",
      "Dropping  the people s nomadic heritage\n",
      "Dropping  glasgow academicals\n",
      "Dropping  fine v fib\n",
      "Dropping  flat end facets\n",
      "Dropping  dense grids\n",
      "Dropping  professor dominique martin\n",
      "Dropping  the fastest overall driver\n",
      "Dropping  their sledging rations\n",
      "Dropping  the lambda company\n",
      "Dropping  the additional rail\n",
      "Dropping  maintenance flaws\n",
      "Dropping  a 75 cm long bundle\n",
      "Dropping  179 fs\n",
      "Dropping  military miniatures\n",
      "Dropping  performance and management flexibility\n",
      "Dropping  two state run polytechnic schools\n",
      "Dropping  scriabin s museum\n",
      "Dropping  protestant dublin lawyer theobald wolfe tone\n",
      "Dropping  16 canadians\n",
      "Dropping  the individual coal plots\n",
      "Dropping  i e visemes\n",
      "Dropping  a e36 m3 compact prototype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15713:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  sal n de la paz\n",
      "Dropping  brian williams lustmord project\n",
      "Dropping  an exponential behavior\n",
      "Dropping  this uncommon case\n",
      "Dropping  only 13 more performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15739:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  then a third wrestling team\n",
      "Dropping  an old watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15748:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a sophisticated propaganda machine\n",
      "Dropping  a successful and effective program\n",
      "Dropping  50 s strongest track\n",
      "Dropping  the yshphh\n",
      "Dropping  the estimated sinking position\n",
      "Dropping  phoenix s citizens\n",
      "Dropping  the cbbb\n",
      "Dropping  re arranged panels\n",
      "Dropping  his 50th birthday celebration\n",
      "Dropping  the male eggs\n",
      "Dropping  montane meadows\n",
      "Dropping  the troops good spirit\n",
      "Dropping  paltrow s performance\n",
      "Dropping  a free demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15808:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  roxy attempts\n",
      "Dropping  either deletion\n",
      "Dropping  frictional behavior\n",
      "Dropping  four successive popes\n",
      "Dropping  engineering design teams\n",
      "Dropping  felix the cat\n",
      "Dropping  tidal venuses\n",
      "Dropping  dsquared2 duo dean and dan caten\n",
      "Dropping  cooper s most important film\n",
      "Dropping  consistent subtest scores\n",
      "Dropping  frances hegarty\n",
      "Dropping  borland s guitar playing\n",
      "Dropping  ahsura\n",
      "Dropping  an unnamed polish clone\n",
      "Dropping  at least the a credit rating\n",
      "Dropping  a radio based transatlantic telephone service\n",
      "Dropping  carddass exclusive storyline series\n",
      "Dropping  a balance sheet hedge\n",
      "Dropping  bluebush saltbush steppe\n"
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "allkeys = list(word2id_db.getall())\n",
    "for key in allkeys:\n",
    "    try:\n",
    "        word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "    except:\n",
    "        print (\"Dropping \", key)\n",
    "        word2id_db_corrected[key] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = key\n",
    "word2id_db_corrected.dump()\n",
    "id2word_db_corrected.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015409708023071289\n",
      "0.0001728534698486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "a = nlp(\"beach on the ocean\")\n",
    "print (time.time()-t)\n",
    "t = time.time()\n",
    "preprocess_word(a)\n",
    "print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7d729ec9cce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcorrected_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpiped_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mword2id_db_corrected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mid2word_db_corrected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m'''Sytax sugar for set()'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autodumpdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36m_autodumpdb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m'''Write/save the json dump into the file if auto_dump is enabled'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_dump\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m'''Force dump memory db to file'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         self.dthread = Thread(\n\u001b[1;32m     94\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# t = time.time()\n",
    "import sys, time\n",
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "# allkeys = list(word2id_db.getall())\n",
    "# for key in allkeys:\n",
    "#     try:\n",
    "#         word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "#         id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "#     except:\n",
    "#         print (\"Dropping \", key)\n",
    "#         word2id_db_corrected[key] = word2id_db[key]\n",
    "#         id2word_db_corrected[word2id_db[key]] = key\n",
    "# word2id_db_corrected.dump()\n",
    "# id2word_db_corrected.dump()\n",
    "\n",
    "def p(word):\n",
    "    try:\n",
    "        return preprocess_word(word)\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n",
    "        pass\n",
    "    except Exception:\n",
    "        return word.text\n",
    "# t = time.time()\n",
    "window_size = 1000000\n",
    "batches = int(len(allkeys)/window_size)\n",
    "for i in range(batches):\n",
    "    piped_words = list(nlp.pipe(allkeys[i*window_size: (i+1)*window_size]))\n",
    "    for corrected_key in piped_words:\n",
    "        word = p(corrected_key)\n",
    "        word2id_db_corrected[word] = str(idx)\n",
    "        id2word_db_corrected[str(idx)] = word\n",
    "        idx += 1\n",
    "\n",
    "# print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-94146dd7f3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mword2id_db_corrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mid2word_db_corrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autodumpdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36m_autodumpdb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m'''Write/save the json dump into the file if auto_dump is enabled'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_dump\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pickledb.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m'''Force dump memory db to file'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         self.dthread = Thread(\n\u001b[1;32m     94\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "\n",
    "for corrected_key in piped_words:\n",
    "    word = p(corrected_key)\n",
    "    word2id_db_corrected.set(word, str(idx))\n",
    "    id2word_db_corrected.set(str(idx), word)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
