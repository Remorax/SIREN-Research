{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"/data/Vivek/original/HypeNET/dataset/custom_train_0.0_0.05.tsv\"\n",
    "test_file =  \"/data/Vivek/original/HypeNET/dataset/custom_test_0.0_0.05.tsv\"\n",
    "\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/db_files/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use_unbracketed.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def to_list_mixed(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list_mixed(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list_mixed(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup, resolve=True):\n",
    "    x, y = [entity_to_id(word2id_db, elem, resolve) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset, resolve=True):\n",
    "    parsed_dicts = [parse_tuple(tup, resolve) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list_mixed(paths)), counts, targets\n",
    "\n",
    "def get_instance_key(tup):\n",
    "    return tuple([\" \".join([tok.text for tok in nlp(elem)]) for elem in tup])\n",
    "\n",
    "def parse_instance(tup):\n",
    "    \n",
    "    paths_x = list(instances_db.get(get_instance_key(tup), {}).items())\n",
    "    paths_y = list(instances_db.get(get_instance_key(tup[::-1]), {}).items())\n",
    "    path_count_dict_x = { path.replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { path.replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "paths_instances_tot = []\n",
    "def parse_instance_dataset(dataset):\n",
    "    parsed_dicts = [parse_instance(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    global paths_instances_tot\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    paths_instances_tot.append(paths)\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list_mixed(paths)), counts, targets\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity, resolve=True):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "    if not resolve:\n",
    "        return -1\n",
    "    closest_entity = resolved.get(entity, \"\")\n",
    "    if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "        success.append(entity)\n",
    "        return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Instance DB\n",
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path, maxlen):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if maxlen >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks, maxlen):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            filtered = [stringifyFilterPath(path, maxlen) for path in appendingElem]\n",
    "            paths.extend(filtered)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    try:\n",
    "        filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "        start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = noun[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        return np_filt\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n",
    "        pass\n",
    "    except Exception:\n",
    "        return noun.text\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing  ../files/dataset/security0_short.tsv\n",
      "Doing  ../files/dataset/security1_short.tsv\n",
      "Doing  ../files/dataset/security2_short.tsv\n",
      "Doing  ../files/dataset/security3_short.tsv\n",
      "Train len: 10739, Test len: 1197, Instance len: 21, Knocked len: 5538\n",
      "6869 28120\n"
     ]
    }
   ],
   "source": [
    "def to_tuple(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, list):\n",
    "            yield tuple(to_tuple(item))\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "# thresholds = [0.5, 0.59, 0.6, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
    "\n",
    "# for threshold in thresholds:\n",
    "\n",
    "threshold = 0.86\n",
    "failed, success = [], []\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "nodes_instances_original, nodes_instances_webpage, nodes_instances_hybrid = [], [], []\n",
    "paths_instances_original, paths_instances_webpage, paths_instances_hybrid = [], [], []\n",
    "counts_instances_original, counts_instances_webpage, counts_instances_hybrid = [], [], []\n",
    "targets_instances_original, targets_instances_webpage, targets_instances_hybrid = [], [], []\n",
    "\n",
    "instance_files = sorted(glob.glob(\"../files/dataset/security*_short.tsv\"))\n",
    "security_files = sorted(glob.glob(\"../files/dataset/security*.txt\"))\n",
    "\n",
    "for instance_file, security_file in list(zip(instance_files, security_files)):\n",
    "\n",
    "    print (\"Doing \", instance_file)\n",
    "    \n",
    "    test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instance_file).read().split(\"\\n\")}\n",
    "    paths_instances_old, counts_instances_old, targets_instances = parse_dataset(test_instances, False)\n",
    "    nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "    \n",
    "    doc = open(security_file).read()\n",
    "    all_nounchunks = list(nlp(doc).noun_chunks).copy()\n",
    "\n",
    "    sentences = [list(nlp(nlp(sent.text)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \")).sents)[0]\n",
    "                 for sent in nlp(doc).sents]\n",
    "    # [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    all_deps = []\n",
    "    instances_db = {}\n",
    "    for sentence in sentences:\n",
    "        noun_chunks = [n for n in all_nounchunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "        noun_chunks = list(nlp(sentence.text).noun_chunks)\n",
    "        dependencies = getDependencyPaths(sentence, nlp, noun_chunks, 10)\n",
    "        for dep in dependencies:\n",
    "            if not dep:\n",
    "                continue\n",
    "            key = tuple([preprocess_word(nlp(word)) for word in dep[:2]])\n",
    "            path = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in dep[-1].split(\"/\")])\n",
    "            if key not in instances_db:\n",
    "                instances_db[key] = [path]\n",
    "            else:\n",
    "                instances_db[key].append(path)\n",
    "    instances_db = {key: Counter(instances_db[key]) for key in instances_db}\n",
    "\n",
    "    test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instance_file).read().split(\"\\n\")}\n",
    "    paths_instances_new, counts_instances_new, targets_instances  = parse_instance_dataset(test_instances)\n",
    "    nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "    \n",
    "    paths_instances = []\n",
    "    counts_instances = []\n",
    "    \n",
    "    paths_instances_old_tup, paths_instances_new_tup = list(to_tuple(paths_instances_old)), list(to_tuple(paths_instances_new))\n",
    "    for i,(path_old, count_old) in enumerate(zip(paths_instances_old_tup, counts_instances_old)):\n",
    "        counter = Counter(dict(zip(path_old, count_old))) + Counter(dict(zip(paths_instances_new_tup[i], counts_instances_new[i])))\n",
    "        if NULL_PATH in counter and len(counter) > 1:\n",
    "            del counter[NULL_PATH]\n",
    "        paths_instances.append(list(to_list(list(counter.keys()))))\n",
    "        counts_instances.append(list(counter.values()))\n",
    "    \n",
    "    nodes_instances_original.extend(nodes_instances)\n",
    "    nodes_instances_webpage.extend(nodes_instances)\n",
    "    nodes_instances_hybrid.extend(nodes_instances)\n",
    "\n",
    "    paths_instances_original.extend(paths_instances_old)\n",
    "    paths_instances_webpage.extend(paths_instances_new)\n",
    "    paths_instances_hybrid.extend(paths_instances)\n",
    "    \n",
    "    counts_instances_original.extend(counts_instances_old)\n",
    "    counts_instances_webpage.extend(counts_instances_new)\n",
    "    counts_instances_hybrid.extend(counts_instances)\n",
    "    \n",
    "    targets_instances_original.extend(targets_instances)\n",
    "    targets_instances_webpage.extend(targets_instances)\n",
    "    targets_instances_hybrid.extend(targets_instances)\n",
    "\n",
    "print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "print (len(failed), len(success))\n",
    "emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "output_file = \"../Input/data_instances_v3.pkl\"\n",
    "f = open(output_file, \"wb+\")\n",
    "pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "             nodes_test, paths_test, counts_test, targets_test,\n",
    "             nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "             nodes_instances_original, nodes_instances_webpage, nodes_instances_hybrid,\n",
    "             paths_instances_original, paths_instances_webpage, paths_instances_hybrid,\n",
    "             counts_instances_original, counts_instances_webpage, counts_instances_hybrid,\n",
    "             targets_instances_original, targets_instances_webpage, targets_instances_hybrid,\n",
    "             emb_indexer, emb_indexer_inv, emb_vals, pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15386050939559937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "cos_sim(*extractUSEEmbeddings([\"elevators\", \"Information Security\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['security policies', '0.6106608510017400', 'TRUE'],\n",
       " ['effective security', '0.5821649432182310', 'FALSE'],\n",
       " ['cyberattacks', '0.5356408357620240', 'TRUE'],\n",
       " ['Endpoint security', '0.5318240523338320', 'TRUE'],\n",
       " ['\"real-time, security\"', '0.5170236229896550', 'FALSE'],\n",
       " ['security teams', '0.4942314326763150', 'FALSE'],\n",
       " ['cyber attack data', '0.46496352553367600', 'TRUE'],\n",
       " ['threat intelligence', '0.4527074992656710', 'TRUE'],\n",
       " ['phishing', '0.44536566734314000', 'TRUE'],\n",
       " ['ZERO-DAY PROTECTION', '0.4420897662639620', 'TRUE'],\n",
       " ['\"anti-virus, firewalls\"', '0.4211788773536680', 'TRUE'],\n",
       " ['prevention', '0.4190710783004760', 'FALSE'],\n",
       " ['data science', '0.40880563855171200', 'FALSE'],\n",
       " ['malware', '0.40299394726753200', 'TRUE'],\n",
       " ['actionable intelligence', '0.4021141827106480', 'TRUE'],\n",
       " ['risk', '0.3983878791332250', 'FALSE'],\n",
       " ['efficient security management', '0.39581161737442000', 'TRUE'],\n",
       " ['technology', '0.39303937554359400', 'FALSE'],\n",
       " ['threats', '0.3784611225128170', 'TRUE'],\n",
       " ['AI-based phishing protection', '0.37762561440467800', 'TRUE'],\n",
       " ['private threat intelligence', '0.3698093891143800', 'TRUE'],\n",
       " ['emails', '0.3601089119911190', 'FALSE'],\n",
       " ['protected assets', '0.35759925842285200', 'FALSE'],\n",
       " ['ZERO-DAY THREATS', '0.356219083070755', 'TRUE'],\n",
       " ['zero-day threats', '0.356219083070755', 'TRUE'],\n",
       " ['documents', '0.3530410826206210', 'FALSE'],\n",
       " ['zero-day phishing', '0.3381061553955080', 'TRUE'],\n",
       " ['reporting', '0.337991863489151', 'FALSE'],\n",
       " ['Compliance', '0.3358439803123470', 'FALSE'],\n",
       " ['malicious content', '0.33476781845092800', 'TRUE'],\n",
       " ['visibility', '0.33356478810310400', 'FALSE'],\n",
       " ['recipient details', '0.33215877413749700', 'FALSE'],\n",
       " ['threat extraction', '0.32998695969581600', 'TRUE'],\n",
       " ['Threat Extraction', '0.32998695969581600', 'TRUE'],\n",
       " ['confidence', '0.32803675532341000', 'FALSE'],\n",
       " ['web', '0.3257787823677060', 'FALSE'],\n",
       " ['email', '0.3224017322063450', 'FALSE'],\n",
       " ['investigation', '0.3206564784049990', 'FALSE'],\n",
       " ['policies', '0.3202439546585080', 'FALSE'],\n",
       " ['fact', '0.3198792636394500', 'FALSE'],\n",
       " ['business', '0.3183532953262330', 'FALSE'],\n",
       " ['files', '0.31647178530693100', 'FALSE'],\n",
       " ['network', '0.3130895495414730', 'FALSE'],\n",
       " ['malicious files', '0.31177037954330400', 'TRUE'],\n",
       " ['policy updates', '0.3094674348831180', 'FALSE'],\n",
       " ['Click-Time Protection', '0.3077496886253360', 'TRUE'],\n",
       " ['defense', '0.3069821298122410', 'FALSE'],\n",
       " ['bandwidth', '0.3036783039569860', 'FALSE'],\n",
       " ['administration', '0.3031686842441560', 'FALSE'],\n",
       " ['best zero-day protection', '0.3028203845024110', 'FALSE'],\n",
       " ['powerful zero-day protection', '0.3016890585422520', 'FALSE'],\n",
       " ['networks', '0.30022692680358900', 'FALSE'],\n",
       " ['latest threats', '0.2976371645927430', 'FALSE'],\n",
       " ['Pre-emptive User Protections', '0.2960294187068940', 'FALSE'],\n",
       " ['pre-emptive user protections', '0.2960294187068940', 'TRUE'],\n",
       " ['incoming email', '0.29477807879447900', 'FALSE'],\n",
       " ['malware strains', '0.29367467761039700', 'TRUE'],\n",
       " ['instant alerts', '0.2926754355430600', 'FALSE'],\n",
       " ['best2 zero-day protection', '0.29220011830329900', 'FALSE'],\n",
       " ['newest threats', '0.29156845808029200', 'FALSE'],\n",
       " ['parameters', '0.29092279076576200', 'FALSE'],\n",
       " ['best threat intelligence', '0.2897984981536870', 'FALSE'],\n",
       " ['suspicious files', '0.2884351313114170', 'FALSE'],\n",
       " ['processing', '0.28792068362236000', 'FALSE'],\n",
       " ['background', '0.2863217890262600', 'FALSE'],\n",
       " ['email messages', '0.2852173149585720', 'FALSE'],\n",
       " ['web downloads', '0.2839469909667970', 'FALSE'],\n",
       " ['advantages', '0.28310295939445500', 'FALSE'],\n",
       " ['COMMON NETWORK SECURITY APPROACHES', '0.28209683299064600', 'FALSE'],\n",
       " ['zero-day malware', '0.2813718318939210', 'TRUE'],\n",
       " ['zero-day phishing and social engineering attacks',\n",
       "  '0.27986598014831500',\n",
       "  'TRUE'],\n",
       " ['practical prevention', '0.2787751257419590', 'FALSE'],\n",
       " ['executables', '0.27826255559921300', 'FALSE'],\n",
       " ['advanced email protections', '0.2762841284275060', 'TRUE'],\n",
       " ['enterprise IoT', '0.2761740982532500', 'TRUE'],\n",
       " ['social engineering', '0.2761363089084630', 'TRUE'],\n",
       " ['industry', '0.275949090719223', 'FALSE'],\n",
       " ['other core security solutions', '0.2675186097621920', 'FALSE'],\n",
       " ['damage', '0.26526179909706100', 'FALSE'],\n",
       " ['user activity', '0.26444998383522000', 'FALSE'],\n",
       " ['LIMITATIONS', '0.2633727788925170', 'FALSE'],\n",
       " ['sender', '0.26336541771888700', 'FALSE'],\n",
       " ['antiphishing', '0.2631893754005430', 'TRUE'],\n",
       " ['AI-based fraud protection', '0.26253971457481400', 'TRUE'],\n",
       " ['malware families', '0.26061496138572700', 'TRUE'],\n",
       " ['power', '0.25986790657043500', 'FALSE'],\n",
       " ['Trojans', '0.25737014412879900', 'TRUE'],\n",
       " ['analysis', '0.2573162615299230', 'FALSE'],\n",
       " ['cameras', '0.255668967962265', 'TRUE'],\n",
       " ['four billion security decisions', '0.2501031756401060', 'FALSE'],\n",
       " ['malicious links', '0.2494836449623110', 'TRUE'],\n",
       " ['users', '0.24935637414455400', 'FALSE'],\n",
       " ['aspects', '0.24759478867054000', 'FALSE'],\n",
       " ['email links', '0.2461477816104890', 'FALSE'],\n",
       " ['unknown ransomware', '0.24590842425823200', 'TRUE'],\n",
       " ['NUMBER', '0.24585597217083000', 'FALSE'],\n",
       " ['Network settings', '0.24528300762176500', 'FALSE'],\n",
       " ['Point', '0.24236752092838300', 'FALSE'],\n",
       " ['URL reputation', '0.2412734031677250', 'TRUE'],\n",
       " ['’ mailbox', '0.2399427890777590', 'FALSE'],\n",
       " ['forensic reports', '0.2391592562198640', 'FALSE'],\n",
       " ['cost-effective zero-day protection strategy',\n",
       "  '0.2388201206922530',\n",
       "  'FALSE'],\n",
       " ['clicks', '0.23822426795959500', 'FALSE'],\n",
       " ['perimeter', '0.2382020503282550', 'FALSE'],\n",
       " ['browsing', '0.23763304948806800', 'FALSE'],\n",
       " ['auto-updated threat prevention engines', '0.23736993968486800', 'TRUE'],\n",
       " ['zero day protection', '0.23708999156951900', 'TRUE'],\n",
       " ['email attachments', '0.2346426397562030', 'FALSE'],\n",
       " ['compliance stance', '0.23447178304195400', 'FALSE'],\n",
       " ['network perimeter', '0.23361371457576800', 'FALSE'],\n",
       " ['combination', '0.23160788416862500', 'FALSE'],\n",
       " ['datacenters', '0.22976014018058800', 'FALSE'],\n",
       " ['guest network', '0.2282191962003710', 'FALSE'],\n",
       " ['organizations', '0.2266017496585850', 'FALSE'],\n",
       " ['box', '0.22581519186496700', 'FALSE'],\n",
       " ['network segment', '0.223292738199234', 'FALSE'],\n",
       " ['remediation', '0.21984899044036900', 'FALSE'],\n",
       " ['agility', '0.21962480247020700', 'FALSE'],\n",
       " ['current email', '0.21951907873153700', 'FALSE'],\n",
       " ['incident response', '0.21689899265766100', 'FALSE'],\n",
       " ['It', '0.21655704081058500', 'FALSE'],\n",
       " ['SOC infrastructure', '0.21534012258052800', 'FALSE'],\n",
       " ['Threat Emulation', '0.21043913066387200', 'TRUE'],\n",
       " ['links', '0.20958156883716600', 'FALSE'],\n",
       " ['IOCs', '0.20885303616523700', 'FALSE'],\n",
       " ['moment', '0.2087678462266920', 'FALSE'],\n",
       " ['malware evasion techniques', '0.20818421244621300', 'TRUE'],\n",
       " ['endpoints', '0.20800012350082400', 'FALSE'],\n",
       " ['need', '0.20320145785808600', 'FALSE'],\n",
       " ['\"8,3001 new, previously undiscovered cyber attacks\"',\n",
       "  '0.20226575434207900',\n",
       "  'FALSE'],\n",
       " ['email and web downloads', '0.2016180455684660', 'FALSE'],\n",
       " ['compromise', '0.20051369071006800', 'FALSE'],\n",
       " ['policy changes', '0.19993430376052900', 'FALSE'],\n",
       " ['time', '0.1996004730463030', 'FALSE'],\n",
       " ['PC', '0.19955776631832100', 'FALSE'],\n",
       " ['cloud', '0.19922921061515800', 'FALSE'],\n",
       " ['AI heuristics', '0.19825002551078800', 'FALSE'],\n",
       " ['enterprise', '0.19806846976280200', 'FALSE'],\n",
       " ['only zero-day protection solution', '0.19508710503578200', 'FALSE'],\n",
       " ['world', '0.19142311811447100', 'FALSE'],\n",
       " ['full visibility', '0.19017621874809300', 'FALSE'],\n",
       " ['enterprise users', '0.18955598771572100', 'FALSE'],\n",
       " ['log server', '0.18782280385494200', 'FALSE'],\n",
       " ['broad array', '0.18662501871585800', 'FALSE'],\n",
       " ['NLP', '0.18641237914562200', 'FALSE'],\n",
       " ['CHECK POINT', '0.18580739200115200', 'FALSE'],\n",
       " ['Check Point', '0.18580739200115200', 'FALSE'],\n",
       " ['view', '0.18539154529571500', 'FALSE'],\n",
       " ['logs', '0.18087202310562100', 'FALSE'],\n",
       " ['file types', '0.18067403137683900', 'FALSE'],\n",
       " ['text', '0.17969271540641800', 'FALSE'],\n",
       " ['devastating attacks', '0.177400603890419', 'FALSE'],\n",
       " ['wild', '0.17582464218139600', 'FALSE'],\n",
       " ['Mac devices', '0.17499683797359500', 'FALSE'],\n",
       " ['audit-ready reports', '0.16808652877807600', 'FALSE'],\n",
       " ['current infrastructure', '0.16736270487308500', 'FALSE'],\n",
       " ['they', '0.16680006682872800', 'FALSE'],\n",
       " ['best AV solutions', '0.16619740426540400', 'FALSE'],\n",
       " ['first strategy', '0.16489620506763500', 'FALSE'],\n",
       " ['latest features', '0.16409289836883500', 'FALSE'],\n",
       " ['default', '0.16077132523059800', 'FALSE'],\n",
       " ['ThreatCloud – Dynamic Threat Intelligence Repository',\n",
       "  '0.1601058393716810',\n",
       "  'FALSE'],\n",
       " ['end', '0.15968407690525100', 'FALSE'],\n",
       " ['MITRE', '0.15937627851963000', 'FALSE'],\n",
       " ['advanced network forensics', '0.1591525822877880', 'FALSE'],\n",
       " ['BEC', '0.15833283960819200', 'FALSE'],\n",
       " ['vast network', '0.15822523832321200', 'FALSE'],\n",
       " ['record speed', '0.15702353417873400', 'FALSE'],\n",
       " ['SIEM', '0.15591683983802800', 'FALSE'],\n",
       " ['what', '0.1552528440952300', 'FALSE'],\n",
       " ['HVAC systems', '0.15430355072021500', 'TRUE'],\n",
       " ['elevators', '0.15386047959327700', 'TRUE'],\n",
       " ['other email-based threats', '0.15248465538024900', 'FALSE'],\n",
       " ['government-grade compliance', '0.1518973857164380', 'FALSE'],\n",
       " ['advanced forensics', '0.15147686004638700', 'FALSE'],\n",
       " ['you', '0.1469598263502120', 'FALSE'],\n",
       " ['YOU', '0.1469598263502120', 'FALSE'],\n",
       " ['nation states', '0.14518707990646400', 'FALSE'],\n",
       " ['ThreatCloud', '0.14327000081539200', 'TRUE'],\n",
       " ['e.g. data center', '0.14320185780525200', 'FALSE'],\n",
       " ['broad range', '0.14275839924812300', 'FALSE'],\n",
       " ['SandBlast Threat Extraction', '0.14238208532333400', 'TRUE'],\n",
       " ['gold standard', '0.14023809134960200', 'FALSE'],\n",
       " ['millions', '0.13791760802269000', 'FALSE'],\n",
       " ['multiple attack vectors', '0.13781219720840500', 'FALSE'],\n",
       " ['business productivity', '0.13540171086788200', 'FALSE'],\n",
       " ['half', '0.1351367086172100', 'FALSE'],\n",
       " ['emulation', '0.13332435488700900', 'FALSE'],\n",
       " ['SOC Workflows', '0.1297955960035320', 'FALSE'],\n",
       " ['internal network etc', '0.12934370338916800', 'FALSE'],\n",
       " ['regulatory mandates', '0.12695714831352200', 'FALSE'],\n",
       " ['evasive attacks', '0.1255146861076360', 'FALSE'],\n",
       " ['Point ThreatCloud', '0.12231909483671200', 'FALSE'],\n",
       " ['only line', '0.12062881141901000', 'FALSE'],\n",
       " ['1.5 seconds', '0.11830376833677300', 'FALSE'],\n",
       " ['real time', '0.1168893352150920', 'FALSE'],\n",
       " ['specific network segment', '0.11192964017391200', 'FALSE'],\n",
       " ['associated file signatures', '0.10857092589139900', 'FALSE'],\n",
       " ['OS-level inspection', '0.10786625742912300', 'FALSE'],\n",
       " ['SANDBLAST NETWORK', '0.10331163555383700', 'TRUE'],\n",
       " ['runtime behaviors', '0.1006123423576360', 'FALSE'],\n",
       " ['world’s best malware catch rate', '0.10018744319677400', 'FALSE'],\n",
       " ['unacceptable delays', '0.09976793825626370', 'FALSE'],\n",
       " ['actionable configuration guidelines', '0.09857267886400220', 'FALSE'],\n",
       " ['evasion-resistant threat emulation', '0.09749031811952590', 'FALSE'],\n",
       " ['Top notch research', '0.09699559956789020', 'FALSE'],\n",
       " ['detection-first strategy', '0.094681017100811', 'FALSE'],\n",
       " ['such critical limitations', '0.0931977927684784', 'FALSE'],\n",
       " ['revolutionary AI', '0.09270374476909640', 'FALSE'],\n",
       " ['94%', '0.09178472310304640', 'FALSE'],\n",
       " ['\"blazing-speed, AI-generated Threat Emulation verdicts\"',\n",
       "  '0.09111746400594710',\n",
       "  'FALSE'],\n",
       " ['R80 console', '0.08973774313926700', 'FALSE'],\n",
       " ['largest repository', '0.0878528282046318', 'FALSE'],\n",
       " ['Current SIEM', '0.08781061321496960', 'FALSE'],\n",
       " ['AI-Generated Threat Emulation Verdicts', '0.0808173194527626', 'FALSE'],\n",
       " ['browsing workflows', '0.08056822419166570', 'FALSE'],\n",
       " ['prevention-first strategy', '0.08055651932954790', 'FALSE'],\n",
       " ['breakthrough AI engines', '0.07833743095397950', 'FALSE'],\n",
       " ['up to 70%', '0.07581987977027890', 'FALSE'],\n",
       " ['dedicated servers', '0.07450313121080400', 'FALSE'],\n",
       " ['optimal network performance', '0.07144876569509510', 'FALSE'],\n",
       " ['rules-based engines', '0.06826262176036840', 'FALSE'],\n",
       " ['non-AI engines', '0.06611663848161700', 'FALSE'],\n",
       " ['Natural Language Processing', '0.06582783162593840', 'FALSE'],\n",
       " ['automated policy configuration', '0.06435248255729680', 'FALSE'],\n",
       " ['multiple innovative technologies', '0.05776631087064740', 'FALSE'],\n",
       " ['existing indicators', '0.05625474825501440', 'FALSE'],\n",
       " ['several approaches', '0.048341263085603700', 'FALSE'],\n",
       " ['exhaustive AI engines', '0.046918049454689000', 'FALSE'],\n",
       " ['over 300 parameters', '0.046039994806051300', 'FALSE'],\n",
       " ['worldwide breaches3', '0.04545942321419720', 'FALSE'],\n",
       " ['integrated document and image sanitization solution',\n",
       "  '0.04468446969985960',\n",
       "  'FALSE'],\n",
       " ['Conventional sandboxing solutions', '0.03863333538174630', 'FALSE'],\n",
       " ['Hundreds of millions', '0.0341552197933197', 'FALSE'],\n",
       " ['uninterrupted business flow', '0.03404047712683680', 'FALSE'],\n",
       " ['true prevention-first strategy', '0.02740487828850750', 'FALSE'],\n",
       " ['SandBlast Network SIEM platform', '0.015882128849625600', 'FALSE'],\n",
       " ['revolutionary AI engines', '0.014436127617955200', 'FALSE'],\n",
       " ['best practice policies', '0.011685686185956000', 'FALSE'],\n",
       " ['rich rule-based engines', '0.008992294780910020', 'FALSE'],\n",
       " ['INTUITIVE MANAGEMENT SandBlast Network', '0.008608808740973470', 'FALSE'],\n",
       " ['Check Point Research Labs', '0.006936517544090750', 'FALSE'],\n",
       " ['broad integration ecosystem', '0.003819325240328910', 'FALSE'],\n",
       " ['SMOOTH BUSINESS PRODUCTIVITY', '-0.014359516091644800', 'FALSE'],\n",
       " ['traditional sandboxes', '-0.019048547372221900', 'FALSE'],\n",
       " ['single click setup4', '-0.021812399849295600', 'FALSE'],\n",
       " ['Easy-to-use best practices', '-0.03438681364059450', 'FALSE'],\n",
       " ['\"numerous innovative, proprietary technologies\"',\n",
       "  '-0.044883064925670600',\n",
       "  'FALSE'],\n",
       " ['deep CPU-level emulation', '-0.06624933332204820', 'FALSE'],\n",
       " ['single conclusive AI-generated verdict', '-0.08466362953186040', 'FALSE'],\n",
       " ['clean and reconstructed versions', '-0.09125840663909910', 'FALSE']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "entities = list(set(flatten([line.split(\"\\t\")[:2] for line in open(\"../Outputs/Output_instances_Instances1 (hybrid).tsv\").read().split(\"\\n\")])))\n",
    "\n",
    "embeds = extractUSEEmbeddings([\"Information security\"] + entities)\n",
    "# open(\"../files/dataset/webpage_terms.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Windows Windows': False,\n",
       " 'buffer size': False,\n",
       " 'ways': False,\n",
       " '/write': False,\n",
       " 'understanding': False,\n",
       " 'allocation': False,\n",
       " 'more-than-adequate description': False,\n",
       " 'Microsoft': False,\n",
       " 'third-party CMMs': False,\n",
       " 'stream': False,\n",
       " 'ships': False,\n",
       " 'pattern': False,\n",
       " 'Corpus': False,\n",
       " '16-core machine': False,\n",
       " 'color space': False,\n",
       " 'Crashes': False,\n",
       " 'transformations': False,\n",
       " 'modern operating system': False,\n",
       " 'LCMS': False,\n",
       " 'Project Zero and ZDI researchers': False,\n",
       " 'internet': True,\n",
       " 'dictionary': False,\n",
       " 'itself': False,\n",
       " 'supported Windows APIs': False,\n",
       " 'file': False,\n",
       " 'integer overflow': False,\n",
       " 'attackers': True,\n",
       " 'ICC specification': False,\n",
       " 'element': False,\n",
       " 'feedback': False,\n",
       " 'ICC color profiles': False,\n",
       " 'something': False,\n",
       " 'old module': False,\n",
       " 'different types': False,\n",
       " 'ICC Color Profile': False,\n",
       " 'MSDN': False,\n",
       " 'multiple bugs': False,\n",
       " 'Color Management': False,\n",
       " 'fuzzing dictionaries': False,\n",
       " 'Adobe’s CMM': False,\n",
       " 'OS': False,\n",
       " 'information': True,\n",
       " 'color transformations': False,\n",
       " 'device source': False,\n",
       " 'number': False,\n",
       " 'uninitialized memory': False,\n",
       " 'file format': False,\n",
       " 'new vulnerability': True,\n",
       " 'information disclosure': True,\n",
       " 'ICC profile': False,\n",
       " 'small module': False,\n",
       " 'good corpus': False,\n",
       " 'ICC': False,\n",
       " 'it': False,\n",
       " 'set': False,\n",
       " 'loading': False,\n",
       " 'XML file format': False,\n",
       " 'multiple rounds': False,\n",
       " 'viewing requirement': False,\n",
       " 'ICM32': False,\n",
       " 'me': False,\n",
       " 'other main source': False,\n",
       " 'OpenColorProfile': False,\n",
       " 'multiple vulnerabilities': False,\n",
       " 'EXEs': False,\n",
       " 'previous vulnerability': False,\n",
       " 'third-party software': False,\n",
       " 'color input or output device': False,\n",
       " 'complete content': False,\n",
       " 'hopes': False,\n",
       " 'color profile': False,\n",
       " 'Image parsing': False,\n",
       " 'specifications': False,\n",
       " 'offset': False,\n",
       " 'ncl2’ elements': False,\n",
       " 'same call': False,\n",
       " 'we': False,\n",
       " 'offsets': False,\n",
       " 'couple': False,\n",
       " 'PCS': False,\n",
       " 'ICC related operations': False,\n",
       " 'buffer': False,\n",
       " 'mapping': False,\n",
       " 'math': False,\n",
       " 'I': False,\n",
       " 'CIELAB': False,\n",
       " 'other two functions': False,\n",
       " 'ICC color profile': False,\n",
       " 'third-party': False,\n",
       " 'same code': False,\n",
       " 'DLL': False,\n",
       " 'Additional Vulnerabilities': True,\n",
       " 'interpolation': False,\n",
       " 'function': False,\n",
       " 'CMConvIndexToNameProfile': False,\n",
       " 'CMM': False,\n",
       " 'error': False,\n",
       " 'ICM32.dll': False,\n",
       " '3 byte': False,\n",
       " 'additional code paths': False,\n",
       " 'fix': False,\n",
       " 'fuzzing': False,\n",
       " 'related file formats': False,\n",
       " 'multiple color profiles': False,\n",
       " 'allocBuffer': False,\n",
       " 'series': False,\n",
       " 'Microsoft’s Color Management Module': False,\n",
       " 'Mappings': False,\n",
       " 'open source fuzzing attempts': False,\n",
       " 'many image files': False,\n",
       " 'three vulnerabilities': False,\n",
       " 'International Color Consortium': False,\n",
       " 'small set': False,\n",
       " 'History': False,\n",
       " 'remote code execution': False,\n",
       " 'fuzzer': False,\n",
       " 'API sequence': False,\n",
       " 'Dictionary Sites': False,\n",
       " 'Code coverage information': True,\n",
       " 'Little CMS': False,\n",
       " 'code': False,\n",
       " 'Image Color Management (ICM) version': False,\n",
       " 'open source Color Management Systems': False,\n",
       " 'code coverage information': True,\n",
       " 'unchecked offset access': False,\n",
       " 'Windows binaries': False,\n",
       " 'Wikipedia': False,\n",
       " 'days': False,\n",
       " 'part': False,\n",
       " 'Microsoft’s CMM': False,\n",
       " 'XML': False,\n",
       " 'APIs': False,\n",
       " 'security fixes': True,\n",
       " 'ncl2’ tag/element': False,\n",
       " 'external researchers': False,\n",
       " 'color profile APIs': False,\n",
       " 'seven different color profiles': False,\n",
       " 'Windows 95 era': False,\n",
       " 'list': False,\n",
       " 'programming/tools': False,\n",
       " 'MSCMS.dll': False,\n",
       " 'one': False,\n",
       " 'color management': False,\n",
       " 'exported and documented MSCMS functions': False,\n",
       " 'unconstrained size': False,\n",
       " 'Windows OS’ built-in image parsers': False,\n",
       " 'programming/tools color profile': False,\n",
       " 'bugs': False,\n",
       " 'WCS color profiles': False,\n",
       " 'disassembly': False,\n",
       " 'binary files': False,\n",
       " 'corpuses': False,\n",
       " 'Part': False,\n",
       " 'functions': False,\n",
       " 'accessible attack surface': False,\n",
       " 'simpler terms': False,\n",
       " 'content': True,\n",
       " 'Windows Vista': False,\n",
       " 'software processes': False,\n",
       " '100 pages': False,\n",
       " 'functionality': False,\n",
       " 'to stand-alone files': False,\n",
       " 'dictionaries': False,\n",
       " 'ICM': False,\n",
       " 'One': False,\n",
       " 'size': False,\n",
       " 'bug': False,\n",
       " 'sort': False,\n",
       " 'such instances': False,\n",
       " 'vulnerabilities': True,\n",
       " 'first set': False,\n",
       " 'blog post': False,\n",
       " 'us': False,\n",
       " 'specification': False,\n",
       " 'generic entry point': False,\n",
       " 'buffer pointer': False,\n",
       " 'color transformation': False,\n",
       " 'named APIs': False,\n",
       " 'system32 directory': False,\n",
       " 'hope': False,\n",
       " 'recent vulnerabilities': False,\n",
       " 'feature': False,\n",
       " 'parameters': False,\n",
       " 'internal product security teams': False,\n",
       " 'vulnerability': True,\n",
       " 'binary file': False,\n",
       " 'particular device': False,\n",
       " 'values': True,\n",
       " 'bound read': False,\n",
       " 'Windows Color System': False,\n",
       " 'minimum length check': False,\n",
       " 'approach': False,\n",
       " 'What': False,\n",
       " 'none': False,\n",
       " 'CMGetPartialProfileElement function': False,\n",
       " 'one sample': False,\n",
       " 'multi-part blog series': False,\n",
       " 'L*a*b': False,\n",
       " 'unique tag names': False,\n",
       " 'blog series': False,\n",
       " 'Color Profiles Image': False,\n",
       " 'harness': False,\n",
       " 'standards': False,\n",
       " 'Profiles': False,\n",
       " 'GetNamedProfileInfo': False,\n",
       " 'major overhaul': False,\n",
       " 'fact': False,\n",
       " 'better understanding': False,\n",
       " 'image format': False,\n",
       " 'certain degree': False,\n",
       " 'corpus': False,\n",
       " 'vulnerability class': False,\n",
       " 'color attributes': False,\n",
       " 'data': True,\n",
       " 'Windows DLLs': False,\n",
       " 'verification': False,\n",
       " 'file format internals': False,\n",
       " 'three instances': False,\n",
       " 'images': False,\n",
       " 'version': False,\n",
       " 'basic features': False,\n",
       " 'tables': False,\n",
       " 'decades': False,\n",
       " 'count': False,\n",
       " 'Windows': False,\n",
       " 'review': False,\n",
       " 'color profiles': False,\n",
       " 'WCS': False,\n",
       " 'relative silence': False,\n",
       " 'ICM APIs': False,\n",
       " 'profile connection space': False}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1217"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "all_lines = [l.split(\"\\t\") for l in open(\"../Outputs/Output_instances_Instances4 (hybrid).tsv\").read().split(\"\\n\")]\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "entities = list(set(flatten([line[:2] for line in all_lines])))\n",
    "\n",
    "embeds = extractUSEEmbeddings([\"Information security\"] + entities)\n",
    "# open(\"../files/dataset/webpage_terms.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}\n",
    "\n",
    "def get(key, dictionary):\n",
    "    try:\n",
    "        return dictionary[key]\n",
    "    except KeyboardInterrupt as e:\n",
    "        sys.exit()\n",
    "    except:\n",
    "        print (key)\n",
    "        dictionary_lower = {elem.lower(): dictionary[elem] for elem in dictionary}\n",
    "        return dictionary_lower[key.lower()]\n",
    "    \n",
    "filtered_lines = []\n",
    "for elem in all_lines:\n",
    "    try:\n",
    "        if get(elem[0], scores_dict) and get(elem[1], scores_dict):\n",
    "            filtered_lines.append(elem)\n",
    "    except:\n",
    "        print (elem)\n",
    "open(\"../Lalit-results/keysight.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in [[\"a\",\"b\",\"pred\",\"label\"]] + [elem for elem in filtered_lines if elem[-2]!=\"none\"]]))\n",
    "\n",
    "# [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "# precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_added = [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "precision = len([elem for elem in to_be_added if elem[-1]==elem[-2]])/len(to_be_added)\n",
    "\n",
    "all_gt_pairs_in_doc = [tuple(elem[:2]) for elem in all_lines if elem[-1]!=\"none\"]\n",
    "recall = len([elem for elem in to_be_added if elem[-1]==elem[-2] and tuple(elem[:2]) in all_gt_pairs_in_doc])/len([elem for elem in to_be_added if elem[-1]==elem[-2]])\n",
    "2*precision*recall / (precision + recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"../files/dataset/security1_short.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(l.split(\"\\t\")[:2] + l.split(\"\\t\")[3:]) for l in open(\"../Lalit-results/hyperbox.tsv\").read().split(\"\\n\")[1:]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lines = [line.strip().split(\"\\t\") for line in open(\"../files/dataset/webpage_terms.tsv\", \"r\").read().split(\"\\n\")]\n",
    "scores = [float(elem[1]) for elem in lines]\n",
    "min_score, max_score = round(min(scores), 4), round(max(scores), 4)\n",
    "gt = [True if elem[2]==\"TRUE\" else False for elem in lines]\n",
    "opt_accuracy = -100\n",
    "opt_threshold = -100\n",
    "for threshold in np.arange(min_score, max_score, 0.0001):\n",
    "    pred = [True if float(elem[1])>threshold else False for elem in lines]\n",
    "#     pred = [elem for (i,elem) in enumerate(pred) if not gt[i]]\n",
    "    accuracy = accuracy_score(gt, pred)\n",
    "    if accuracy > opt_accuracy:\n",
    "        opt_accuracy = accuracy\n",
    "        opt_threshold = threshold\n",
    "scores_dict = dict([elem[:2] for elem in lines])\n",
    "scores_dict = {elem: float(scores_dict[elem]) > opt_threshold for elem in scores_dict}\n",
    "\n",
    "# opt_threshold, opt_accuracy\n",
    "\n",
    "# [elem for elem in lines if]\n",
    "# 0.0001\n",
    "# min_score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Access Control Regulation Control Compliant Organization',\n",
       " 'Access Control Regulation Policy',\n",
       " 'Access Log Book',\n",
       " 'Access Regulation Control Compliant Building',\n",
       " 'Access System',\n",
       " 'Access System A',\n",
       " 'Access System B',\n",
       " 'Access System C',\n",
       " 'Activities Monitoring Software',\n",
       " 'Air Condition System',\n",
       " 'Air Condition System A',\n",
       " 'Air Condition System B',\n",
       " 'Air Conditioning Control Compliant Server Room',\n",
       " 'Alarm System',\n",
       " 'Alarm System A',\n",
       " 'Alarm System B',\n",
       " 'Alarm System C',\n",
       " 'Anti Virus Control Compliant Mobile Device',\n",
       " 'Antivirus Software Control Compliant Computer',\n",
       " 'Application Evaluation Control Compliant Organization',\n",
       " 'Application Evaluation Policy',\n",
       " 'Appropriate Contacts Control Compliant Organization',\n",
       " 'Appropriate Contacts Policy',\n",
       " 'Asset',\n",
       " 'Asset Importance',\n",
       " 'Asset Threat Probability',\n",
       " 'Assets Control Compliant Organization',\n",
       " 'Assets Control Policy',\n",
       " 'Attacker Profile',\n",
       " 'Attribute',\n",
       " 'Audit Logging And Monitoring Control Compliant Organization',\n",
       " 'Audit Logging And Monitoring Policy',\n",
       " 'Automatic Locking Control Compliant Computer',\n",
       " 'Automatic Locking Control Compliant Mobile Device',\n",
       " 'Automatic Locking Software',\n",
       " 'Backup Media',\n",
       " 'Backup Software',\n",
       " 'Backup Storage In House Control Compliant Backup Media',\n",
       " 'Backup Storage Outside Control Compliant Backup Media',\n",
       " 'Bared Window',\n",
       " 'Bared Window Control Compliant Section',\n",
       " 'Boolean Scale',\n",
       " 'Building',\n",
       " 'Business Continuity Management Process Control Compliant Organization',\n",
       " 'Business Continuity Policy',\n",
       " 'CD And DVD',\n",
       " 'CO2Fire Extinguisher',\n",
       " 'Cabling Security Control Compliant Building',\n",
       " 'Cabling Security Policy',\n",
       " 'Change Control Procedures Control Compliant Organization',\n",
       " 'Change Control Procedures Policy',\n",
       " 'Checklist',\n",
       " 'Clear Desk Control Compliant Organization',\n",
       " 'Clear Desk Policy',\n",
       " 'Clear Desk Policy A',\n",
       " 'Clear Desk Policy B',\n",
       " 'Clock Synchronization Control Compliant Computer',\n",
       " 'Clock Synchronization Software',\n",
       " 'Cloud Computing Control Compliant Organization',\n",
       " 'Cloud Computing Policy',\n",
       " 'Cloud Service Contract',\n",
       " 'Complex Firewall',\n",
       " 'Compliance Control Compliance Organization',\n",
       " 'Compliance Policy',\n",
       " 'Compliant Control',\n",
       " 'Computer',\n",
       " 'Configuration Data Control Compliant Organization',\n",
       " 'Configuration Data Digital',\n",
       " 'Configuration Data Hard Copy',\n",
       " 'Contract',\n",
       " 'Control',\n",
       " 'Control Implementation Effectiveness',\n",
       " 'Control Type',\n",
       " 'Data',\n",
       " 'Data Archiving Control Compliant Organization',\n",
       " 'Data Archiving Policy',\n",
       " 'Data Backup Control Compliant Organization',\n",
       " 'Data Backup Policy',\n",
       " 'Data Backup Policy A',\n",
       " 'Data Backup Policy B',\n",
       " 'Data Backup Policy C',\n",
       " 'Data Backup Storage Control Compliant Organization',\n",
       " 'Data Backup Storage Policy',\n",
       " 'Data Backup Storage Policy A',\n",
       " 'Data Backup Storage Policy B',\n",
       " 'Data Backup Storage Policy C',\n",
       " 'Data Backup Strategyand Backup Media Control Compliant Organization',\n",
       " 'Data Breach Notification Control Compliant Organization',\n",
       " 'Data Breach Notification Policy',\n",
       " 'Data Check Software',\n",
       " 'Data Deletion Software',\n",
       " 'Data Disposal Control Compliant Organization',\n",
       " 'Data Disposal Policy',\n",
       " 'Data Encryption Policy',\n",
       " 'Data Encryption Software Control Compliant Computer',\n",
       " 'Data Secrecy Control Compliant Organization',\n",
       " 'Data Secrecy Policy',\n",
       " 'Data Security Handbook Control Compliant Organization',\n",
       " 'Delivery And Loading Areas Control Compliant Building',\n",
       " 'Delivery Area',\n",
       " 'Disciplinary Process Control Compliant Organization',\n",
       " 'Disciplinary Process Policy',\n",
       " 'Document',\n",
       " 'Documented Operational Procedures Control Compliant Organization',\n",
       " 'Door',\n",
       " 'Electric Installation Control Compliant Organization',\n",
       " 'Electric Installation Policy',\n",
       " 'Electronic Commerce Services Control Compliant Organization',\n",
       " 'Electronic Messaging Protection Control Compliant Organization',\n",
       " 'Emergency Planning Control Compliant Organization',\n",
       " 'Emergency Planning Policy',\n",
       " 'Emergency Recovery Control Compliant Organization',\n",
       " 'Emergency Recovery Policy',\n",
       " 'Emergency Substitute Device',\n",
       " 'Employment Contract',\n",
       " 'Encryption Of Confidental Data Policy',\n",
       " 'Encryption Of Confidential Data Control Compliant Organization',\n",
       " 'Encryption Software',\n",
       " 'Entry Checkpoint',\n",
       " 'Entry Checkpoint A',\n",
       " 'Entry Checkpoint B',\n",
       " 'Environmental Conditions Monitoring Control Compliant Organization',\n",
       " 'Environmental Conditions Monitoring Control Compliant Server Room',\n",
       " 'Environmental Conditions Monitoring Software',\n",
       " 'Equipment Identification Control Compliant Computer',\n",
       " 'Equipment Location Control Compliant Building',\n",
       " 'Equipment Sitting And Protection Policy',\n",
       " 'Exchange Agreements Control Compliant Organization',\n",
       " 'Exploitation:Degree:Relation',\n",
       " 'External Connections Authentication Control Compliant Organization',\n",
       " 'External Connections Policy',\n",
       " 'External Employment Contract',\n",
       " 'External Role',\n",
       " 'External Staff Control Compliant Organization',\n",
       " 'External Staff Policy',\n",
       " 'Fire Extinguisher',\n",
       " 'Fire Extinguisher A',\n",
       " 'Fire Extinguisher B',\n",
       " 'Fire Extinguisher Control Compliant Section',\n",
       " 'Fire Extinguisher Control Compliant Server Room',\n",
       " 'Fire Safety And Training Policy',\n",
       " 'Fire Safety Control Compliant Organization',\n",
       " 'Fire Suppression Control Compliant Section',\n",
       " 'Fire Suppression System',\n",
       " 'Fire Suppression System A',\n",
       " 'Fire Suppression System B',\n",
       " 'Firewall',\n",
       " 'Firewall A',\n",
       " 'Firewall B',\n",
       " 'Firewall Control Compliant Computer',\n",
       " 'Firewall Policy',\n",
       " 'Firewall Regulation Control Compliant Organization',\n",
       " 'Human Behavior Guidelines Control Compliant Control Organization',\n",
       " 'Human Resource Development Control Compliant Organization',\n",
       " 'Human Resource Development Policy',\n",
       " 'Humidity Surveillance System',\n",
       " 'ISO27001',\n",
       " 'IT Component',\n",
       " 'Identification Software',\n",
       " 'Immovable Asset',\n",
       " 'Information Passing Control Compliant Organization',\n",
       " 'Information Passing Policy',\n",
       " 'Information Security Independent Review Control Compliant Organization',\n",
       " 'Information Security Independent Review Policy',\n",
       " 'Information Security Policy',\n",
       " 'Information Security Policy Control Compliant Organization',\n",
       " 'Information Security Risk Awareness Control Compliant Organization',\n",
       " 'Installation Media',\n",
       " 'Intangibe Asset',\n",
       " 'Internal Data',\n",
       " 'Internal Role',\n",
       " 'Internet Filter',\n",
       " 'Internet Filter Control Compliant Computer',\n",
       " 'Internet Gateway',\n",
       " 'Internet Regulation Control Compliant Organization',\n",
       " 'Internet Regulation Policy',\n",
       " 'Internet Regulation Policy A',\n",
       " 'Internet Regulation Policy B',\n",
       " 'Intrusion Alarm System Control Compliant Section',\n",
       " 'Kensington Lock',\n",
       " 'Kensington Lock Control Compliant Computer',\n",
       " 'Language',\n",
       " 'Level',\n",
       " 'Lightning Arrester',\n",
       " 'Lightning Arrester A',\n",
       " 'Lightning Arrester B',\n",
       " 'Lightning Arrester Control Compliant Building',\n",
       " 'Likert Scale',\n",
       " 'Loading Area',\n",
       " 'Location',\n",
       " 'Lockable Depositiory',\n",
       " 'Lockable Depository Outside Building',\n",
       " 'Lockable Depository Within Building',\n",
       " 'Locked Doors After Working Hours Policy',\n",
       " 'Locked Doors Control Compliant Organization',\n",
       " 'Low Level Threat',\n",
       " 'Magnetic Tape',\n",
       " 'Maintenance Contract',\n",
       " 'Maintenance Contract Control Compliant IT Component',\n",
       " 'Material Storing Control Compliant Building',\n",
       " 'Material Storing Policy',\n",
       " 'Media And Communication Scanning Control Compliant Organization',\n",
       " 'Media Disposal Control Compliant Organization',\n",
       " 'Mobile Code Blocking Control Compliant Organization',\n",
       " 'Mobile Device',\n",
       " 'Mobile Device Control Compliant Organization',\n",
       " 'Mobile Device Regulation Policy',\n",
       " 'Movable Asset',\n",
       " 'Multifunctional Firewall',\n",
       " 'Network Component',\n",
       " 'Network Logging And Monitoring Control Compliant Organization',\n",
       " 'Network Monitoring Software',\n",
       " 'Network Routing Control Compliant Organization',\n",
       " 'Network Segregation Control Compliant Organization',\n",
       " 'Network Services Agreement Control Compliant Organization',\n",
       " 'Networking Policy',\n",
       " 'New Facilities Authorization Control Compliant Organization',\n",
       " 'Notebook',\n",
       " 'OS',\n",
       " 'Offline Encryption Software',\n",
       " 'Online Encryption Software',\n",
       " 'Operational Procedures Policy',\n",
       " 'Organization',\n",
       " 'Organizational Records Protection Control Compliant Organization',\n",
       " 'Outsourcing Control Compliant Organization',\n",
       " 'PC And Internet Control Compliant Organization',\n",
       " 'PC Internet And Email Regulation Policy',\n",
       " 'PDA',\n",
       " 'Packet Filter Firewall',\n",
       " 'Password Compliant Check',\n",
       " 'Password Manager',\n",
       " 'Password Regulation Control Compliant Computer',\n",
       " 'Person',\n",
       " 'Personal Firewall',\n",
       " 'Personal Firewall Control Compliant Computer',\n",
       " 'Physical Access Key Control Compliant Organization',\n",
       " 'Physical Key Distribution Policy',\n",
       " 'Physical Media Transit Control Compliant Organization',\n",
       " 'Physical Media Transit Policy',\n",
       " 'Physical Perimeter Protection Control Compliant Organization',\n",
       " 'Physical Perimeter Protection Policy',\n",
       " 'Placement Control Compliant IT Component',\n",
       " 'Policy',\n",
       " 'Port Security And Configuration Policy',\n",
       " 'Port Security Diagnostic And Configuration Control Compliant Organization',\n",
       " 'Position Responsibilites And Deputizing Control Compliant Organization',\n",
       " 'Position Resposibility Policy',\n",
       " 'Power Source',\n",
       " 'Printer',\n",
       " 'Private Software And Hardware Control Compliant Organization',\n",
       " 'Private Software And Hardware Use Policy',\n",
       " 'Processing Validation Control Compliant Computer',\n",
       " 'Processing Validation Policy',\n",
       " 'Public Data',\n",
       " 'Raised Floor Control Compliant Level',\n",
       " 'Reception Regulation Policy',\n",
       " 'Recording Of Configuration Data Policy',\n",
       " 'Removable Hard Disk',\n",
       " 'Removable Media Control Compliant Organization',\n",
       " 'Removable Media Policy',\n",
       " 'Reputation',\n",
       " 'Restrictive Granting Of Access Rights Control Compliant Organization',\n",
       " 'Restrictive Granting Of Access Rights Policy',\n",
       " 'Restrictive Granting Of Access Rights Policy A',\n",
       " 'Restrictive Granting Of Access Rights Policy B',\n",
       " 'Risk Assessment Control Compliance Organization',\n",
       " 'Risks Policy',\n",
       " 'Role',\n",
       " 'Safety Door',\n",
       " 'Safety Door A',\n",
       " 'Safety Door B',\n",
       " 'Safety Door Control Compliant Section',\n",
       " 'Scale',\n",
       " 'Section',\n",
       " 'Section Connector',\n",
       " 'Sectionwith1Section Connector',\n",
       " 'Sectionwith2Section Connector',\n",
       " 'Sectionwith3Section Connector',\n",
       " 'Secure Areas Control Compliant Building',\n",
       " 'Secure Areas Policy',\n",
       " 'Secure Log On Control Compliant Organization',\n",
       " 'Security Attribute',\n",
       " 'Security Incidents Procedures Control Compliant Organization',\n",
       " 'Security Management Framework Control Compliant Organization',\n",
       " 'Security Management Framework Policy',\n",
       " 'Security Relevant Patches And Updates Control Compliant Organization',\n",
       " 'Security Relevant Patches And Updates Policy',\n",
       " 'Security Relevant Patches And Updates Policy A',\n",
       " 'Security Relevant Patches And Updates Policy B',\n",
       " 'Security Relevant Patches And Updates Policy C',\n",
       " 'Security Relevant Updates And Testing Control Compliant Organization',\n",
       " 'Security Training Control Compliant Organization',\n",
       " 'Security Training Policy',\n",
       " 'Sensitive Data',\n",
       " 'Sensitive Information Control Compliant Organization',\n",
       " 'Server',\n",
       " 'Server Rack',\n",
       " 'Server Room',\n",
       " 'Service Contract',\n",
       " 'Service Contract Policy',\n",
       " 'Session Time Control Compliant Organization',\n",
       " 'Session Time Policy',\n",
       " 'Settings',\n",
       " 'Shredder',\n",
       " 'Site',\n",
       " 'Smartphone',\n",
       " 'Smoke Detector',\n",
       " 'Smoke Detector Control Compliant Section',\n",
       " 'Social Network Control Compliant Organization',\n",
       " 'Social Network Policy',\n",
       " 'Software',\n",
       " 'Software Application Evaluation Policy',\n",
       " 'Staff Employment Policy',\n",
       " 'Standard Control',\n",
       " 'Standard Door',\n",
       " 'Switch',\n",
       " 'System',\n",
       " 'System Capacity Monitoring Control Compliant Organization',\n",
       " 'System Capacity Monitoring Software',\n",
       " 'System Image',\n",
       " 'Systems Testing And Acquisition Control Compliant Organization',\n",
       " 'Systems Testing And Acquisition Policy',\n",
       " 'Tablet',\n",
       " 'Tangible Asset',\n",
       " 'Technical Vulnerabilities Control Compliant Organization',\n",
       " 'Technical Vulnerabilities Policy',\n",
       " 'Teleworking Control Compliant Organization',\n",
       " 'Teleworking Policy',\n",
       " 'Teleworkstation',\n",
       " 'Temperature Surveillance Control Compliant Server Room',\n",
       " 'Temperature Surveillance System',\n",
       " 'Test System',\n",
       " 'Theft Protection Control Compliant Mobile Device',\n",
       " 'Theft Protection Software',\n",
       " 'Third Party Service Management Control Compliant Organization',\n",
       " 'Threat',\n",
       " 'Threat Apriori Probability',\n",
       " 'Threat Origin',\n",
       " 'Threat Source',\n",
       " 'Top Level Threat',\n",
       " 'Transaction Security And Virus Protection Software',\n",
       " 'Transaction Security And Virus Protection Software A',\n",
       " 'Transaction Security And Virus Protection Software B',\n",
       " 'Transparent Encryption Software',\n",
       " 'USB Stick',\n",
       " 'Unauthorized Use Prevention Control Compliant Organization',\n",
       " 'Uninterruptible Power Supply Control Compliant Server Room',\n",
       " 'Uninterruptible Power Supply Unit',\n",
       " 'Uninterruptible Power Supply Unit A',\n",
       " 'Uninterruptible Power Supply Unit B',\n",
       " 'User Identification Authentication Control Compliant Organization',\n",
       " 'User Profiles',\n",
       " 'User Registration And De Registration Control Compliant Organization',\n",
       " 'Virus Infection Control Compliant Organization',\n",
       " 'Virus Infection Emergency Regulation Policy',\n",
       " 'Visible Identification Control Compliant Building',\n",
       " 'Visitors Control Compliant Organization',\n",
       " 'Vulnerability',\n",
       " 'WKOIT Sicherheitshandbuch',\n",
       " 'WLAN Control Compliant Organization',\n",
       " 'WLAN Policy',\n",
       " 'Water Alarm System',\n",
       " 'Water Alarm System Control Compliant Section',\n",
       " 'Window',\n",
       " 'Wireless Access Point',\n",
       " 'Workstation',\n",
       " 'Workstation Access Rights Policy']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import finditer\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches])\n",
    "\n",
    "from pronto import Ontology\n",
    "[camel_case_split(elem) for elem in Ontology(\"/data/Vivek/securityontology.owl\").terms.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "for i,file in enumerate(sorted(glob.glob(\"../files/dataset/security*\"))):\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    instances_pairs = [\"\\t\".join(list(pair) + [\"none\"]) for pair in instances_pairs if pair]\n",
    "\n",
    "    open(\"../files/dataset/instances\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAN YOU DEFEND AGAINST ZERO-DAY THREATS? ',\n",
       " 'Every day, 8,3001 new, previously undiscovered cyber attacks emerge, including zero-day malware, zero-day phishing and social engineering attacks.',\n",
       " 'With no associated file signatures, anti-virus, firewalls and other core security solutions cannot identify no associated file signatures, anti-virus, firewalls and other core security solutions as malicious and block no associated file signatures, anti-virus, firewalls and other core security solutions from entering the network.',\n",
       " 'In fact, even the best AV solutions detect only half of malware strains in the wild. ',\n",
       " 'With no existing indicators of compromise (IOCs), how do you protect against what you do not know? COMMON NETWORK SECURITY APPROACHES HAVE LIMITATIONS ',\n",
       " 'To protect against zero-day threats, organizations use several approaches. ',\n",
       " 'These include: •',\n",
       " 'Conventional sandboxing solutions, which are susceptible to malware evasion techniques, and by default, are configured to let malware enter the network before analysis is complete. ',\n",
       " '•',\n",
       " 'Endpoint security, which has its advantages but cannot protect datacenters running dedicated servers and enterprise IoT, such as cameras, elevators and HVAC systems—for which the network perimeter often serves as the only line of defense. ',\n",
       " '•',\n",
       " 'A detection-first strategy that mainly relies on incident response, which is expensive, and often kicks in after the damage is already done. ',\n",
       " 'With such critical limitations, how can you protect your network from zero-day threats? ',\n",
       " 'CHECK POINT SANDBLAST NETWORK – NUMBER ONE IN ZERO-DAY PROTECTION ',\n",
       " 'Check Point SandBlast Network provides the world',\n",
       " '’s best2 zero-day protection, through a combination of evasion-resistant threat emulation, revolutionary AI engines and threat extraction that pre-emptively sanitizes email and web downloads. ',\n",
       " 'Empowering organizations to take a prevention-first strategy to cyberattacks, SandBlast Network defends against the most devastating attacks, including unknown ransomware, Trojans, phishing and social engineering. ',\n",
       " 'SandBlast Network deploys with your current infrastructure, offering fully automated policy configuration, without compromising business productivity and agility. ',\n",
       " 'BEST ZERO-DAY CATCH RATE ',\n",
       " 'To achieve the world’s best malware catch rate at record speed, SandBlast Network employs numerous innovative, proprietary technologies.',\n",
       " 'These include pre-emptive user protections, a vast network of up-to-the-moment threat intelligence and revolutionary AI and non-AI engines. ',\n",
       " 'Pre-emptive User Protections To protect users across email and web, SandBlast network employs pre-emptive user protections, namely threat extraction and advanced email protections. ',\n",
       " '•',\n",
       " 'SandBlast Threat Extraction promptly delivers clean and reconstructed versions of potentially malicious files that are received by email or downloaded from the web.',\n",
       " 'Maintaining uninterrupted business flow, while emulation continues in the background, SandBlast Threat Extraction eliminates unacceptable delays created by traditional sandboxes, offering a practical prevention-first strategy that blocks malicious content from reaching users at all.',\n",
       " 'SandBlast Threat Extraction instantly cleans web downloads and email with the industry’s only fully integrated document and image sanitization solution. ',\n",
       " '•',\n",
       " 'Advanced Email Protections',\n",
       " '–',\n",
       " 'With emails accounting for 94% of worldwide breaches3, defending against phishing, business email compromise (BEC), social engineering and other email-based threats has become imperative.',\n",
       " 'SandBlast Network protects users against these threats, using Threat Extraction to eliminate risk from all incoming email, as well as vetting all aspects of email messages before email messages enter your users’ mailbox, including email attachments, email links, sender and recipient details and the text within.',\n",
       " 'To this end, SandBlast Network evaluates over 300 parameters per email with multiple innovative technologies and rules-based engines, that include Natural Language Processing (NLP), Threat Emulation, AI-based phishing protection, AI-based fraud protection, URL reputation, emulating clicks on links and Click-Time Protection (also called URL rewriting) which analyzes and blocks malicious links in real time, as SandBlast Network are clicked. ',\n",
       " 'ThreatCloud – Dynamic Threat Intelligence Repository Comprising the largest repository of real-time, security intelligence— utilized in four billion security decisions daily—Check',\n",
       " 'Point ThreatCloud examines suspicious files and emails with breakthrough AI engines to determine if they are malicious or benign. ',\n",
       " 'Powering SandBlast Network’s zero day protection, including antiphishing and safe browsing, ThreatCloud gleans cyber attack data from: •',\n",
       " 'Hundreds of millions of protected assets worldwide across cloud, endpoints and networks •',\n",
       " 'Over 100,000 security gateways ',\n",
       " '•',\n",
       " 'Top notch research by Check Point Research Labs ',\n",
       " '•',\n",
       " 'The industry’s best threat intelligence feeds AI-Generated Threat Emulation Verdicts ',\n",
       " 'Inspecting files and emails for which no threat intelligence exists, SandBlast Network performs deep CPU-level emulation that is resistant to the most evasive attacks, even by nation states.',\n",
       " 'It also employs OS-level inspection to examine a broad range of file types, including executables and documents, and emulates threats across PC and Mac devices, ensuring the best zero-day protection for all enterprise users. ',\n",
       " 'SandBlast Network leverages the power of data science to detect the newest threats with exhaustive AI engines and rich rule-based engines that process millions of parameters collected from runtime behaviors—reaching a single conclusive AI-generated verdict.',\n",
       " 'AI heuristics are continually optimized against the latest threats unleashed to the wild. ',\n",
       " 'INTUITIVE MANAGEMENT SandBlast Network offers single click setup4 of security policies thanks to out-of-the-box best practice profiles that eliminate the need to manually configure policies for each network segment, e.g. data center, guest network, perimeter, internal network etc.',\n",
       " 'Network settings are optimized per business need to provide the most effective security while maintaining optimal network performance.',\n",
       " 'By only deploying policies that are relevant to the specific network segment being protected, organizations save on bandwidth and processing power for a more cost-effective zero-day protection strategy. ',\n",
       " 'And thanks to auto-updated threat prevention engines, organizations always run with the latest features, best practice policies and technology, as these are automatically updated in the background, with no need to push policy updates manually. ',\n",
       " 'Supports Current SIEM and SOC Workflows ',\n",
       " 'SandBlast Network offers advanced network forensics and actionable intelligence that integrate with your SIEM and SOC infrastructure, enabling security teams to: • Quickly integrate logs and forensic reports into SandBlast Network SIEM platform •',\n",
       " 'Enforce private threat intelligence in SandBlast Network ',\n",
       " '• Accelerate investigation and time-to-remediation with advanced forensics ',\n",
       " '• Gain visibility into zero-day phishing and malware targeting the network, including malware families, MITRE ',\n",
       " 'ATT&CK techniques used and much more ',\n",
       " '• Build confidence in a prevention-first strategy through insights and transparency Compliance and Reporting Serving as the gold standard for efficient security management, Check Point',\n",
       " '’s',\n",
       " 'R80 console provides enterprise and government-grade compliance and reporting, including: • Compliance –',\n",
       " 'Easy-to-use best practices, mapped to a broad array of regulatory mandates, offer full visibility into your compliance stance with actionable configuration guidelines, and instant alerts that apprise of any policy changes in real time • Logging and Reporting',\n",
       " '– Generate audit-ready reports, view logs online, integrate view logs online right into your log server or SIEM with our broad integration ecosystem, or export right into your log server or SIEM as needed SMOOTH BUSINESS PRODUCTIVITY ',\n",
       " 'SandBlast Network is the only zero-day protection solution that does not compromise business productivity, enabling a true prevention-first strategy.',\n",
       " 'Letting users maintain users current email and browsing workflows, SandBlast Threat Extraction cleans email attachments and web downloads in 1.5 seconds, while slashing administration overhead by up to 70%. . ',\n",
       " 'Thanks to blazing-speed, AI-generated Threat Emulation verdicts, Sandblast Network protects user activity across email, web and networks, for powerful zero-day protection against multiple attack vectors.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"../Outputs/Output_instances_softmax_Instances1 (original)_corrected.tsv\",\"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickledb\n",
    "prefix = \"../junk/db_files/\"\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "allkeys = list(word2id_db.getall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  tylgiv\n",
      "Dropping  valtra\n",
      "Dropping  matsika\n",
      "Dropping  frenstrup\n",
      "Dropping  kakkassery\n",
      "Dropping  only martelly\n",
      "Dropping  n700\n",
      "Dropping  mitteldeutschland\n",
      "Dropping  n5348a\n",
      "Dropping  hiramic\n",
      "Dropping  defined fields\n",
      "Dropping  the s j p harvie professor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a terminating binary expansion\n",
      "Dropping  the online canvas design elements\n",
      "Dropping  the instantaneous angular velocity vector\n",
      "Dropping  fitting anorexic illnesses\n",
      "Dropping  a 1920s proposal\n",
      "Dropping  an international non profit and non governmental student society\n",
      "Dropping  william a trimble\n",
      "Dropping  a provincial regiment\n",
      "Dropping  first real studio experience\n",
      "Dropping  a lycoming o 360 a4 m\n",
      "Dropping  other graphics systems\n",
      "Dropping  polish tradition\n",
      "Dropping  a practising teacher\n",
      "Dropping  close diplomatic and economic relationships\n",
      "Dropping  kiley\n",
      "Dropping  original or reconstructed fabric\n",
      "Dropping  scriptural or customary laws\n",
      "Dropping  national economics challenge champions\n",
      "Dropping  a long horizontal jump\n",
      "Dropping  the open bloodstream\n",
      "Dropping  the officer s blooded horses\n",
      "Dropping  classical comedy\n",
      "Dropping  the continental exchanges\n",
      "Dropping  the most frequent uses\n",
      "Dropping  major local developers\n",
      "Dropping  184 restaurants\n",
      "Dropping  maria s young son\n",
      "Dropping  utsu\n",
      "Dropping  archeologist hugo winckler\n",
      "Dropping  zp120\n",
      "Dropping  the early 1950s dubuffet\n",
      "Dropping  merina and betsileo families\n",
      "Dropping  impersonalization\n",
      "Dropping  all necessary activities\n",
      "Dropping  more complex background settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15446:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "Exception in thread Thread-15449:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  periodic recitals\n",
      "Dropping  last weekend s post coup presidential election\n",
      "Dropping  so2 james suh\n",
      "Dropping  silvie iii\n",
      "Dropping  pot au feu\n",
      "Dropping  its operational readiness\n",
      "Dropping  no one reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15472:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  the exterior mirror\n",
      "Dropping  free agent greg holland\n",
      "Dropping  keio university hospital\n",
      "Dropping  negative at skew\n",
      "Dropping  the former coalfield area\n",
      "Dropping  a coherent personality\n",
      "Dropping  intevation\n",
      "Dropping  fgm 148 javelin\n",
      "Dropping  17 august robert ritter von greim s fliegerkorps v\n",
      "Dropping  neither military training\n",
      "Dropping  self service passport control\n",
      "Dropping  sierra s salon\n",
      "Dropping  general no l de castelnau\n",
      "Dropping  debra delee\n",
      "Dropping  davis second term\n",
      "Dropping  the oldest literary account\n",
      "Dropping  each wall inlet\n",
      "Dropping  the people s nomadic heritage\n",
      "Dropping  glasgow academicals\n",
      "Dropping  fine v fib\n",
      "Dropping  flat end facets\n",
      "Dropping  dense grids\n",
      "Dropping  professor dominique martin\n",
      "Dropping  the fastest overall driver\n",
      "Dropping  their sledging rations\n",
      "Dropping  the lambda company\n",
      "Dropping  the additional rail\n",
      "Dropping  maintenance flaws\n",
      "Dropping  a 75 cm long bundle\n",
      "Dropping  179 fs\n",
      "Dropping  military miniatures\n",
      "Dropping  performance and management flexibility\n",
      "Dropping  two state run polytechnic schools\n",
      "Dropping  scriabin s museum\n",
      "Dropping  protestant dublin lawyer theobald wolfe tone\n",
      "Dropping  16 canadians\n",
      "Dropping  the individual coal plots\n",
      "Dropping  i e visemes\n",
      "Dropping  a e36 m3 compact prototype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15713:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  sal n de la paz\n",
      "Dropping  brian williams lustmord project\n",
      "Dropping  an exponential behavior\n",
      "Dropping  this uncommon case\n",
      "Dropping  only 13 more performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15739:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  then a third wrestling team\n",
      "Dropping  an old watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15748:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a sophisticated propaganda machine\n",
      "Dropping  a successful and effective program\n",
      "Dropping  50 s strongest track\n",
      "Dropping  the yshphh\n",
      "Dropping  the estimated sinking position\n",
      "Dropping  phoenix s citizens\n",
      "Dropping  the cbbb\n",
      "Dropping  re arranged panels\n",
      "Dropping  his 50th birthday celebration\n",
      "Dropping  the male eggs\n",
      "Dropping  montane meadows\n",
      "Dropping  the troops good spirit\n",
      "Dropping  paltrow s performance\n",
      "Dropping  a free demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15808:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  roxy attempts\n",
      "Dropping  either deletion\n",
      "Dropping  frictional behavior\n",
      "Dropping  four successive popes\n",
      "Dropping  engineering design teams\n",
      "Dropping  felix the cat\n",
      "Dropping  tidal venuses\n",
      "Dropping  dsquared2 duo dean and dan caten\n",
      "Dropping  cooper s most important film\n",
      "Dropping  consistent subtest scores\n",
      "Dropping  frances hegarty\n",
      "Dropping  borland s guitar playing\n",
      "Dropping  ahsura\n",
      "Dropping  an unnamed polish clone\n",
      "Dropping  at least the a credit rating\n",
      "Dropping  a radio based transatlantic telephone service\n",
      "Dropping  carddass exclusive storyline series\n",
      "Dropping  a balance sheet hedge\n",
      "Dropping  bluebush saltbush steppe\n"
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "allkeys = list(word2id_db.getall())\n",
    "for key in allkeys:\n",
    "    try:\n",
    "        word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "    except:\n",
    "        print (\"Dropping \", key)\n",
    "        word2id_db_corrected[key] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = key\n",
    "word2id_db_corrected.dump()\n",
    "id2word_db_corrected.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.25, [0.3300000000000004, 13, 0.15384615384615385, 0.6666666666666666]), (0.28571428571428575, [0.3330000000000004, 11, 0.18181818181818182, 0.6666666666666666]), (0.3333333333333333, [0.3340000000000004, 9, 0.2222222222222222, 0.6666666666666666]), (0.36363636363636365, [0.3360000000000004, 8, 0.25, 0.6666666666666666]), (0.4444444444444445, [0.3390000000000004, 7, 0.2857142857142857, 1.0]), (0.5, [0.3540000000000004, 6, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.36100000000000043, 4, 0.5, 1.0]), (0.5, [0.37900000000000045, 3, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.39600000000000046, 2, 0.5, 1.0]), (1.0, [0.40300000000000047, 1, 1.0, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances3 (hybrid).tsv 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.05555555555555556, [0.30099999999999977, 36, 0.02857142857142857, 1.0]), (0.05882352941176471, [0.30299999999999977, 34, 0.030303030303030304, 1.0]), (0.06060606060606061, [0.3139999999999997, 32, 0.03125, 1.0]), (0.06451612903225806, [0.3149999999999997, 30, 0.03333333333333333, 1.0]), (0.06666666666666667, [0.3169999999999997, 29, 0.034482758620689655, 1.0]), (0.0689655172413793, [0.32199999999999973, 28, 0.03571428571428571, 1.0]), (0.07407407407407407, [0.32299999999999973, 26, 0.038461538461538464, 1.0]), (0.07692307692307693, [0.32899999999999974, 25, 0.04, 1.0]), (0.07999999999999999, [0.33199999999999974, 24, 0.041666666666666664, 1.0]), (0.08695652173913045, [0.33599999999999974, 22, 0.045454545454545456, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances4 (original).tsv 0.08695652173913045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.027027027027027025, [0.18600000000000028, 187, 0.013986013986013986, 0.4]), (0.027972027972027972, [0.18800000000000028, 175, 0.014492753623188406, 0.4]), (0.02857142857142857, [0.18900000000000028, 172, 0.014814814814814815, 0.4]), (0.028776978417266185, [0.19100000000000028, 171, 0.014925373134328358, 0.4]), (0.028985507246376812, [0.19300000000000028, 169, 0.015037593984962405, 0.4]), (0.015503875968992246, [0.19400000000000028, 162, 0.007936507936507936, 0.3333333333333333]), (0.015748031496062992, [0.19500000000000028, 159, 0.008064516129032258, 0.3333333333333333]), (0.01652892561983471, [0.19600000000000029, 151, 0.00847457627118644, 0.3333333333333333]), (0.017391304347826087, [0.19700000000000029, 143, 0.008928571428571428, 0.3333333333333333]), (0.017699115044247787, [0.2010000000000003, 140, 0.00909090909090909, 0.3333333333333333])]\n",
      "../Outputs/Output_instances_softmax_Instances1 (webpage).tsv 0.040816326530612256\n",
      "[(0.25, [0.3300000000000004, 13, 0.15384615384615385, 0.6666666666666666]), (0.28571428571428575, [0.3330000000000004, 11, 0.18181818181818182, 0.6666666666666666]), (0.3333333333333333, [0.3340000000000004, 9, 0.2222222222222222, 0.6666666666666666]), (0.36363636363636365, [0.3360000000000004, 8, 0.25, 0.6666666666666666]), (0.4444444444444445, [0.3390000000000004, 7, 0.2857142857142857, 1.0]), (0.5, [0.3540000000000004, 6, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.36100000000000043, 4, 0.5, 1.0]), (0.5, [0.37900000000000045, 3, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.39600000000000046, 2, 0.5, 1.0]), (1.0, [0.40300000000000047, 1, 1.0, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances3 (original).tsv 1.0\n",
      "[(0.25, [0.3300000000000004, 13, 0.15384615384615385, 0.6666666666666666]), (0.28571428571428575, [0.3330000000000004, 11, 0.18181818181818182, 0.6666666666666666]), (0.3333333333333333, [0.3340000000000004, 9, 0.2222222222222222, 0.6666666666666666]), (0.36363636363636365, [0.3360000000000004, 8, 0.25, 0.6666666666666666]), (0.4444444444444445, [0.3390000000000004, 7, 0.2857142857142857, 1.0]), (0.5, [0.3540000000000004, 6, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.36100000000000043, 4, 0.5, 1.0]), (0.5, [0.37900000000000045, 3, 0.3333333333333333, 1.0]), (0.6666666666666666, [0.39600000000000046, 2, 0.5, 1.0]), (1.0, [0.40300000000000047, 1, 1.0, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances3 (webpage).tsv 1.0\n",
      "[(0.02666666666666666, [0.2140000000000003, 94, 0.0136986301369863, 0.5]), (0.02702702702702703, [0.2160000000000003, 90, 0.013888888888888888, 0.5]), (0.028169014084507043, [0.2170000000000003, 88, 0.014285714285714285, 1.0]), (0.028985507246376812, [0.2210000000000003, 86, 0.014705882352941176, 1.0]), (0.03125, [0.2240000000000003, 81, 0.015873015873015872, 1.0]), (0.031746031746031744, [0.2280000000000003, 79, 0.016129032258064516, 1.0]), (0.03278688524590164, [0.23000000000000032, 76, 0.016666666666666666, 1.0]), (0.037037037037037035, [0.23400000000000032, 68, 0.018867924528301886, 1.0]), (0.0392156862745098, [0.23500000000000032, 65, 0.02, 1.0]), (0.042553191489361694, [0.23800000000000032, 60, 0.021739130434782608, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances1 (original).tsv 0.05755395683453237\n",
      "[(0.05555555555555556, [0.30099999999999977, 36, 0.02857142857142857, 1.0]), (0.05882352941176471, [0.30299999999999977, 34, 0.030303030303030304, 1.0]), (0.06060606060606061, [0.3139999999999997, 32, 0.03125, 1.0]), (0.06451612903225806, [0.3149999999999997, 30, 0.03333333333333333, 1.0]), (0.06666666666666667, [0.3169999999999997, 29, 0.034482758620689655, 1.0]), (0.0689655172413793, [0.32199999999999973, 28, 0.03571428571428571, 1.0]), (0.07407407407407407, [0.32299999999999973, 26, 0.038461538461538464, 1.0]), (0.07692307692307693, [0.32899999999999974, 25, 0.04, 1.0]), (0.07999999999999999, [0.33199999999999974, 24, 0.041666666666666664, 1.0]), (0.08695652173913045, [0.33599999999999974, 22, 0.045454545454545456, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances4 (webpage).tsv 0.08695652173913045\n",
      "[(0.06896551724137931, [0.2500000000000003, 29, 0.037037037037037035, 0.5]), (0.07407407407407407, [0.2510000000000003, 27, 0.04, 0.5]), (0.07692307692307693, [0.2540000000000003, 26, 0.041666666666666664, 0.5]), (0.08, [0.2620000000000003, 24, 0.043478260869565216, 0.5]), (0.08333333333333334, [0.2630000000000003, 23, 0.045454545454545456, 0.5]), (0.08695652173913042, [0.2650000000000003, 22, 0.047619047619047616, 0.5]), (0.10526315789473684, [0.2660000000000003, 18, 0.058823529411764705, 0.5]), (0.1111111111111111, [0.2690000000000003, 17, 0.0625, 0.5]), (0.11764705882352941, [0.2760000000000003, 16, 0.06666666666666667, 0.5]), (0.13333333333333336, [0.2780000000000003, 13, 0.07692307692307693, 0.5])]\n",
      "../Outputs/Output_instances_softmax_Instances2 (original).tsv 0.13333333333333336\n",
      "[(0.05555555555555556, [0.30099999999999977, 36, 0.02857142857142857, 1.0]), (0.05882352941176471, [0.30299999999999977, 34, 0.030303030303030304, 1.0]), (0.06060606060606061, [0.3139999999999997, 32, 0.03125, 1.0]), (0.06451612903225806, [0.3149999999999997, 30, 0.03333333333333333, 1.0]), (0.06666666666666667, [0.3169999999999997, 29, 0.034482758620689655, 1.0]), (0.0689655172413793, [0.32199999999999973, 28, 0.03571428571428571, 1.0]), (0.07407407407407407, [0.32299999999999973, 26, 0.038461538461538464, 1.0]), (0.07692307692307693, [0.32899999999999974, 25, 0.04, 1.0]), (0.07999999999999999, [0.33199999999999974, 24, 0.041666666666666664, 1.0]), (0.08695652173913045, [0.33599999999999974, 22, 0.045454545454545456, 1.0])]\n",
      "../Outputs/Output_instances_softmax_Instances4 (hybrid).tsv 0.08695652173913045\n",
      "[(0.026845637583892617, [0.18600000000000028, 187, 0.013888888888888888, 0.4]), (0.02777777777777778, [0.18800000000000028, 175, 0.014388489208633094, 0.4]), (0.028368794326241134, [0.18900000000000028, 172, 0.014705882352941176, 0.4]), (0.02857142857142857, [0.19100000000000028, 171, 0.014814814814814815, 0.4]), (0.028776978417266185, [0.19300000000000028, 169, 0.014925373134328358, 0.4]), (0.015384615384615385, [0.19400000000000028, 162, 0.007874015748031496, 0.3333333333333333]), (0.015748031496062992, [0.19500000000000028, 159, 0.008064516129032258, 0.3333333333333333]), (0.016393442622950817, [0.19600000000000029, 151, 0.008403361344537815, 0.3333333333333333]), (0.017241379310344827, [0.19700000000000029, 143, 0.008849557522123894, 0.3333333333333333]), (0.017543859649122806, [0.2010000000000003, 140, 0.009009009009009009, 0.3333333333333333])]\n",
      "../Outputs/Output_instances_softmax_Instances1 (hybrid).tsv 0.04073319755600815\n",
      "[(0.06896551724137931, [0.2500000000000003, 29, 0.037037037037037035, 0.5]), (0.07407407407407407, [0.2510000000000003, 27, 0.04, 0.5]), (0.07692307692307693, [0.2540000000000003, 26, 0.041666666666666664, 0.5]), (0.08, [0.2550000000000003, 25, 0.043478260869565216, 0.5]), (0.08333333333333334, [0.2620000000000003, 24, 0.045454545454545456, 0.5]), (0.08695652173913042, [0.2650000000000003, 22, 0.047619047619047616, 0.5]), (0.10526315789473684, [0.2660000000000003, 18, 0.058823529411764705, 0.5]), (0.1111111111111111, [0.2690000000000003, 17, 0.0625, 0.5]), (0.11764705882352941, [0.2760000000000003, 16, 0.06666666666666667, 0.5]), (0.13333333333333336, [0.2780000000000003, 13, 0.07692307692307693, 0.5])]\n",
      "../Outputs/Output_instances_softmax_Instances2 (webpage).tsv 0.13333333333333336\n",
      "[(0.06060606060606061, [0.24600000000000027, 35, 0.03225806451612903, 0.5]), (0.07142857142857144, [0.2500000000000003, 29, 0.038461538461538464, 0.5]), (0.07692307692307693, [0.2510000000000003, 27, 0.041666666666666664, 0.5]), (0.08, [0.2540000000000003, 26, 0.043478260869565216, 0.5]), (0.08333333333333334, [0.2620000000000003, 24, 0.045454545454545456, 0.5]), (0.08695652173913042, [0.2650000000000003, 22, 0.047619047619047616, 0.5]), (0.10526315789473684, [0.2660000000000003, 18, 0.058823529411764705, 0.5]), (0.1111111111111111, [0.2690000000000003, 17, 0.0625, 0.5]), (0.11764705882352941, [0.2760000000000003, 16, 0.06666666666666667, 0.5]), (0.13333333333333336, [0.2780000000000003, 13, 0.07692307692307693, 0.5])]\n",
      "../Outputs/Output_instances_softmax_Instances2 (hybrid).tsv 0.13333333333333336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import spatial\n",
    "import glob, math\n",
    "from orderedset import OrderedSet\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def calculate_recall(true, pred):\n",
    "    true_f, pred_f = [], []\n",
    "    for i,elem in enumerate(true):\n",
    "        if elem!=\"none\":\n",
    "            true_f.append(elem)\n",
    "            pred_f.append(pred[i])\n",
    "    return accuracy_score(true_f, pred_f)\n",
    "\n",
    "def calculate_precision(true, pred):\n",
    "    true_f, pred_f = [], []\n",
    "    for i,elem in enumerate(pred):\n",
    "        if elem!=\"none\":\n",
    "            pred_f.append(elem)\n",
    "            true_f.append(true[i])\n",
    "    return accuracy_score(true_f, pred_f)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "SECURITY_WORD = \"Information Security\"\n",
    "for file in glob.glob(\"../Outputs/Output_instances_softmax_Instances*).tsv\"):\n",
    "    lines = [l.split(\"\\t\") for l in open(file, \"r\").read().split(\"\\n\")]\n",
    "    words = [SECURITY_WORD] + list(set(flatten([l[:2] for l in lines])))\n",
    "    embeds = extractUSEEmbeddings(words)\n",
    "    emb_indexer = dict(zip(words, embeds))\n",
    "    all_fscores = []\n",
    "    sims = flatten([(cos_sim(emb_indexer[SECURITY_WORD], emb_indexer[elem[0]]), cos_sim(emb_indexer[SECURITY_WORD], emb_indexer[elem[1]])) for elem in lines])\n",
    "    for threshold in np.arange(round(min(sims), 3), round(max(sims), 3), 0.001):\n",
    "        lines_short = [elem for elem in lines if cos_sim(emb_indexer[SECURITY_WORD], emb_indexer[elem[0]]) > threshold and cos_sim(emb_indexer[SECURITY_WORD], emb_indexer[elem[1]]) > threshold]\n",
    "        if not lines_short:\n",
    "            continue\n",
    "        pred, gt = list(zip(*[line[-2:] for line in lines_short]))\n",
    "        recall = calculate_recall(gt, pred)\n",
    "        precision = calculate_precision(gt, pred)\n",
    "        f1score = 2 * (precision * recall)/ (precision + recall)\n",
    "#         print (threshold, precision, recall, f1score, len(lines_short))\n",
    "        if math.isnan(f1score):\n",
    "            continue\n",
    "        all_fscores.append((lines_short, threshold, len(lines_short), precision, recall, f1score))\n",
    "    opt_elem = max(all_fscores, key = lambda l: l[-1])\n",
    "    print([(k, [elem[0] for elem in list(list(zip(*g))[1:-1])]) for k, g in groupby(all_fscores, itemgetter(5))][-10:])\n",
    "    print (file, opt_elem[-1])\n",
    "    new_file = file.rsplit(\".\",1)[0] + \"_shortened.tsv\"\n",
    "    open(new_file, \"w+\").write(\"\\n\".join([\"\\t\".join(elem) for elem in opt_elem[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Outputs/Output_instances_softmax_Instances2 (hybrid).tsv'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24888888888888888"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, gt = list(zip(*[l.split(\"\\t\")[2:] for l in open(\"../Outputs/Output_instances_softmax_Instances1 (original)_corrected.tsv\", \"r\").read().split(\"\\n\")[:408]]))\n",
    "precision = calculate_precision(gt, pred)\n",
    "recall = calculate_recall(gt, pred) \n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-74e40c04e389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mallkeys_corrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/convolution.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X__bi, drop)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX__bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX__bo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mfinish_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_finish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX__bo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.NumpyOps.seq2col\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36mascontiguousarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# t = time.time()\n",
    "import sys\n",
    "def p(word):\n",
    "    try:\n",
    "        return preprocess_word(word)\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n",
    "        pass\n",
    "    except Exception:\n",
    "        return word\n",
    "t = time.time()\n",
    "allkeys_corrected = [p(word) for word in list(nlp.pipe(allkeys))]\n",
    "print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lines = \"\\n\".join([l.strip() for l in open(\"../files/dataset/instances3_validated.tsv\").read().split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28747"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"../files/dataset/instances3_validated.tsv\", \"w+\").write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
