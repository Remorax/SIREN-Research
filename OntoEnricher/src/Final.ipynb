{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"/data/Vivek/original/HypeNET/dataset/custom_train_0.0_0.05.tsv\"\n",
    "test_file =  \"/data/Vivek/original/HypeNET/dataset/custom_test_0.0_0.05.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/db_files/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = [entity_to_id(word2id_db, elem) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "failed, success = [], []\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "    closest_entity = resolved.get(entity, \"\")\n",
    "    if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "        success.append(entity)\n",
    "        return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold = 0.8\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "paths_instances, counts_instances, targets_instances  = parse_dataset(test_instances)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "\n",
    "emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "output_file = \"../Input/data_use_\" + str(threshold) + \".pkl\"\n",
    "f = open(output_file, \"wb+\")\n",
    "pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "             nodes_test, paths_test, counts_test, targets_test,\n",
    "             nodes_instances, paths_instances, counts_instances, targets_instances,\n",
    "             nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "             emb_indexer, emb_indexer_inv, emb_vals, \n",
    "             pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_padded = np.array([elem + [0 for i in range(max_paths - len(elem))] for elem in counts_train][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 289)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "paths_output = torch.randn((100, 289, 250, 2))\n",
    "counts_padded = torch.DoubleTensor(counts_padded)\n",
    "prod = torch.bmm(paths_output.reshape(-1, 289, 250*2).permute(0,2,1), counts_padded.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hypernym': 0, 'hyponym': 1, 'concept': 2, 'instance': 3, 'none': 4}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8237, -0.3493],\n",
       "         [ 0.2487, -0.9980],\n",
       "         [ 1.7283, -0.3358],\n",
       "         ...,\n",
       "         [ 0.5942, -0.1741],\n",
       "         [-0.9849,  1.0004],\n",
       "         [-0.2682,  1.6092]],\n",
       "\n",
       "        [[-0.7238, -0.1685],\n",
       "         [-0.5104, -1.3679],\n",
       "         [ 0.5790,  0.3465],\n",
       "         ...,\n",
       "         [ 0.5371, -0.9790],\n",
       "         [-0.8587, -0.3445],\n",
       "         [-0.2285,  0.9683]],\n",
       "\n",
       "        [[ 0.7231,  0.0178],\n",
       "         [ 1.4549, -1.0254],\n",
       "         [ 1.3443,  0.2672],\n",
       "         ...,\n",
       "         [-0.5941,  0.8811],\n",
       "         [ 0.0183, -0.7739],\n",
       "         [ 1.1137, -0.2095]],\n",
       "\n",
       "        [[-0.5445,  0.8521],\n",
       "         [-1.0216,  0.4876],\n",
       "         [-0.2075,  0.6121],\n",
       "         ...,\n",
       "         [ 0.0787, -1.2297],\n",
       "         [ 0.2751, -0.0413],\n",
       "         [ 0.2628,  0.9497]],\n",
       "\n",
       "        [[-3.2454,  0.8706],\n",
       "         [ 1.8683,  0.2403],\n",
       "         [-1.9614,  1.6072],\n",
       "         ...,\n",
       "         [ 1.5648,  0.6847],\n",
       "         [-0.3047, -0.4647],\n",
       "         [ 0.0309,  0.7784]],\n",
       "\n",
       "        [[-0.6388, -0.7616],\n",
       "         [-0.7573,  0.0929],\n",
       "         [-0.1377, -0.8018],\n",
       "         ...,\n",
       "         [-0.8199, -0.6388],\n",
       "         [ 0.1134,  0.7666],\n",
       "         [ 0.2336,  0.1250]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_output[15][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_train[:100].index([1, 2, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
