{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"../files/dataset/pizza_train.tsv\"\n",
    "test_file =  \"../files/dataset/pizza_test.tsv\"\n",
    "# instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/pizza_knockedout.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/Pizza/temp/pizza_threshold_7_10/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use_unbracketed.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def preprocess_db(db):\n",
    "    final_db = {}\n",
    "    for key in db:\n",
    "        try:\n",
    "            new_key = key.decode(\"utf-8\")\n",
    "        except:\n",
    "            new_key = key\n",
    "        try:\n",
    "            new_val = db[key].decode(\"utf-8\")\n",
    "        except:\n",
    "            new_val = db[key]\n",
    "        final_db[new_key] = new_val\n",
    "    return final_db\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = [entity_to_id(word2id_db, elem) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def get_instance_key(tup):\n",
    "    return tuple([\" \".join([tok.text for tok in nlp(elem)]) for elem in tup])\n",
    "\n",
    "def parse_instance(tup):\n",
    "    \n",
    "    paths_x = list(instances_db.get(get_instance_key(tup), {}).items())\n",
    "    paths_y = list(instances_db.get(get_instance_key(tup[::-1]), {}).items())\n",
    "    path_count_dict_x = { path.replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { path.replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_instance_dataset(dataset):\n",
    "    parsed_dicts = [parse_instance(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "#     closest_entity = resolved.get(entity, \"\")\n",
    "#     if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "#         success.append(entity)\n",
    "#         return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = preprocess_db(pickle.load(open(prefix + \"pizza_word_to_id_dict.pkl\", \"rb\")))\n",
    "id2word_db = preprocess_db(pickle.load(open(prefix + \"pizza_id_to_word_dict.pkl\", \"rb\")))\n",
    "path2id_db = preprocess_db(pickle.load(open(prefix + \"pizza_path_to_id_dict.pkl\", \"rb\")))\n",
    "id2path_db = preprocess_db(pickle.load(open(prefix + \"pizza_id_to_path_dict.pkl\", \"rb\")))\n",
    "relations_db = preprocess_db(pickle.load(open(prefix + \"pizza_word_occurence_map.pkl\", \"rb\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Instance DB\n",
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path, maxlen):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if maxlen >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks, maxlen):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            filtered = [stringifyFilterPath(path, maxlen) for path in appendingElem]\n",
    "            paths.extend(filtered)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "doc = open(\"../files/dataset/security4.txt\").read()\n",
    "all_nounchunks = list(nlp(doc).noun_chunks).copy()\n",
    "\n",
    "sentences = [list(nlp(nlp(sent.text)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \")).sents)[0]\n",
    "             for sent in nlp(doc).sents]\n",
    "# [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "all_deps = []\n",
    "instances_db = {}\n",
    "for sentence in sentences:\n",
    "    noun_chunks = [n for n in all_nounchunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "    noun_chunks = list(nlp(sentence.text).noun_chunks)\n",
    "    dependencies = getDependencyPaths(sentence, nlp, noun_chunks, 10)\n",
    "    for dep in dependencies:\n",
    "        if not dep:\n",
    "            continue\n",
    "        key = tuple([preprocess_word(nlp(word)) for word in dep[:2]])\n",
    "        path = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in dep[-1].split(\"/\")])\n",
    "        if key not in instances_db:\n",
    "            instances_db[key] = [path]\n",
    "        else:\n",
    "            instances_db[key].append(path)\n",
    "instances_db = {key: Counter(instances_db[key]) for key in instances_db}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# thresholds = [0.5, 0.59, 0.6, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
    "\n",
    "# for threshold in thresholds:\n",
    "threshold = 0.86\n",
    "    \n",
    "failed, success = [], []\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "# test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "# paths_instances, counts_instances, targets_instances  = parse_instance_dataset(test_instances)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "# nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "# nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "# nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "# nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "# print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "# print (len(failed), len(success))\n",
    "# emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "# embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "# emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "# output_file = \"../Input/data_instances_sample.pkl\"\n",
    "# f = open(output_file, \"wb+\")\n",
    "# pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "#              nodes_test, paths_test, counts_test, targets_test,\n",
    "#              nodes_instances, paths_instances, counts_instances, targets_instances,\n",
    "#              nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "#              emb_indexer, emb_indexer_inv, emb_vals, \n",
    "#              pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "# f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([failed, list(word2id_db.keys())], open(\"../junk/failed_words_pizza\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "for i,file in enumerate(sorted(glob.glob(\"../files/dataset/security*\"))):\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    instances_pairs = [\"\\t\".join(list(pair) + [\"none\"]) for pair in instances_pairs if pair]\n",
    "\n",
    "    open(\"../files/dataset/instances\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "db type is dbm.gnu, but the module is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-50d6e68da899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../junk/db_files/pizza_term_to_id.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise error[0](\"db type is {0}, but the module is not \"\n\u001b[0;32m---> 91\u001b[0;31m                        \"available\".format(result))\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: db type is dbm.gnu, but the module is not available"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "shelve.open(\"../junk/db_files/pizza_term_to_id.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'equatorial guinea',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'cameroon',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'brem',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'media',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'morocco',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'cloverfield',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'agisymba',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'denmark',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'elmer',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'don luis',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'bsisa',\n",
       " 'food',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'little shop',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'food',\n",
       " 'fried onion',\n",
       " 'food',\n",
       " 'munchos',\n",
       " 'coconut jam',\n",
       " 'food',\n",
       " 'task force',\n",
       " 'soho',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'soho',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'bellisio foods',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'songpyeon',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'ice cream',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'angola',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'romano cheese',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'burmese tofu',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'electronic media',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'rosa',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'monsuno',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'julius erving',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'street food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'al forno',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'banitsa',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'media',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'itunes store',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'flatbread',\n",
       " 'pizza',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'pizza',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'comestibles',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'andre agassi',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'nicaragua',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'hot',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'jameed',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'dumpling',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'siciliana',\n",
       " 'style',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the bahamas',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'hot',\n",
       " 'hot',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'gyeongju national park',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'rudists',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'garth brooks',\n",
       " 'country',\n",
       " 'country',\n",
       " 'solomon islands',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'ukraine',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'sixteen candles',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the new spirit',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'granola',\n",
       " 'food',\n",
       " 'country',\n",
       " 'moravia',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'monsters university',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the jerry springer show',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'mali',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'human food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'ice cream',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489929"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  tylgiv\n",
      "Dropping  valtra\n",
      "Dropping  matsika\n",
      "Dropping  frenstrup\n",
      "Dropping  kakkassery\n",
      "Dropping  only martelly\n",
      "Dropping  n700\n",
      "Dropping  mitteldeutschland\n",
      "Dropping  n5348a\n",
      "Dropping  hiramic\n",
      "Dropping  defined fields\n",
      "Dropping  the s j p harvie professor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a terminating binary expansion\n",
      "Dropping  the online canvas design elements\n",
      "Dropping  the instantaneous angular velocity vector\n",
      "Dropping  fitting anorexic illnesses\n",
      "Dropping  a 1920s proposal\n",
      "Dropping  an international non profit and non governmental student society\n",
      "Dropping  william a trimble\n",
      "Dropping  a provincial regiment\n",
      "Dropping  first real studio experience\n",
      "Dropping  a lycoming o 360 a4 m\n",
      "Dropping  other graphics systems\n",
      "Dropping  polish tradition\n",
      "Dropping  a practising teacher\n",
      "Dropping  close diplomatic and economic relationships\n",
      "Dropping  kiley\n",
      "Dropping  original or reconstructed fabric\n",
      "Dropping  scriptural or customary laws\n",
      "Dropping  national economics challenge champions\n",
      "Dropping  a long horizontal jump\n",
      "Dropping  the open bloodstream\n",
      "Dropping  the officer s blooded horses\n",
      "Dropping  classical comedy\n",
      "Dropping  the continental exchanges\n",
      "Dropping  the most frequent uses\n",
      "Dropping  major local developers\n",
      "Dropping  184 restaurants\n",
      "Dropping  maria s young son\n",
      "Dropping  utsu\n",
      "Dropping  archeologist hugo winckler\n",
      "Dropping  zp120\n",
      "Dropping  the early 1950s dubuffet\n",
      "Dropping  merina and betsileo families\n",
      "Dropping  impersonalization\n",
      "Dropping  all necessary activities\n",
      "Dropping  more complex background settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15446:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "Exception in thread Thread-15449:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  periodic recitals\n",
      "Dropping  last weekend s post coup presidential election\n",
      "Dropping  so2 james suh\n",
      "Dropping  silvie iii\n",
      "Dropping  pot au feu\n",
      "Dropping  its operational readiness\n",
      "Dropping  no one reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15472:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  the exterior mirror\n",
      "Dropping  free agent greg holland\n",
      "Dropping  keio university hospital\n",
      "Dropping  negative at skew\n",
      "Dropping  the former coalfield area\n",
      "Dropping  a coherent personality\n",
      "Dropping  intevation\n",
      "Dropping  fgm 148 javelin\n",
      "Dropping  17 august robert ritter von greim s fliegerkorps v\n",
      "Dropping  neither military training\n",
      "Dropping  self service passport control\n",
      "Dropping  sierra s salon\n",
      "Dropping  general no l de castelnau\n",
      "Dropping  debra delee\n",
      "Dropping  davis second term\n",
      "Dropping  the oldest literary account\n",
      "Dropping  each wall inlet\n",
      "Dropping  the people s nomadic heritage\n",
      "Dropping  glasgow academicals\n",
      "Dropping  fine v fib\n",
      "Dropping  flat end facets\n",
      "Dropping  dense grids\n",
      "Dropping  professor dominique martin\n",
      "Dropping  the fastest overall driver\n",
      "Dropping  their sledging rations\n",
      "Dropping  the lambda company\n",
      "Dropping  the additional rail\n",
      "Dropping  maintenance flaws\n",
      "Dropping  a 75 cm long bundle\n",
      "Dropping  179 fs\n",
      "Dropping  military miniatures\n",
      "Dropping  performance and management flexibility\n",
      "Dropping  two state run polytechnic schools\n",
      "Dropping  scriabin s museum\n",
      "Dropping  protestant dublin lawyer theobald wolfe tone\n",
      "Dropping  16 canadians\n",
      "Dropping  the individual coal plots\n",
      "Dropping  i e visemes\n",
      "Dropping  a e36 m3 compact prototype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15713:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  sal n de la paz\n",
      "Dropping  brian williams lustmord project\n",
      "Dropping  an exponential behavior\n",
      "Dropping  this uncommon case\n",
      "Dropping  only 13 more performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15739:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  then a third wrestling team\n",
      "Dropping  an old watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15748:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a sophisticated propaganda machine\n",
      "Dropping  a successful and effective program\n",
      "Dropping  50 s strongest track\n",
      "Dropping  the yshphh\n",
      "Dropping  the estimated sinking position\n",
      "Dropping  phoenix s citizens\n",
      "Dropping  the cbbb\n",
      "Dropping  re arranged panels\n",
      "Dropping  his 50th birthday celebration\n",
      "Dropping  the male eggs\n",
      "Dropping  montane meadows\n",
      "Dropping  the troops good spirit\n",
      "Dropping  paltrow s performance\n",
      "Dropping  a free demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15808:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  roxy attempts\n",
      "Dropping  either deletion\n",
      "Dropping  frictional behavior\n",
      "Dropping  four successive popes\n",
      "Dropping  engineering design teams\n",
      "Dropping  felix the cat\n",
      "Dropping  tidal venuses\n",
      "Dropping  dsquared2 duo dean and dan caten\n",
      "Dropping  cooper s most important film\n",
      "Dropping  consistent subtest scores\n",
      "Dropping  frances hegarty\n",
      "Dropping  borland s guitar playing\n",
      "Dropping  ahsura\n",
      "Dropping  an unnamed polish clone\n",
      "Dropping  at least the a credit rating\n",
      "Dropping  a radio based transatlantic telephone service\n",
      "Dropping  carddass exclusive storyline series\n",
      "Dropping  a balance sheet hedge\n",
      "Dropping  bluebush saltbush steppe\n"
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "allkeys = list(word2id_db.getall())\n",
    "for key in allkeys:\n",
    "    try:\n",
    "        word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "    except:\n",
    "        print (\"Dropping \", key)\n",
    "        word2id_db_corrected[key] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = key\n",
    "word2id_db_corrected.dump()\n",
    "id2word_db_corrected.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015409708023071289\n",
      "0.0001728534698486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "a = nlp(\"beach on the ocean\")\n",
    "print (time.time()-t)\n",
    "t = time.time()\n",
    "preprocess_word(a)\n",
    "print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "lines = [l.split(\"\\t\")[:3] for l in open(\"../files/dataset/pizza_knockedout.tsv\", \"r\").read().split(\"\\n\")]\n",
    "final_lines = []\n",
    "for line in lines:\n",
    "    elem = line\n",
    "    if random.random()>0.5:\n",
    "        label = line[2]\n",
    "        if label == \"hyponym\":\n",
    "            label = \"hypernym\"\n",
    "        elif label == \"hypernym\":\n",
    "            label = \"hyponym\"\n",
    "        elif label == \"concept\":\n",
    "            label = \"instance\"\n",
    "        elif label == \"instance\":\n",
    "            label = \"concept\"\n",
    "        elem = [line[1], line[0], label]\n",
    "    final_lines.append(elem)\n",
    "random.shuffle(final_lines)\n",
    "# final_lines_none = [elem for elem in final_lines if elem==\"none\"]\n",
    "# final_lines_none_train = final_lines_none[:int(0.9 * len(final_lines_none))]\n",
    "# final_lines_none_test = final_lines_none[int(0.9 * len(final_lines_none)):]\n",
    "\n",
    "# final_lines_rest = [elem for elem in final_lines if elem!=\"none\"]\n",
    "# final_lines_rest_train = final_lines_rest[:int(0.9 * len(final_lines_rest))]\n",
    "# final_lines_rest_test = final_lines_rest[int(0.9 * len(final_lines_rest)):]\n",
    "\n",
    "# final_lines_train = final_lines_none_train + final_lines_rest_train\n",
    "# final_lines_test = final_lines_none_test + final_lines_rest_test\n",
    "\n",
    "open(\"../files/dataset/pizza_knockedout_shuffled.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines]))\n",
    "# open(\"../files/dataset/pizza_test.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, itertools\n",
    "from scipy import spatial\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "# Extracting Instances from a document\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "\n",
    "for file in glob.glob(\"../files/dataset/pizza_webpages/*.txt\"):\n",
    "\n",
    "\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    entities = list(set(flatten(instances)))\n",
    "\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    all_lines = [list(pair) + [\"none\"] for pair in instances_pairs if pair]\n",
    "    \n",
    "    embeds = extractUSEEmbeddings([\"Pizza\"] + entities)\n",
    "    # open(\"../files/dataset/webpage_terms.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "    lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "    scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}\n",
    "\n",
    "    def get(key, dictionary):\n",
    "        try:\n",
    "            return dictionary[key]\n",
    "        except KeyboardInterrupt as e:\n",
    "            sys.exit()\n",
    "        except:\n",
    "            print (key)\n",
    "            dictionary_lower = {elem.lower(): dictionary[elem] for elem in dictionary}\n",
    "            return dictionary_lower[key.lower()]\n",
    "\n",
    "    filtered_lines = []\n",
    "    for elem in all_lines:\n",
    "        try:\n",
    "            if get(elem[0], scores_dict) and get(elem[1], scores_dict):\n",
    "                filtered_lines.append(elem)\n",
    "        except:\n",
    "            print (elem)\n",
    "    open(file.rsplit(\".\",1)[0] + \"_shortened.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in filtered_lines]))\n",
    "\n",
    "    # [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "    # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'e'),\n",
       " ('n', 'a'),\n",
       " ('o', 's'),\n",
       " ('t', 'a'),\n",
       " ('i', ' '),\n",
       " ('i', 'n'),\n",
       " ('e', 'y'),\n",
       " ('d', 'y'),\n",
       " ('o', ' '),\n",
       " ('p', 'y'),\n",
       " ('t', 'n'),\n",
       " ('l', 'p'),\n",
       " ('r', 'c'),\n",
       " ('a', 'o'),\n",
       " ('n', ','),\n",
       " ('a', 'a'),\n",
       " ('r', 't'),\n",
       " ('r', 'i'),\n",
       " ('t', 'e'),\n",
       " ('d', 't'),\n",
       " (' ', 'l'),\n",
       " ('p', 't'),\n",
       " ('a', 'n'),\n",
       " ('s', 't'),\n",
       " (',', 'i'),\n",
       " (' ', 'p'),\n",
       " ('n', ' '),\n",
       " (',', 'y'),\n",
       " ('c', 'a'),\n",
       " ('s', 'y'),\n",
       " ('o', 'c'),\n",
       " ('l', 'l'),\n",
       " ('d', 'a'),\n",
       " ('t', 'd'),\n",
       " ('t', 'p'),\n",
       " ('o', 'i'),\n",
       " ('p', 'a'),\n",
       " ('c', 'i'),\n",
       " (',', 'a'),\n",
       " ('i', 'l'),\n",
       " ('l', 'e'),\n",
       " ('n', 'y'),\n",
       " ('i', 'a'),\n",
       " ('d', 'n'),\n",
       " ('r', ' '),\n",
       " (',', ' '),\n",
       " ('r', 'n'),\n",
       " ('a', 'i'),\n",
       " ('t', 'o'),\n",
       " ('t', 't'),\n",
       " ('d', 'e'),\n",
       " ('i', 'i'),\n",
       " ('p', 'e'),\n",
       " ('r', 'a'),\n",
       " ('o', 'a'),\n",
       " ('r', 'o'),\n",
       " ('a', 'c'),\n",
       " ('l', ','),\n",
       " ('i', 'c'),\n",
       " ('c', 'l'),\n",
       " ('a', 'y'),\n",
       " ('t', 'l'),\n",
       " (' ', 'e'),\n",
       " ('l', 'i'),\n",
       " ('d', 'l'),\n",
       " (' ', 't'),\n",
       " ('p', 'l'),\n",
       " ('r', 'y'),\n",
       " ('t', ' '),\n",
       " ('s', 'c'),\n",
       " ('i', 'p'),\n",
       " ('o', 't'),\n",
       " ('t', 'r'),\n",
       " ('o', 'e'),\n",
       " ('n', 't'),\n",
       " ('l', 's'),\n",
       " (',', 'l'),\n",
       " ('e', 'c'),\n",
       " ('d', 'p'),\n",
       " ('e', 't'),\n",
       " ('a', ' '),\n",
       " ('r', 'd'),\n",
       " ('r', 'p'),\n",
       " (',', 'p'),\n",
       " ('s', 'p'),\n",
       " (' ', 'i'),\n",
       " ('r', 'l'),\n",
       " ('n', 's'),\n",
       " ('l', 't'),\n",
       " ('a', 's'),\n",
       " (',', 't'),\n",
       " (',', 'e'),\n",
       " ('t', ','),\n",
       " ('d', 'c'),\n",
       " ('p', 'c'),\n",
       " (' ', 's'),\n",
       " ('c', 'y'),\n",
       " ('s', 'l'),\n",
       " ('i', 'e'),\n",
       " ('d', 'i'),\n",
       " ('p', 'i'),\n",
       " ('t', 'i'),\n",
       " ('i', 't'),\n",
       " ('o', 'p'),\n",
       " ('a', ','),\n",
       " ('l', ' '),\n",
       " ('l', 'c'),\n",
       " ('o', 'n'),\n",
       " ('t', 's'),\n",
       " ('s', 'e'),\n",
       " ('i', 's'),\n",
       " ('s', 'i'),\n",
       " ('i', 'y'),\n",
       " ('c', 't'),\n",
       " ('e', 'i'),\n",
       " ('n', 'e'),\n",
       " ('o', ','),\n",
       " (',', 's'),\n",
       " ('a', 't'),\n",
       " (' ', 'c'),\n",
       " ('n', 'p'),\n",
       " ('s', 'a'),\n",
       " ('o', 'l'),\n",
       " ('n', 'l'),\n",
       " ('t', 'y'),\n",
       " ('n', 'c'),\n",
       " ('t', 'c'),\n",
       " ('d', ','),\n",
       " ('l', 'a'),\n",
       " ('r', 'e'),\n",
       " ('r', ','),\n",
       " ('n', 'i'),\n",
       " ('d', 'o'),\n",
       " (' ', 'y'),\n",
       " ('i', 'o'),\n",
       " ('d', 's'),\n",
       " ('i', ','),\n",
       " ('r', 's'),\n",
       " ('a', 'l'),\n",
       " ('d', ' '),\n",
       " ('o', 'y'),\n",
       " (' ', 'a'),\n",
       " (',', 'c'),\n",
       " ('a', 'd'),\n",
       " ('a', 'p'),\n",
       " ('e', 'l'),\n",
       " ('e', 'a'),\n",
       " ('l', 'y'),\n",
       " ('e', 'c'),\n",
       " ('o', 'o'),\n",
       " (' ', 'n'),\n",
       " (' ', 'c'),\n",
       " ('o', 'l'),\n",
       " ('g', 'l'),\n",
       " ('l', 'r'),\n",
       " ('g', 'd'),\n",
       " ('c', 'o'),\n",
       " ('o', ' '),\n",
       " (' ', 'o'),\n",
       " ('n', 'r'),\n",
       " ('l', 'e'),\n",
       " ('e', 'o'),\n",
       " ('o', 'r'),\n",
       " ('d', 'n'),\n",
       " ('d', 'c'),\n",
       " ('l', 'd'),\n",
       " ('g', 'e'),\n",
       " ('n', 'c'),\n",
       " ('r', 'n'),\n",
       " ('g', ' '),\n",
       " ('g', 'n'),\n",
       " ('d', 'e'),\n",
       " ('o', 'd'),\n",
       " ('d', 'o'),\n",
       " ('l', 'n'),\n",
       " ('n', 'o'),\n",
       " ('l', ' '),\n",
       " (' ', 'r'),\n",
       " ('g', 'o'),\n",
       " ('l', 'c'),\n",
       " ('e', 'r'),\n",
       " ('o', 'n'),\n",
       " ('g', 'c'),\n",
       " ('l', 'o'),\n",
       " ('c', 'r'),\n",
       " ('e', ' '),\n",
       " ('e', 'n'),\n",
       " ('c', 'n'),\n",
       " ('d', ' '),\n",
       " ('n', ' '),\n",
       " ('d', 'r'),\n",
       " ('n', 'n'),\n",
       " ('o', 'e'),\n",
       " ('o', 'c'),\n",
       " ('g', 'r'),\n",
       " ('i', 'a'),\n",
       " ('P', 'z'),\n",
       " ('z', 'a'),\n",
       " ('P', 'i'),\n",
       " ('P', 'a'),\n",
       " ('z', 'z'),\n",
       " ('i', 'z'),\n",
       " ('p', 'a'),\n",
       " ('e', 't'),\n",
       " ('t', 'p'),\n",
       " ('s', 'a'),\n",
       " ('t', 'a'),\n",
       " ('s', 'p'),\n",
       " (' ', 'i'),\n",
       " ('b', 'z'),\n",
       " ('i', 'z'),\n",
       " ('i', 'a'),\n",
       " ('e', 's'),\n",
       " ('b', 'p'),\n",
       " ('z', 'a'),\n",
       " ('s', ' '),\n",
       " ('s', 'z'),\n",
       " ('p', 'i'),\n",
       " ('e', 'p'),\n",
       " ('b', 't'),\n",
       " ('t', 'i'),\n",
       " (' ', 'z'),\n",
       " ('b', 'e'),\n",
       " ('b', ' '),\n",
       " ('s', 't'),\n",
       " ('e', ' '),\n",
       " ('t', 'z'),\n",
       " ('b', 's'),\n",
       " ('s', 'i'),\n",
       " ('p', 'z'),\n",
       " (' ', 'p'),\n",
       " (' ', 'a'),\n",
       " ('e', 'z'),\n",
       " ('t', ' '),\n",
       " ('e', 'i'),\n",
       " ('e', 'a'),\n",
       " ('b', 'a'),\n",
       " ('b', 'i'),\n",
       " ('z', 'z'),\n",
       " ('w', 'o'),\n",
       " ('h', 'o'),\n",
       " ('w', 'h'),\n",
       " ('w', 'e'),\n",
       " ('e', 'e'),\n",
       " ('w', 's'),\n",
       " ('e', 't'),\n",
       " ('e', 's'),\n",
       " ('w', 't'),\n",
       " ('e', 'b'),\n",
       " ('w', 'e'),\n",
       " ('i', 'e'),\n",
       " ('b', 't'),\n",
       " ('i', 't'),\n",
       " ('t', 'e'),\n",
       " ('b', 'e'),\n",
       " ('w', 'i'),\n",
       " ('s', 't'),\n",
       " ('s', 'e'),\n",
       " ('b', 's'),\n",
       " ('w', 'b'),\n",
       " ('s', 'i'),\n",
       " ('e', 'i'),\n",
       " ('b', 'i'),\n",
       " ('l', 's'),\n",
       " ('e', 'c'),\n",
       " ('o', 'b'),\n",
       " ('e', 'e'),\n",
       " ('m', 'c'),\n",
       " ('b', 'c'),\n",
       " ('b', 'n'),\n",
       " ('i', 'r'),\n",
       " ('o', 'i'),\n",
       " ('o', 's'),\n",
       " (' ', 'c'),\n",
       " (' ', 'n'),\n",
       " ('o', 'l'),\n",
       " ('i', ' '),\n",
       " ('l', 'r'),\n",
       " ('b', 'r'),\n",
       " ('i', 'n'),\n",
       " ('m', 'i'),\n",
       " ('m', 'e'),\n",
       " ('m', 'o'),\n",
       " ('b', 'l'),\n",
       " ('o', ' '),\n",
       " ('i', 'l'),\n",
       " ('l', 'e'),\n",
       " ('o', 'r'),\n",
       " ('e', 's'),\n",
       " ('s', 'r'),\n",
       " ('r', 'n'),\n",
       " (' ', 's'),\n",
       " ('r', 'e'),\n",
       " ('i', 'e'),\n",
       " ('m', 'l'),\n",
       " ('m', ' '),\n",
       " ('l', 'n'),\n",
       " ('b', 'e'),\n",
       " ('m', 'r'),\n",
       " ('m', 'n'),\n",
       " ('b', ' '),\n",
       " ('i', 'c'),\n",
       " ('l', ' '),\n",
       " ('l', 'c'),\n",
       " (' ', 'r'),\n",
       " ('e', 'r'),\n",
       " ('o', 'n'),\n",
       " ('c', 'r'),\n",
       " ('s', 'e'),\n",
       " ('e', ' '),\n",
       " ('i', 's'),\n",
       " ('b', 's'),\n",
       " ('e', 'n'),\n",
       " (' ', 'e'),\n",
       " ('c', 'n'),\n",
       " ('c', 'e'),\n",
       " ('m', 'b'),\n",
       " ('s', 'c'),\n",
       " ('s', 'n'),\n",
       " ('m', 's'),\n",
       " ('o', 'e'),\n",
       " ('b', 'i'),\n",
       " ('o', 'c'),\n",
       " ('g', 's'),\n",
       " ('n', 'a'),\n",
       " ('o', '-'),\n",
       " ('v', 'i'),\n",
       " ('v', 's'),\n",
       " ('e', 'e'),\n",
       " ('o', 'i'),\n",
       " ('o', 's'),\n",
       " ('-', 'i'),\n",
       " ('-', 's'),\n",
       " ('e', 'g'),\n",
       " ('e', 't'),\n",
       " ('t', 'a'),\n",
       " ('i', 'n'),\n",
       " ('g', 'a'),\n",
       " ('n', 's'),\n",
       " ('a', 's'),\n",
       " ('v', 'r'),\n",
       " ('n', 'r'),\n",
       " ('t', 'n'),\n",
       " ('i', 'a'),\n",
       " ('o', 'r'),\n",
       " ('-', 'r'),\n",
       " ('e', 's'),\n",
       " ('g', 'e'),\n",
       " ('r', 'n'),\n",
       " ('v', 't'),\n",
       " ('g', 't'),\n",
       " ('n', 'v'),\n",
       " ('a', 'a'),\n",
       " ('g', 'n'),\n",
       " ('a', 'i'),\n",
       " ('n', 'g'),\n",
       " ('n', 'i'),\n",
       " ('v', 'a'),\n",
       " ('t', 'i'),\n",
       " ('r', 'i'),\n",
       " ('r', 'a'),\n",
       " ('-', 'e'),\n",
       " ('n', 'o'),\n",
       " ('n', '-'),\n",
       " ('o', 'a'),\n",
       " ('-', 'a'),\n",
       " ('g', 'i'),\n",
       " ('a', 'n'),\n",
       " ('o', 'v'),\n",
       " ('-', 'v'),\n",
       " ('e', 'r'),\n",
       " ('o', 'n'),\n",
       " ('-', 'n'),\n",
       " ('t', 's'),\n",
       " ('a', 'r'),\n",
       " ('r', 's'),\n",
       " ('v', 'n'),\n",
       " ('i', 's'),\n",
       " ('e', 'n'),\n",
       " ('v', 'g'),\n",
       " ('o', 'g'),\n",
       " ('-', 'g'),\n",
       " ('o', 't'),\n",
       " ('n', 'e'),\n",
       " ('n', 'n'),\n",
       " ('-', 't'),\n",
       " ('v', 'e'),\n",
       " ('e', 'a'),\n",
       " ('o', 'e'),\n",
       " ('n', 't'),\n",
       " ('e', 'i'),\n",
       " ('t', 'r'),\n",
       " ('g', 'r'),\n",
       " ('t', 'u'),\n",
       " ('u', 'o'),\n",
       " ('u', 'a'),\n",
       " ('u', 'i'),\n",
       " ('s', 'a'),\n",
       " ('t', 'a'),\n",
       " ('i', 'n'),\n",
       " ('u', 'n'),\n",
       " ('s', 'u'),\n",
       " ('t', 'n'),\n",
       " ('i', 'a'),\n",
       " ('u', 't'),\n",
       " ('a', 'o'),\n",
       " ('i', 'u'),\n",
       " ('a', 'i'),\n",
       " ('t', 'o'),\n",
       " ('t', 't'),\n",
       " ('i', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'i'),\n",
       " ('i', 'o'),\n",
       " ('a', 'n'),\n",
       " ('s', 't'),\n",
       " ('o', 'n'),\n",
       " ('s', 'o'),\n",
       " ('s', 'i'),\n",
       " ('s', 'n'),\n",
       " ('a', 't'),\n",
       " (' ', 'k'),\n",
       " ('d', 'p'),\n",
       " ('p', 'p'),\n",
       " ('d', 'a'),\n",
       " ('p', 'a'),\n",
       " ('r', 'd'),\n",
       " ('r', 'p'),\n",
       " (' ', 'i'),\n",
       " ('r', 'r'),\n",
       " ('i', 'a'),\n",
       " ('e', 'k'),\n",
       " ('r', ' '),\n",
       " ('r', 'e'),\n",
       " ('a', 'a'),\n",
       " ('a', 'i'),\n",
       " ('d', 'i'),\n",
       " ('p', 'i'),\n",
       " ('e', 'p'),\n",
       " ('e', 'd'),\n",
       " ('r', 'i'),\n",
       " ('r', 'a'),\n",
       " (' ', 'r'),\n",
       " ('a', 'r'),\n",
       " ('e', 'r'),\n",
       " ('r', 'k'),\n",
       " ('i', 'k'),\n",
       " ('e', ' '),\n",
       " ('d', 'k'),\n",
       " ('p', 'k'),\n",
       " ('a', 'k'),\n",
       " ('d', ' '),\n",
       " (' ', 'p'),\n",
       " (' ', 'a'),\n",
       " ('a', 'p'),\n",
       " ('d', 'r'),\n",
       " ('p', 'r'),\n",
       " ('k', 'a'),\n",
       " ('e', 'i'),\n",
       " ('e', 'a'),\n",
       " ('i', 'o'),\n",
       " ('n', 'o'),\n",
       " ('o', 'o'),\n",
       " ('o', 'i'),\n",
       " ('n', 'n'),\n",
       " ('i', 'n'),\n",
       " ('o', 'n'),\n",
       " ('n', 'i'),\n",
       " ('g', 's'),\n",
       " ('v', 'i'),\n",
       " ('v', 's'),\n",
       " ('e', 'e'),\n",
       " ('e', 't'),\n",
       " ('e', 'g'),\n",
       " ('t', 'a'),\n",
       " ('i', 'n'),\n",
       " ('g', 'a'),\n",
       " ('n', 's'),\n",
       " ('a', 's'),\n",
       " ('v', 'r'),\n",
       " ('t', 'n'),\n",
       " ('i', 'a'),\n",
       " ('e', 's'),\n",
       " ('g', 'e'),\n",
       " ('r', 'n'),\n",
       " ('g', 't'),\n",
       " ('a', 'a'),\n",
       " ('g', 'n'),\n",
       " ('a', 'i'),\n",
       " ('v', 'a'),\n",
       " ('t', 'i'),\n",
       " ('r', 'i'),\n",
       " ('r', 'a'),\n",
       " ('g', 'i'),\n",
       " ('a', 'n'),\n",
       " ('a', 'r'),\n",
       " ('e', 'r'),\n",
       " ('t', 's'),\n",
       " ('r', 's'),\n",
       " ('v', 'n'),\n",
       " ('i', 's'),\n",
       " ('e', 'n'),\n",
       " ('v', 'g'),\n",
       " ('v', 'e'),\n",
       " ('e', 'a'),\n",
       " ('e', 'i'),\n",
       " ('t', 'r'),\n",
       " ('v', 't'),\n",
       " ('g', 'r'),\n",
       " ('e', 'e'),\n",
       " ('b', 'n'),\n",
       " ('e', 't'),\n",
       " ('c', 'i'),\n",
       " ('c', 'o'),\n",
       " ('l', 'r'),\n",
       " ('b', 'r'),\n",
       " ('i', 'n'),\n",
       " ('l', 't'),\n",
       " ('t', 'n'),\n",
       " ('l', 'e'),\n",
       " ('b', 'a'),\n",
       " ('l', 'a'),\n",
       " ('r', 'n'),\n",
       " ('e', 'b'),\n",
       " ('a', 'o'),\n",
       " ('c', 'b'),\n",
       " ('a', 'i'),\n",
       " ('t', 'o'),\n",
       " ('r', 't'),\n",
       " ('b', 't'),\n",
       " ('t', 'i'),\n",
       " ('r', 'i'),\n",
       " ('l', 'n'),\n",
       " ('r', 'a'),\n",
       " ('i', 'o'),\n",
       " ('r', 'o'),\n",
       " ('a', 'n'),\n",
       " ('c', 'l'),\n",
       " ('e', 'r'),\n",
       " ('o', 'n'),\n",
       " ('c', 'r'),\n",
       " ('l', 'o'),\n",
       " ('l', 'i'),\n",
       " ('e', 'n'),\n",
       " ('c', 'n'),\n",
       " ('c', 'e'),\n",
       " ('e', 'l'),\n",
       " ('c', 't'),\n",
       " ('l', 'b'),\n",
       " ('e', 'i'),\n",
       " ('e', 'a'),\n",
       " ('b', 'o'),\n",
       " ('c', 'a'),\n",
       " ('e', 'o'),\n",
       " ('b', 'i'),\n",
       " ('a', 't'),\n",
       " ('b', 'v'),\n",
       " ('l', 's'),\n",
       " ('k', ' '),\n",
       " ('k', 'l'),\n",
       " ('a', 'e'),\n",
       " ('v', 's'),\n",
       " ('b', 'c'),\n",
       " ('o', 'i'),\n",
       " ('a', ' '),\n",
       " ('c', 'k'),\n",
       " ('c', 'i'),\n",
       " ('c', 's'),\n",
       " ('o', 's'),\n",
       " ('o', 'l'),\n",
       " ('c', 'o'),\n",
       " (' ', 'i'),\n",
       " ('b', 'l'),\n",
       " ('a', 's'),\n",
       " ('k', 'v'),\n",
       " ('b', 'k'),\n",
       " (' ', 'o'),\n",
       " ('l', 'e'),\n",
       " (' ', 'v'),\n",
       " ('e', 's'),\n",
       " ('l', 'a'),\n",
       " ('a', 'o'),\n",
       " (' ', 's'),\n",
       " ('k', 's'),\n",
       " ('a', 'i'),\n",
       " ('i', 'e'),\n",
       " ('l', 'k'),\n",
       " ('a', 'v'),\n",
       " (' ', 'l'),\n",
       " ('b', 'e'),\n",
       " ('c', 'v'),\n",
       " ('a', 'c'),\n",
       " ('l', ' '),\n",
       " ('b', ' '),\n",
       " ('o', 'e'),\n",
       " ('l', 'c'),\n",
       " ('c', 'l'),\n",
       " ('o', 'v'),\n",
       " ('k', 'i'),\n",
       " ('l', 'o'),\n",
       " ('k', 'e'),\n",
       " ('k', 'o'),\n",
       " (' ', 'e'),\n",
       " ('a', 'l'),\n",
       " ('l', 'i'),\n",
       " ('b', 's'),\n",
       " ('a', 'k'),\n",
       " ('c', ' '),\n",
       " ('i', 's'),\n",
       " ('l', 'v'),\n",
       " ('c', 'e'),\n",
       " ('v', 'e'),\n",
       " ('b', 'o'),\n",
       " ('i', 'v'),\n",
       " ('b', 'a'),\n",
       " ('b', 'i'),\n",
       " ('l', 'l'),\n",
       " ('p', 'z'),\n",
       " ('i', 'a'),\n",
       " ('p', 'a'),\n",
       " ('z', 'a'),\n",
       " ('p', 'i'),\n",
       " ('z', 'z'),\n",
       " ('i', 'z'),\n",
       " ('n', 'a'),\n",
       " ('n', 'd'),\n",
       " ('d', 'a'),\n",
       " ('g', 'd'),\n",
       " ('g', 'a'),\n",
       " ('o', ' '),\n",
       " ('d', 'y'),\n",
       " ('n', 'y'),\n",
       " ('l', 'd'),\n",
       " ('l', 'a'),\n",
       " ('g', ' '),\n",
       " ('n', 'g'),\n",
       " ('o', 'd'),\n",
       " (' ', 'y'),\n",
       " ('l', 'n'),\n",
       " ('o', 'a'),\n",
       " ('l', ' '),\n",
       " ('a', 'y'),\n",
       " ('o', 'n'),\n",
       " ('l', 'o'),\n",
       " ('l', 'g'),\n",
       " ('o', 'y'),\n",
       " (' ', 'd'),\n",
       " ('n', ' '),\n",
       " (' ', 'a'),\n",
       " ('o', 'g'),\n",
       " ('g', 'y'),\n",
       " ('l', 'y'),\n",
       " ('c', 'u'),\n",
       " ('c', 's'),\n",
       " ('c', 't'),\n",
       " ('u', 's'),\n",
       " ('u', 't'),\n",
       " ('r', 'u'),\n",
       " ('s', 't'),\n",
       " ('c', 'r'),\n",
       " ('r', 's'),\n",
       " ('r', 't'),\n",
       " ('h', 'e'),\n",
       " ('t', 'e'),\n",
       " ('e', 'y'),\n",
       " ('t', 'h'),\n",
       " ('t', 'y'),\n",
       " ('h', 'y'),\n",
       " ('t', 'u'),\n",
       " ('e', 'u'),\n",
       " ('a', 'e'),\n",
       " ('w', 's'),\n",
       " ('o', 's'),\n",
       " (' ', 'o'),\n",
       " ('o', ' '),\n",
       " ('N', 'c'),\n",
       " ('N', '-'),\n",
       " ('u', 't'),\n",
       " ('h', 'r'),\n",
       " ('a', 'o'),\n",
       " ('-', 'u'),\n",
       " ('h', 's'),\n",
       " ('r', 't'),\n",
       " ('e', 'd'),\n",
       " ('-', 'd'),\n",
       " ('d', 't'),\n",
       " ('n', 'o'),\n",
       " ('-', 'e'),\n",
       " ('t', 'e'),\n",
       " ('N', ' '),\n",
       " ('s', 's'),\n",
       " ('N', 't'),\n",
       " ('a', 'n'),\n",
       " (' ', 'h'),\n",
       " ('s', 't'),\n",
       " ('N', 'h'),\n",
       " (' ', 'd'),\n",
       " ('c', 'u'),\n",
       " ('h', 't'),\n",
       " ('n', ' '),\n",
       " ('d', 'r'),\n",
       " ('n', 'u'),\n",
       " ('o', 'c'),\n",
       " ('N', 'r'),\n",
       " ('e', 'e'),\n",
       " ('-', 'o'),\n",
       " ('t', 'd'),\n",
       " ('e', 'w'),\n",
       " ('h', 'o'),\n",
       " ('o', 'r'),\n",
       " ('e', 's'),\n",
       " ('N', 's'),\n",
       " ('s', ' '),\n",
       " ('N', 'w'),\n",
       " ('t', 'o'),\n",
       " ('d', 'e'),\n",
       " ('t', 't'),\n",
       " ('d', '-'),\n",
       " ('a', 'c'),\n",
       " ('w', 'o'),\n",
       " (' ', 'r'),\n",
       " ('u', 's'),\n",
       " ('e', 'r'),\n",
       " (' ', 'e'),\n",
       " (' ', 't'),\n",
       " ('t', ' '),\n",
       " ('s', 'c'),\n",
       " ('o', 't'),\n",
       " ('t', 'r'),\n",
       " ('e', '-'),\n",
       " ('n', 't'),\n",
       " ('o', 'e'),\n",
       " ('d', 'u'),\n",
       " ('e', 'c'),\n",
       " ('w', 'r'),\n",
       " ('d', 'd'),\n",
       " ('e', 't'),\n",
       " ('a', ' '),\n",
       " ('-', 's'),\n",
       " ('s', 'd'),\n",
       " ('n', 's'),\n",
       " (' ', '-'),\n",
       " ('w', 'a'),\n",
       " ('a', 's'),\n",
       " ('n', 'r'),\n",
       " ('-', ' '),\n",
       " ('w', 'h'),\n",
       " ('d', 'c'),\n",
       " ('N', 'o'),\n",
       " ('w', 'u'),\n",
       " (' ', 's'),\n",
       " ('a', '-'),\n",
       " ('o', 'u'),\n",
       " ('r', 'u'),\n",
       " ('w', ' '),\n",
       " ('o', 'd'),\n",
       " ('n', '-'),\n",
       " ('h', 'a'),\n",
       " ('a', 'r'),\n",
       " ('t', 's'),\n",
       " ('c', 'r'),\n",
       " ('s', 'e'),\n",
       " ('h', 'u'),\n",
       " ('w', 'c'),\n",
       " ('h', ' '),\n",
       " ('h', 'n'),\n",
       " ('c', 't'),\n",
       " ('n', 'e'),\n",
       " ('-', 'c'),\n",
       " ('a', 't'),\n",
       " ('N', 'n'),\n",
       " (' ', ' '),\n",
       " (' ', 'n'),\n",
       " (' ', 'c'),\n",
       " ('n', 'd'),\n",
       " ('N', 'u'),\n",
       " ('c', 's'),\n",
       " ('w', 'd'),\n",
       " ('N', 'a'),\n",
       " ('h', '-'),\n",
       " ('s', 'u'),\n",
       " ('h', 'c'),\n",
       " ('-', 'r'),\n",
       " ('s', 'r'),\n",
       " ('w', 't'),\n",
       " ('n', 'c'),\n",
       " ('t', 'c'),\n",
       " ('w', 'n'),\n",
       " ('w', 'e'),\n",
       " ('d', 'o'),\n",
       " ('a', 'u'),\n",
       " ('w', '-'),\n",
       " ('h', 'd'),\n",
       " ('N', 'e'),\n",
       " ('d', 's'),\n",
       " (' ', 'u'),\n",
       " ('r', 's'),\n",
       " ('e', ' '),\n",
       " ('e', 'n'),\n",
       " ('h', 'e'),\n",
       " ('d', ' '),\n",
       " ('N', 'd'),\n",
       " (' ', 'a'),\n",
       " ('a', 'd'),\n",
       " ('-', 't'),\n",
       " ('e', 'a'),\n",
       " ('e', 'o'),\n",
       " ('e', 'h'),\n",
       " ('s', 'i'),\n",
       " ('s', 's'),\n",
       " ('e', 's'),\n",
       " ('i', 'd'),\n",
       " ('d', 's'),\n",
       " ('s', 'd'),\n",
       " ('i', 'e'),\n",
       " ('s', 'e'),\n",
       " ('d', 'e'),\n",
       " ('i', 's'),\n",
       " ('e', 'c'),\n",
       " ('e', 'e'),\n",
       " (' ', 'c'),\n",
       " ('c', 'i'),\n",
       " ('c', 's'),\n",
       " ('h', 'i'),\n",
       " ('e', 'y'),\n",
       " ('y', 'e'),\n",
       " (' ', 'i'),\n",
       " ('l', 'e'),\n",
       " ('h', 'c'),\n",
       " ('e', 's'),\n",
       " (' ', 's'),\n",
       " ('c', 'y'),\n",
       " ('s', ' '),\n",
       " ('h', 's'),\n",
       " ('s', 'l'),\n",
       " ('i', 'e'),\n",
       " ('h', 'y'),\n",
       " ('y', ' '),\n",
       " (' ', 'l'),\n",
       " ('h', 'l'),\n",
       " ('s', 's'),\n",
       " ('i', 'c'),\n",
       " ('c', 'l'),\n",
       " ('l', 'c'),\n",
       " ('y', 'c'),\n",
       " ('s', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'e'),\n",
       " ('l', 'i'),\n",
       " ('s', 'i'),\n",
       " ('c', ' '),\n",
       " ('h', 'e'),\n",
       " ('c', 'e'),\n",
       " ('h', ' '),\n",
       " ('e', 'l'),\n",
       " ('y', 's'),\n",
       " ('y', 'i'),\n",
       " ('s', 'c'),\n",
       " ('e', 'i'),\n",
       " ('y', 'l'),\n",
       " ('c', 'h'),\n",
       " ('s', 'y'),\n",
       " ('c', 'c'),\n",
       " ('t', 'u'),\n",
       " ('e', 'u'),\n",
       " ('u', 'a'),\n",
       " ('e', 't'),\n",
       " ('s', 'a'),\n",
       " ('t', 'a'),\n",
       " ('u', 'n'),\n",
       " ('s', 'u'),\n",
       " ('t', 'n'),\n",
       " ('u', 'r'),\n",
       " ('r', 'r'),\n",
       " ('e', 's'),\n",
       " ('s', 'r'),\n",
       " ('u', 't'),\n",
       " ('r', 'n'),\n",
       " ('r', 'u'),\n",
       " ('r', 'e'),\n",
       " ('a', 'a'),\n",
       " ('r', 't'),\n",
       " ('t', 't'),\n",
       " ('r', 'a'),\n",
       " ('a', 'u'),\n",
       " ('a', 'n'),\n",
       " ('a', 'r'),\n",
       " ('e', 'r'),\n",
       " ('s', 't'),\n",
       " ('r', 's'),\n",
       " ('e', 'n'),\n",
       " ('s', 'n'),\n",
       " ('e', 'a'),\n",
       " ('t', 'r'),\n",
       " ('n', 't'),\n",
       " ('a', 't'),\n",
       " ('t', 'u'),\n",
       " ('e', 'u'),\n",
       " ('c', \"'\"),\n",
       " ('o', 's'),\n",
       " ('i', ' '),\n",
       " ('e', 'D'),\n",
       " ('i', 'n'),\n",
       " ('t', 'm'),\n",
       " (' ', 'D'),\n",
       " (' ', 'o'),\n",
       " ('o', ' '),\n",
       " ('t', 'n'),\n",
       " ('D', ' '),\n",
       " ('u', 't'),\n",
       " ('i', 'u'),\n",
       " ('r', 't'),\n",
       " (' ', 'm'),\n",
       " ('r', 'i'),\n",
       " ('t', 'e'),\n",
       " ('n', 'o'),\n",
       " ('D', 'u'),\n",
       " ('u', 'm'),\n",
       " ('u', \"'\"),\n",
       " ('r', \"'\"),\n",
       " ('o', \"'\"),\n",
       " ('u', ' '),\n",
       " ('c', 'u'),\n",
       " ('n', ' '),\n",
       " ('n', 'u'),\n",
       " ('m', 's'),\n",
       " ('c', 'D'),\n",
       " ('u', 'o'),\n",
       " ('e', 'e'),\n",
       " (\"'\", 's'),\n",
       " ('o', 'i'),\n",
       " ('c', 'i'),\n",
       " ('m', 'i'),\n",
       " ('D', 'n'),\n",
       " ('m', 'e'),\n",
       " ('i', 'm'),\n",
       " ('D', 'm'),\n",
       " ('e', 's'),\n",
       " ('r', ' '),\n",
       " ('r', 'n'),\n",
       " ('s', ' '),\n",
       " ('t', 'D'),\n",
       " ('D', 'i'),\n",
       " ('t', 'o'),\n",
       " ('m', 'm'),\n",
       " ('r', 'o'),\n",
       " ('D', 'e'),\n",
       " ('c', 'm'),\n",
       " ('m', 'n'),\n",
       " ('o', 'm'),\n",
       " ('u', 's'),\n",
       " ('n', \"'\"),\n",
       " (' ', 'e'),\n",
       " ('t', ' '),\n",
       " ('D', 's'),\n",
       " ('o', 'e'),\n",
       " ('n', 't'),\n",
       " (\"'\", ' '),\n",
       " ('u', 'i'),\n",
       " (\"'\", 'm'),\n",
       " ('e', 't'),\n",
       " ('r', 'm'),\n",
       " (' ', 'i'),\n",
       " ('n', 's'),\n",
       " ('m', 'u'),\n",
       " ('e', \"'\"),\n",
       " ('r', 'u'),\n",
       " (' ', 's'),\n",
       " ('o', 'u'),\n",
       " ('i', 'e'),\n",
       " ('t', 'i'),\n",
       " ('m', ' '),\n",
       " ('r', 'D'),\n",
       " ('o', 'n'),\n",
       " ('t', 's'),\n",
       " ('c', 'r'),\n",
       " ('s', 'e'),\n",
       " ('i', 's'),\n",
       " ('u', 'D'),\n",
       " ('c', 'e'),\n",
       " ('D', \"'\"),\n",
       " ('c', 't'),\n",
       " ('e', 'i'),\n",
       " ('n', 'e'),\n",
       " ('u', 'u'),\n",
       " (' ', \"'\"),\n",
       " (' ', ' '),\n",
       " ('o', 'o'),\n",
       " (' ', 'n'),\n",
       " ('c', 's'),\n",
       " ('c', 'o'),\n",
       " ('u', 'n'),\n",
       " ('n', 'm'),\n",
       " ('m', 'o'),\n",
       " ('s', 'u'),\n",
       " ('t', \"'\"),\n",
       " ('u', 'r'),\n",
       " ('r', 'r'),\n",
       " (\"'\", 'e'),\n",
       " ('D', 'o'),\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38540000000000973, 0.8152173913043478)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "lines = [line.strip().split(\"\\t\") for line in open(\"../files/dataset/webpage_terms_pizza.tsv\", \"r\").read().split(\"\\n\")]\n",
    "scores = [float(elem[1]) for elem in lines]\n",
    "min_score, max_score = round(min(scores), 4), round(max(scores), 4)\n",
    "gt = [True if elem[2]==\"TRUE\" else False for elem in lines]\n",
    "opt_accuracy = -100\n",
    "opt_threshold = -100\n",
    "for threshold in np.arange(min_score, max_score, 0.0001):\n",
    "    pred = [True if float(elem[1])>threshold else False for elem in lines]\n",
    "#     pred = [elem for (i,elem) in enumerate(pred) if not gt[i]]\n",
    "    accuracy = accuracy_score(gt, pred)\n",
    "    if accuracy > opt_accuracy:\n",
    "        opt_accuracy = accuracy\n",
    "        opt_threshold = threshold\n",
    "scores_dict = dict([elem[:2] for elem in lines])\n",
    "scores_dict = {elem: float(scores_dict[elem]) > opt_threshold for elem in scores_dict}\n",
    "\n",
    "opt_threshold, opt_accuracy\n",
    "\n",
    "# [elem for elem in lines if]\n",
    "# 0.0001\n",
    "# min_score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2860"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, glob\n",
    "from scipy import spatial\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "file = \"../files/dataset/dominos.txt\"\n",
    "paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "entities = list(set(flatten([preprocess(nlp(para).noun_chunks) for para in paras])))\n",
    "\n",
    "\n",
    "# open(\"../files/dataset/webpage_terms_pizza.tsv\", \"w+\").write(\"\\n\".join(instances))\n",
    "\n",
    "embeds = extractUSEEmbeddings([\"Pizza\"] + entities)\n",
    "open(\"../files/dataset/webpage_terms_pizza.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "#     lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "#     scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}\n",
    "\n",
    "#     def get(key, dictionary):\n",
    "#         try:\n",
    "#             return dictionary[key]\n",
    "#         except KeyboardInterrupt as e:\n",
    "#             sys.exit()\n",
    "#         except:\n",
    "#             print (key)\n",
    "#             dictionary_lower = {elem.lower(): dictionary[elem] for elem in dictionary}\n",
    "#             return dictionary_lower[key.lower()]\n",
    "\n",
    "#     filtered_lines = []\n",
    "#     for elem in all_lines:\n",
    "#         try:\n",
    "#             if get(elem[0], scores_dict) and get(elem[1], scores_dict):\n",
    "#                 filtered_lines.append(elem)\n",
    "#         except:\n",
    "#             print (elem)\n",
    "#     open(file.rsplit(\".\",1)[0] + \"_shortened.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in filtered_lines]))\n",
    "\n",
    "#     # [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "#     # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
