{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"../files/dataset/pizza_train.tsv\"\n",
    "test_file =  \"../files/dataset/pizza_test.tsv\"\n",
    "# instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/pizza_knockedout.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/Pizza/temp/pizza_threshold_7_10/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use_unbracketed.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def preprocess_db(db):\n",
    "    final_db = {}\n",
    "    for key in db:\n",
    "        try:\n",
    "            new_key = key.decode(\"utf-8\")\n",
    "        except:\n",
    "            new_key = key\n",
    "        try:\n",
    "            new_val = db[key].decode(\"utf-8\")\n",
    "        except:\n",
    "            new_val = db[key]\n",
    "        final_db[new_key] = new_val\n",
    "    return final_db\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = [entity_to_id(word2id_db, elem) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def get_instance_key(tup):\n",
    "    return tuple([\" \".join([tok.text for tok in nlp(elem)]) for elem in tup])\n",
    "\n",
    "def parse_instance(tup):\n",
    "    \n",
    "    paths_x = list(instances_db.get(get_instance_key(tup), {}).items())\n",
    "    paths_y = list(instances_db.get(get_instance_key(tup[::-1]), {}).items())\n",
    "    path_count_dict_x = { path.replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { path.replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_instance_dataset(dataset):\n",
    "    parsed_dicts = [parse_instance(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "#     closest_entity = resolved.get(entity, \"\")\n",
    "#     if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "#         success.append(entity)\n",
    "#         return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = preprocess_db(pickle.load(open(prefix + \"pizza_word_to_id_dict.pkl\", \"rb\")))\n",
    "id2word_db = preprocess_db(pickle.load(open(prefix + \"pizza_id_to_word_dict.pkl\", \"rb\")))\n",
    "path2id_db = preprocess_db(pickle.load(open(prefix + \"pizza_path_to_id_dict.pkl\", \"rb\")))\n",
    "id2path_db = preprocess_db(pickle.load(open(prefix + \"pizza_id_to_path_dict.pkl\", \"rb\")))\n",
    "relations_db = preprocess_db(pickle.load(open(prefix + \"pizza_word_occurence_map.pkl\", \"rb\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Instance DB\n",
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path, maxlen):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if maxlen >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks, maxlen):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            filtered = [stringifyFilterPath(path, maxlen) for path in appendingElem]\n",
    "            paths.extend(filtered)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "doc = open(\"../files/dataset/security4.txt\").read()\n",
    "all_nounchunks = list(nlp(doc).noun_chunks).copy()\n",
    "\n",
    "sentences = [list(nlp(nlp(sent.text)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \")).sents)[0]\n",
    "             for sent in nlp(doc).sents]\n",
    "# [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "all_deps = []\n",
    "instances_db = {}\n",
    "for sentence in sentences:\n",
    "    noun_chunks = [n for n in all_nounchunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "    noun_chunks = list(nlp(sentence.text).noun_chunks)\n",
    "    dependencies = getDependencyPaths(sentence, nlp, noun_chunks, 10)\n",
    "    for dep in dependencies:\n",
    "        if not dep:\n",
    "            continue\n",
    "        key = tuple([preprocess_word(nlp(word)) for word in dep[:2]])\n",
    "        path = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in dep[-1].split(\"/\")])\n",
    "        if key not in instances_db:\n",
    "            instances_db[key] = [path]\n",
    "        else:\n",
    "            instances_db[key].append(path)\n",
    "instances_db = {key: Counter(instances_db[key]) for key in instances_db}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# thresholds = [0.5, 0.59, 0.6, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
    "\n",
    "# for threshold in thresholds:\n",
    "threshold = 0.86\n",
    "    \n",
    "failed, success = [], []\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "# test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "# paths_instances, counts_instances, targets_instances  = parse_instance_dataset(test_instances)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "# nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "# nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "# nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "# nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "# print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "# print (len(failed), len(success))\n",
    "# emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "# embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "# emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "# output_file = \"../Input/data_instances_sample.pkl\"\n",
    "# f = open(output_file, \"wb+\")\n",
    "# pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "#              nodes_test, paths_test, counts_test, targets_test,\n",
    "#              nodes_instances, paths_instances, counts_instances, targets_instances,\n",
    "#              nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "#              emb_indexer, emb_indexer_inv, emb_vals, \n",
    "#              pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "# f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([failed, list(word2id_db.keys())], open(\"../junk/failed_words_pizza\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "for i,file in enumerate(sorted(glob.glob(\"../files/dataset/security*\"))):\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    instances_pairs = [\"\\t\".join(list(pair) + [\"none\"]) for pair in instances_pairs if pair]\n",
    "\n",
    "    open(\"../files/dataset/instances\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "db type is dbm.gnu, but the module is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-50d6e68da899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../junk/db_files/pizza_term_to_id.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise error[0](\"db type is {0}, but the module is not \"\n\u001b[0;32m---> 91\u001b[0;31m                        \"available\".format(result))\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: db type is dbm.gnu, but the module is not available"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "shelve.open(\"../junk/db_files/pizza_term_to_id.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'equatorial guinea',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'cameroon',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'brem',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'media',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'morocco',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'cloverfield',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'agisymba',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'denmark',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'elmer',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'don luis',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'bsisa',\n",
       " 'food',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'little shop',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'food',\n",
       " 'fried onion',\n",
       " 'food',\n",
       " 'munchos',\n",
       " 'coconut jam',\n",
       " 'food',\n",
       " 'task force',\n",
       " 'soho',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'soho',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'bellisio foods',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'songpyeon',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'ice cream',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'angola',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'romano cheese',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'burmese tofu',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'electronic media',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'rosa',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'monsuno',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'julius erving',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'street food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'al forno',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'banitsa',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'media',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'itunes store',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'flatbread',\n",
       " 'pizza',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'pizza',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'comestibles',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'andre agassi',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'nicaragua',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'hot',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'jameed',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'dumpling',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'siciliana',\n",
       " 'style',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the bahamas',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'hot',\n",
       " 'hot',\n",
       " 'american',\n",
       " 'food',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'gyeongju national park',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'rudists',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'media',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'garth brooks',\n",
       " 'country',\n",
       " 'country',\n",
       " 'solomon islands',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'ukraine',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'sixteen candles',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the new spirit',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'granola',\n",
       " 'food',\n",
       " 'country',\n",
       " 'moravia',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'american',\n",
       " 'monsters university',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'the jerry springer show',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'mali',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'media',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'food',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'food',\n",
       " 'human food',\n",
       " 'american',\n",
       " 'country',\n",
       " 'american',\n",
       " 'american',\n",
       " 'country',\n",
       " 'ice cream',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489929"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  tylgiv\n",
      "Dropping  valtra\n",
      "Dropping  matsika\n",
      "Dropping  frenstrup\n",
      "Dropping  kakkassery\n",
      "Dropping  only martelly\n",
      "Dropping  n700\n",
      "Dropping  mitteldeutschland\n",
      "Dropping  n5348a\n",
      "Dropping  hiramic\n",
      "Dropping  defined fields\n",
      "Dropping  the s j p harvie professor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a terminating binary expansion\n",
      "Dropping  the online canvas design elements\n",
      "Dropping  the instantaneous angular velocity vector\n",
      "Dropping  fitting anorexic illnesses\n",
      "Dropping  a 1920s proposal\n",
      "Dropping  an international non profit and non governmental student society\n",
      "Dropping  william a trimble\n",
      "Dropping  a provincial regiment\n",
      "Dropping  first real studio experience\n",
      "Dropping  a lycoming o 360 a4 m\n",
      "Dropping  other graphics systems\n",
      "Dropping  polish tradition\n",
      "Dropping  a practising teacher\n",
      "Dropping  close diplomatic and economic relationships\n",
      "Dropping  kiley\n",
      "Dropping  original or reconstructed fabric\n",
      "Dropping  scriptural or customary laws\n",
      "Dropping  national economics challenge champions\n",
      "Dropping  a long horizontal jump\n",
      "Dropping  the open bloodstream\n",
      "Dropping  the officer s blooded horses\n",
      "Dropping  classical comedy\n",
      "Dropping  the continental exchanges\n",
      "Dropping  the most frequent uses\n",
      "Dropping  major local developers\n",
      "Dropping  184 restaurants\n",
      "Dropping  maria s young son\n",
      "Dropping  utsu\n",
      "Dropping  archeologist hugo winckler\n",
      "Dropping  zp120\n",
      "Dropping  the early 1950s dubuffet\n",
      "Dropping  merina and betsileo families\n",
      "Dropping  impersonalization\n",
      "Dropping  all necessary activities\n",
      "Dropping  more complex background settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15446:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "Exception in thread Thread-15449:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  periodic recitals\n",
      "Dropping  last weekend s post coup presidential election\n",
      "Dropping  so2 james suh\n",
      "Dropping  silvie iii\n",
      "Dropping  pot au feu\n",
      "Dropping  its operational readiness\n",
      "Dropping  no one reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15472:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  the exterior mirror\n",
      "Dropping  free agent greg holland\n",
      "Dropping  keio university hospital\n",
      "Dropping  negative at skew\n",
      "Dropping  the former coalfield area\n",
      "Dropping  a coherent personality\n",
      "Dropping  intevation\n",
      "Dropping  fgm 148 javelin\n",
      "Dropping  17 august robert ritter von greim s fliegerkorps v\n",
      "Dropping  neither military training\n",
      "Dropping  self service passport control\n",
      "Dropping  sierra s salon\n",
      "Dropping  general no l de castelnau\n",
      "Dropping  debra delee\n",
      "Dropping  davis second term\n",
      "Dropping  the oldest literary account\n",
      "Dropping  each wall inlet\n",
      "Dropping  the people s nomadic heritage\n",
      "Dropping  glasgow academicals\n",
      "Dropping  fine v fib\n",
      "Dropping  flat end facets\n",
      "Dropping  dense grids\n",
      "Dropping  professor dominique martin\n",
      "Dropping  the fastest overall driver\n",
      "Dropping  their sledging rations\n",
      "Dropping  the lambda company\n",
      "Dropping  the additional rail\n",
      "Dropping  maintenance flaws\n",
      "Dropping  a 75 cm long bundle\n",
      "Dropping  179 fs\n",
      "Dropping  military miniatures\n",
      "Dropping  performance and management flexibility\n",
      "Dropping  two state run polytechnic schools\n",
      "Dropping  scriabin s museum\n",
      "Dropping  protestant dublin lawyer theobald wolfe tone\n",
      "Dropping  16 canadians\n",
      "Dropping  the individual coal plots\n",
      "Dropping  i e visemes\n",
      "Dropping  a e36 m3 compact prototype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15713:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  sal n de la paz\n",
      "Dropping  brian williams lustmord project\n",
      "Dropping  an exponential behavior\n",
      "Dropping  this uncommon case\n",
      "Dropping  only 13 more performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15739:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  then a third wrestling team\n",
      "Dropping  an old watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15748:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a sophisticated propaganda machine\n",
      "Dropping  a successful and effective program\n",
      "Dropping  50 s strongest track\n",
      "Dropping  the yshphh\n",
      "Dropping  the estimated sinking position\n",
      "Dropping  phoenix s citizens\n",
      "Dropping  the cbbb\n",
      "Dropping  re arranged panels\n",
      "Dropping  his 50th birthday celebration\n",
      "Dropping  the male eggs\n",
      "Dropping  montane meadows\n",
      "Dropping  the troops good spirit\n",
      "Dropping  paltrow s performance\n",
      "Dropping  a free demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15808:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  roxy attempts\n",
      "Dropping  either deletion\n",
      "Dropping  frictional behavior\n",
      "Dropping  four successive popes\n",
      "Dropping  engineering design teams\n",
      "Dropping  felix the cat\n",
      "Dropping  tidal venuses\n",
      "Dropping  dsquared2 duo dean and dan caten\n",
      "Dropping  cooper s most important film\n",
      "Dropping  consistent subtest scores\n",
      "Dropping  frances hegarty\n",
      "Dropping  borland s guitar playing\n",
      "Dropping  ahsura\n",
      "Dropping  an unnamed polish clone\n",
      "Dropping  at least the a credit rating\n",
      "Dropping  a radio based transatlantic telephone service\n",
      "Dropping  carddass exclusive storyline series\n",
      "Dropping  a balance sheet hedge\n",
      "Dropping  bluebush saltbush steppe\n"
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "allkeys = list(word2id_db.getall())\n",
    "for key in allkeys:\n",
    "    try:\n",
    "        word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "    except:\n",
    "        print (\"Dropping \", key)\n",
    "        word2id_db_corrected[key] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = key\n",
    "word2id_db_corrected.dump()\n",
    "id2word_db_corrected.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015409708023071289\n",
      "0.0001728534698486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "a = nlp(\"beach on the ocean\")\n",
    "print (time.time()-t)\n",
    "t = time.time()\n",
    "preprocess_word(a)\n",
    "print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "lines = [l.split(\"\\t\")[:3] for l in open(\"../files/dataset/pizza_knockedout.tsv\", \"r\").read().split(\"\\n\")]\n",
    "final_lines = []\n",
    "for line in lines:\n",
    "    elem = line\n",
    "    if random.random()>0.5:\n",
    "        label = line[2]\n",
    "        if label == \"hyponym\":\n",
    "            label = \"hypernym\"\n",
    "        elif label == \"hypernym\":\n",
    "            label = \"hyponym\"\n",
    "        elif label == \"concept\":\n",
    "            label = \"instance\"\n",
    "        elif label == \"instance\":\n",
    "            label = \"concept\"\n",
    "        elem = [line[1], line[0], label]\n",
    "    final_lines.append(elem)\n",
    "random.shuffle(final_lines)\n",
    "# final_lines_none = [elem for elem in final_lines if elem==\"none\"]\n",
    "# final_lines_none_train = final_lines_none[:int(0.9 * len(final_lines_none))]\n",
    "# final_lines_none_test = final_lines_none[int(0.9 * len(final_lines_none)):]\n",
    "\n",
    "# final_lines_rest = [elem for elem in final_lines if elem!=\"none\"]\n",
    "# final_lines_rest_train = final_lines_rest[:int(0.9 * len(final_lines_rest))]\n",
    "# final_lines_rest_test = final_lines_rest[int(0.9 * len(final_lines_rest)):]\n",
    "\n",
    "# final_lines_train = final_lines_none_train + final_lines_rest_train\n",
    "# final_lines_test = final_lines_none_test + final_lines_rest_test\n",
    "\n",
    "open(\"../files/dataset/pizza_knockedout_shuffled.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines]))\n",
    "# open(\"../files/dataset/pizza_test.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a985eb5940c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mscores_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mopt_threshold\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a985eb5940c2>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mscores_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mopt_threshold\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "import sys, glob\n",
    "from scipy import spatial\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "for file in glob.glob(\"../files/dataset/pizza_webpages/*\"):\n",
    "    all_lines = [l.split(\"\\t\") for l in open(file).read().split(\"\\n\")]\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    entities = list(set(flatten([line[:2] for line in all_lines])))\n",
    "\n",
    "    embeds = extractUSEEmbeddings([\"Pizza\"] + entities)\n",
    "    # open(\"../files/dataset/webpage_terms.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "    lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "    scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}\n",
    "\n",
    "    def get(key, dictionary):\n",
    "        try:\n",
    "            return dictionary[key]\n",
    "        except KeyboardInterrupt as e:\n",
    "            sys.exit()\n",
    "        except:\n",
    "            print (key)\n",
    "            dictionary_lower = {elem.lower(): dictionary[elem] for elem in dictionary}\n",
    "            return dictionary_lower[key.lower()]\n",
    "\n",
    "    filtered_lines = []\n",
    "    for elem in all_lines:\n",
    "        try:\n",
    "            if get(elem[0], scores_dict) and get(elem[1], scores_dict):\n",
    "                filtered_lines.append(elem)\n",
    "        except:\n",
    "            print (elem)\n",
    "    open(file.rsplit(\".\",1)[0] + \"_shortened.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in filtered_lines]))\n",
    "\n",
    "    # [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "    # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "lines = [line.strip().split(\"\\t\") for line in open(\"../files/dataset/webpage_terms.tsv\", \"r\").read().split(\"\\n\")]\n",
    "scores = [float(elem[1]) for elem in lines]\n",
    "min_score, max_score = round(min(scores), 4), round(max(scores), 4)\n",
    "gt = [True if elem[2]==\"TRUE\" else False for elem in lines]\n",
    "opt_accuracy = -100\n",
    "opt_threshold = -100\n",
    "for threshold in np.arange(min_score, max_score, 0.0001):\n",
    "    pred = [True if float(elem[1])>threshold else False for elem in lines]\n",
    "#     pred = [elem for (i,elem) in enumerate(pred) if not gt[i]]\n",
    "    accuracy = accuracy_score(gt, pred)\n",
    "    if accuracy > opt_accuracy:\n",
    "        opt_accuracy = accuracy\n",
    "        opt_threshold = threshold\n",
    "scores_dict = dict([elem[:2] for elem in lines])\n",
    "scores_dict = {elem: float(scores_dict[elem]) > opt_threshold for elem in scores_dict}\n",
    "\n",
    "opt_threshold, opt_accuracy\n",
    "\n",
    "# [elem for elem in lines if]\n",
    "# 0.0001\n",
    "# min_score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When input_signature is provided, all inputs to the Python function must be convertible to tensors:\n  inputs: (\n    ['Pizza', ['you', 'we', 'comfort', 'ones', 'crispy and melty pizza', 'own home'], [\"Domino's it\"], ['Rishton ka time'], ['it', 'kid', 'treat', 'who', 'wife', 'promotion', 'class', 'work', 'long day', 'heart'], ['things', 'perspective', 'celebration', 'cheesy slice', 'one', 'best pizza'], ['you', 'restaurant'], ['call, a few clicks', 'you', 'doorstep', 'website', 'mobile screen', 'pocket', 'few touches'], ['everyone', 'something'], ['pizza slice', 'who', 'lovers', 'they', 'sides', 'last bite', 'time', 'non-vegetarians', 'something', 'last slice', 'ones', 'vegetarians'], ['pizza', 'situation'], ['chicken tikka-', 'golden corn', 'chicken sausage', 'non-veg toppings', 'pepper barbeque chicken', 'toppings - from paneer', 'grilled mushroom', 'grilled chicken rasher', 'jalapeno', 'extra cheese', 'crisp capsicum', 'red paprika', 'onion', 'fresh tomato', 'you', 'black olives', 'pizza', 'freedom', 'peri-peri chicken'], ['top', 'options', 'you', 'choice', 'crust'], ['fresh pan pizza', 'New hand-tossed crust', 'classic hand-tossed crust', 'cheese', 'wheat thin crust'], ['Domino', 'pizza', 'you'], ['region', \"Domino's menu\"], [\"current Domino's menu\", 'United States', 'Italian-American main and side dishes', 'variety'], ['custom pizzas', 'crust styles', 'traditional, specialty', 'toppings', 'Pizza', 'primary focus', 'variety'], [\"In 2011, Domino's launched artisan-style pizzas\"], ['bread bowls', 'pasta', 'oven-baked sandwiches', 'Additional entrees'], ['chicken and bread sides', 'desserts', 'beverages', 'menu'], ['region', \"Domino's menu\"], [\"current Domino's menu\", 'United States', 'Italian-American main and side dishes', 'variety'], ['custom pizzas', 'crust styles', 'traditional, specialty', 'toppings', 'Pizza', 'primary focus', 'variety'], [\"In 2011, Domino's launched artisan-style pizzas\"], ['chicken and bread sides', 'desserts', 'beverages', 'menu']])\n  input_signature: (\n    TensorSpec(shape=<unknown>, dtype=tf.string, name=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m   2700\u001b[0m         flatten_inputs[index] = ops.convert_to_tensor(\n\u001b[0;32m-> 2701\u001b[0;31m             value, dtype_hint=spec.dtype)\n\u001b[0m\u001b[1;32m   2702\u001b[0m         \u001b[0mneed_packing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-388ecff8486d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# open(\"../files/dataset/webpage_terms_pizza.tsv\", \"w+\").write(\"\\n\".join(instances))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractUSEEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Pizza\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../files/dataset/webpage_terms_pizza.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e6909ff04b6c>\u001b[0m in \u001b[0;36mextractUSEEmbeddings\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractUSEEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3170\u001b[0m       args, kwargs = self._function_spec.canonicalize_function_inputs(\n\u001b[0;32m-> 3171\u001b[0;31m           *args, **kwargs)\n\u001b[0m\u001b[1;32m   3172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3173\u001b[0m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2620\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m           self._flat_input_signature)\n\u001b[0m\u001b[1;32m   2623\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m   2705\u001b[0m                          \u001b[0;34m\"the Python function must be convertible to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m                          \u001b[0;34m\"tensors:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2707\u001b[0;31m                          format_error_message(inputs, input_signature))\n\u001b[0m\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m   if any(not spec.is_compatible_with(other) for spec, other in zip(\n",
      "\u001b[0;31mValueError\u001b[0m: When input_signature is provided, all inputs to the Python function must be convertible to tensors:\n  inputs: (\n    ['Pizza', ['you', 'we', 'comfort', 'ones', 'crispy and melty pizza', 'own home'], [\"Domino's it\"], ['Rishton ka time'], ['it', 'kid', 'treat', 'who', 'wife', 'promotion', 'class', 'work', 'long day', 'heart'], ['things', 'perspective', 'celebration', 'cheesy slice', 'one', 'best pizza'], ['you', 'restaurant'], ['call, a few clicks', 'you', 'doorstep', 'website', 'mobile screen', 'pocket', 'few touches'], ['everyone', 'something'], ['pizza slice', 'who', 'lovers', 'they', 'sides', 'last bite', 'time', 'non-vegetarians', 'something', 'last slice', 'ones', 'vegetarians'], ['pizza', 'situation'], ['chicken tikka-', 'golden corn', 'chicken sausage', 'non-veg toppings', 'pepper barbeque chicken', 'toppings - from paneer', 'grilled mushroom', 'grilled chicken rasher', 'jalapeno', 'extra cheese', 'crisp capsicum', 'red paprika', 'onion', 'fresh tomato', 'you', 'black olives', 'pizza', 'freedom', 'peri-peri chicken'], ['top', 'options', 'you', 'choice', 'crust'], ['fresh pan pizza', 'New hand-tossed crust', 'classic hand-tossed crust', 'cheese', 'wheat thin crust'], ['Domino', 'pizza', 'you'], ['region', \"Domino's menu\"], [\"current Domino's menu\", 'United States', 'Italian-American main and side dishes', 'variety'], ['custom pizzas', 'crust styles', 'traditional, specialty', 'toppings', 'Pizza', 'primary focus', 'variety'], [\"In 2011, Domino's launched artisan-style pizzas\"], ['bread bowls', 'pasta', 'oven-baked sandwiches', 'Additional entrees'], ['chicken and bread sides', 'desserts', 'beverages', 'menu'], ['region', \"Domino's menu\"], [\"current Domino's menu\", 'United States', 'Italian-American main and side dishes', 'variety'], ['custom pizzas', 'crust styles', 'traditional, specialty', 'toppings', 'Pizza', 'primary focus', 'variety'], [\"In 2011, Domino's launched artisan-style pizzas\"], ['chicken and bread sides', 'desserts', 'beverages', 'menu']])\n  input_signature: (\n    TensorSpec(shape=<unknown>, dtype=tf.string, name=None))"
     ]
    }
   ],
   "source": [
    "import sys, glob\n",
    "from scipy import spatial\n",
    "\n",
    "# Returns cosine similarity of two vectors\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "file = \"../files/dataset/dominos.txt\"\n",
    "paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "\n",
    "\n",
    "open(\"../files/dataset/webpage_terms.tsv\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n",
    "\n",
    "    \n",
    "\n",
    "    all_lines = [l.split(\"\\t\") for l in open(file).read().split(\"\\n\")]\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    entities = list(set(flatten([line[:2] for line in all_lines])))\n",
    "\n",
    "    embeds = extractUSEEmbeddings([\"Pizza\"] + entities)\n",
    "    # open(\"../files/dataset/webpage_terms.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([entities[i], str(cos_sim(elem, embeds[0]))]) for i,elem in enumerate(embeds[1:])])) \n",
    "\n",
    "    lines = [(entities[i], cos_sim(elem, embeds[0])) for i,elem in enumerate(embeds[1:])]\n",
    "    scores_dict = {elem[0]: elem[1]>opt_threshold for elem in lines}\n",
    "\n",
    "    def get(key, dictionary):\n",
    "        try:\n",
    "            return dictionary[key]\n",
    "        except KeyboardInterrupt as e:\n",
    "            sys.exit()\n",
    "        except:\n",
    "            print (key)\n",
    "            dictionary_lower = {elem.lower(): dictionary[elem] for elem in dictionary}\n",
    "            return dictionary_lower[key.lower()]\n",
    "\n",
    "    filtered_lines = []\n",
    "    for elem in all_lines:\n",
    "        try:\n",
    "            if get(elem[0], scores_dict) and get(elem[1], scores_dict):\n",
    "                filtered_lines.append(elem)\n",
    "        except:\n",
    "            print (elem)\n",
    "    open(file.rsplit(\".\",1)[0] + \"_shortened.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in filtered_lines]))\n",
    "\n",
    "    # [elem for elem in filtered_lines if elem[-2]!=\"none\"]\n",
    "    # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'we', 'comfort', 'ones', 'crispy and melty pizza', 'own home'],\n",
       " [\"Domino's it\"],\n",
       " ['Rishton ka time'],\n",
       " ['it',\n",
       "  'kid',\n",
       "  'treat',\n",
       "  'who',\n",
       "  'wife',\n",
       "  'promotion',\n",
       "  'class',\n",
       "  'work',\n",
       "  'long day',\n",
       "  'heart'],\n",
       " ['things', 'perspective', 'celebration', 'cheesy slice', 'one', 'best pizza'],\n",
       " ['you', 'restaurant'],\n",
       " ['call, a few clicks',\n",
       "  'you',\n",
       "  'doorstep',\n",
       "  'website',\n",
       "  'mobile screen',\n",
       "  'pocket',\n",
       "  'few touches'],\n",
       " ['everyone', 'something'],\n",
       " ['pizza slice',\n",
       "  'who',\n",
       "  'lovers',\n",
       "  'they',\n",
       "  'sides',\n",
       "  'last bite',\n",
       "  'time',\n",
       "  'non-vegetarians',\n",
       "  'something',\n",
       "  'last slice',\n",
       "  'ones',\n",
       "  'vegetarians'],\n",
       " ['pizza', 'situation'],\n",
       " ['chicken tikka-',\n",
       "  'golden corn',\n",
       "  'chicken sausage',\n",
       "  'non-veg toppings',\n",
       "  'pepper barbeque chicken',\n",
       "  'toppings - from paneer',\n",
       "  'grilled mushroom',\n",
       "  'grilled chicken rasher',\n",
       "  'jalapeno',\n",
       "  'extra cheese',\n",
       "  'crisp capsicum',\n",
       "  'red paprika',\n",
       "  'onion',\n",
       "  'fresh tomato',\n",
       "  'you',\n",
       "  'black olives',\n",
       "  'pizza',\n",
       "  'freedom',\n",
       "  'peri-peri chicken'],\n",
       " ['top', 'options', 'you', 'choice', 'crust'],\n",
       " ['fresh pan pizza',\n",
       "  'New hand-tossed crust',\n",
       "  'classic hand-tossed crust',\n",
       "  'cheese',\n",
       "  'wheat thin crust'],\n",
       " ['Domino', 'pizza', 'you'],\n",
       " ['region', \"Domino's menu\"],\n",
       " [\"current Domino's menu\",\n",
       "  'United States',\n",
       "  'Italian-American main and side dishes',\n",
       "  'variety'],\n",
       " ['custom pizzas',\n",
       "  'crust styles',\n",
       "  'traditional, specialty',\n",
       "  'toppings',\n",
       "  'Pizza',\n",
       "  'primary focus',\n",
       "  'variety'],\n",
       " [\"In 2011, Domino's launched artisan-style pizzas\"],\n",
       " ['bread bowls', 'pasta', 'oven-baked sandwiches', 'Additional entrees'],\n",
       " ['chicken and bread sides', 'desserts', 'beverages', 'menu'],\n",
       " ['region', \"Domino's menu\"],\n",
       " [\"current Domino's menu\",\n",
       "  'United States',\n",
       "  'Italian-American main and side dishes',\n",
       "  'variety'],\n",
       " ['custom pizzas',\n",
       "  'crust styles',\n",
       "  'traditional, specialty',\n",
       "  'toppings',\n",
       "  'Pizza',\n",
       "  'primary focus',\n",
       "  'variety'],\n",
       " [\"In 2011, Domino's launched artisan-style pizzas\"],\n",
       " ['chicken and bread sides', 'desserts', 'beverages', 'menu']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
