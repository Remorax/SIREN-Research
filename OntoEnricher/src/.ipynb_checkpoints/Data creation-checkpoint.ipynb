{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import spacy, subprocess, itertools, logging\n",
    "from spacy.tokens.token import Token\n",
    "import concurrent.futures\n",
    "\n",
    "def splitFile (file, n):\n",
    "    inputfile = open(file, 'r')\n",
    "    output = None\n",
    "    suffix = 0\n",
    "    for (i, line) in enumerate(inputfile):\n",
    "        if i % n == 0:\n",
    "            if output:\n",
    "                output.close()\n",
    "            output = open(file + \"_split_\" + str(suffix) + '.txt', 'w+')\n",
    "            suffix += 1\n",
    "        output.write(line)\n",
    "    output.close()\n",
    "    return suffix\n",
    "\n",
    "\n",
    "def parseText(idx, fileName):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "    logging.info(\"Im here\")\n",
    "    file = fileName + \"_split_\" + str(idx) + \".txt\"\n",
    "    op = fileName + \"_parsed_\" + str(idx)\n",
    "\n",
    "    with open(file, \"r\") as inp:\n",
    "        with open(op, \"w+\") as out:\n",
    "            for i,para in enumerate(inp):\n",
    "                if not para.strip(): continue\n",
    "                noun_chunks = nlp(para).noun_chunks\n",
    "                sentences = nlp(para.strip()).sents\n",
    "                for sentence in sentences:\n",
    "                    if \"<doc id=\" in sentence.text or \"</doc>\" in sentence.text:\n",
    "                        continue\n",
    "                    noun_chunks_sent = [n for n in noun_chunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "                    dependencies = get_dependency_paths(sentence, nlp, noun_chunks_sent)\n",
    "                    if dependencies:\n",
    "                        allpaths = [\"\\t\".join(path) for path in dependencies]\n",
    "                        op.write(\"\\n\".join(allpaths))\n",
    "                            \n",
    "def main(file):\n",
    "    \n",
    "    countlines = \"wc -l \" + file \n",
    "    output, _ = subprocess.Popen(countlines.split(), stdout=subprocess.PIPE).communicate()\n",
    "    n = int(output.decode(\"utf-8\").strip().split(\" \")[0]) + 1\n",
    "    m = int(n/20)\n",
    "    suffix = splitFile(file, m)\n",
    "    \n",
    "    print (\"hi\")\n",
    "    formatt = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=formatt, level=logging.INFO,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        executor.map(parseText, [(i, file) for i in range(20)])\n",
    "    print (\"done\")     \n",
    "\n",
    "def get_dependency_paths(sentence, nlp, noun_chunks_sent):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in noun_chunks_sent]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN'])\n",
    "\n",
    "    concepts_paired = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and el[0] != el[1]]\n",
    "    concepts_paired = list(dict.fromkeys(concepts_paired))\n",
    "\n",
    "    paths = [path for path in map(getShortestPath, term_pairs) if path]\n",
    "    logging.info(paths)\n",
    "#     paths = [p for path in paths for p in get_satellite_links(path)]\n",
    "#     paths = [path for path in map(clean_path, paths) if path is not None]\n",
    "\n",
    "#     return paths\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "        \n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return elem.root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), y)\n",
    "    return (len(y), x)\n",
    "\n",
    "def findLowestCommonHead(x_path, y_path, minLength, biggerArray):\n",
    "    lowest_common_head = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if x_path[i]!=y_path[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0]-1\n",
    "            lowest_common_head = biggerArray[idx]\n",
    "        else:\n",
    "            idx = minLength\n",
    "            lowest_common_head = biggerArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if x_path:\n",
    "            lowest_common_head = x_path[0]\n",
    "        elif y_path:\n",
    "            lowest_common_head = y_path[0]\n",
    "        else:\n",
    "            lowest_common_head = None\n",
    "    \n",
    "    return idx, lowest_common_head\n",
    "\n",
    "def filterPaths(function, lowest_common_head, paths):\n",
    "    path1 = [lowest_common_head]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    x, y = breakCompoundWords(tup[0]), breakCompoundWords(tup[1])\n",
    "    \n",
    "    x_path, y_path = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, biggerArray = findMinLength(x_path, y_path)\n",
    "    \n",
    "    idx, lowest_common_head = findLowestCommonHead(x_path, y_path, minLength, biggerArray)\n",
    "    \n",
    "    try:\n",
    "        x_path = x_path[idx+1:]\n",
    "        y_path = y_path[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowest_common_head and (filterPaths(checkLeft, lowest_common_head, x_path) or filterPaths(checkRight, lowest_common_head, y_path)):\n",
    "            return None\n",
    "        x_path = x_path[::-1]\n",
    "        return (x, x_path, lowest_common_head, y_path, y)\n",
    "    except:\n",
    "        return (x, x_path, lowest_common_head, y_path, y)\n",
    "\n",
    "    \n",
    "main(\"../junk/temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_function(name):\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    time.sleep(2)\n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        executor.map(thread_function, range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from docopt import docopt\n",
    "\n",
    "def get_satellite_links(x_hx_lch_hy_y):\n",
    "    \"\"\"\n",
    "    Add the \"satellites\" - single links not already contained in the dependency path added on either side of each noun\n",
    "    :param x: the X token\n",
    "    :param y: the Y token\n",
    "    :param hx: X's head tokens\n",
    "    :param hy: Y's head tokens\n",
    "    :param lch: the lowest common ancestor of X and Y\n",
    "    :return: more paths, with satellite links\n",
    "    \"\"\"\n",
    "    (x, hx, lch, hy, y) = x_hx_lch_hy_y\n",
    "    paths = [(None, x, hx, lch, hy, y, None)]\n",
    "\n",
    "    x_lefts = [tok for tok in x.lefts]\n",
    "    if len(x_lefts) > 0 and x_lefts[0].tag_ != 'PUNCT' and len(x_lefts[0].string.strip()) > 1:\n",
    "        paths.append((x_lefts[0], x, hx, lch, hy, y, None))\n",
    "\n",
    "    y_rights = [tok for tok in y.rights]\n",
    "    if len(y_rights) > 0 and y_rights[0].tag_ != 'PUNCT' and len(y_rights[0].string.strip()) > 1:\n",
    "        paths.append((None, x, hx, lch, hy, y, y_rights[0]))\n",
    "\n",
    "    return paths\n",
    "\n",
    "f = open(\"logging\", \"w+\")\n",
    "\n",
    "def parse_sentence(sent,nlp):\n",
    "    \"\"\"\n",
    "    Get all the dependency paths between nouns in the sentence\n",
    "    :param sent: the sentence to parse\n",
    "    :return: the list of entities and paths\n",
    "    \"\"\"\n",
    "    # Get all noun indices\n",
    "    indices = [(token, i, i) for i, token in enumerate(sent) if token.tag_[:2] == 'NN' and len(token.string.strip()) > 2]\n",
    "    # Add noun chunks for the current sentence\n",
    "    # Don't include noun chunks with only one word - these are nouns already included\n",
    "    indices.extend([(np, np.start, np.end) for np in nlp(sent.doc.text).noun_chunks if sent.start <= np.start < np.end - 1 < sent.end])\n",
    "\n",
    "    # Get all dependency paths between nouns, up to length 4\n",
    "    term_pairs = [(x[0], y[0]) for x in indices for y in indices if x[2] < y[1]]\n",
    "#     return term_pairs\n",
    "    paths = [path for path in map(shortest_path, term_pairs) if path is not None]\n",
    "#     paths = [p for path in paths for p in get_satellite_links(path)]\n",
    "#     paths = [path for path in map(clean_path, paths) if path is not None]\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def shortest_path(x_y):\n",
    "    \"\"\" Returns the shortest dependency path from x to y\n",
    "    :param x: x token\n",
    "    :param y: y token\n",
    "    :return: the shortest dependency path from x to y\n",
    "    \"\"\"\n",
    "    \n",
    "    (x,y) = x_y\n",
    "\n",
    "    x_token = x\n",
    "    y_token = y\n",
    "    if not isinstance(x_token, spacy.tokens.token.Token):\n",
    "        x_token = x_token.root\n",
    "    if not isinstance(y_token, spacy.tokens.token.Token):\n",
    "        y_token = y_token.root\n",
    "\n",
    "    # Get the path from the root to each of the tokens\n",
    "    hx = heads(x_token)\n",
    "    hy = heads(y_token)\n",
    "#     return (x, hx,  hy, y)\n",
    "    # Get the lowest common head\n",
    "    i = -1\n",
    "    for i in range(min(len(hx), len(hy))):\n",
    "        if hx[i] is not hy[i]:\n",
    "            break\n",
    "    try:\n",
    "        f.write(str((hx, hy, type(hx[0]), type(hy[0]) x, y, i)) + \"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    if i == -1:\n",
    "        lch_idx = 0\n",
    "        if len(hy) > 0:\n",
    "            lch = hy[lch_idx]\n",
    "        elif len(hx) > 0:\n",
    "            lch = hx[lch_idx]\n",
    "        else:\n",
    "            lch = None\n",
    "    elif hx[i] == hy[i]:\n",
    "        lch_idx = i\n",
    "        lch = hx[lch_idx]\n",
    "    else:\n",
    "        lch_idx = i-1\n",
    "        lch = hx[lch_idx]\n",
    "    return (x, hx, lch_idx, lch, hy, y)\n",
    "    # The path from x to the lowest common head\n",
    "    hx = hx[lch_idx+1:]\n",
    "    if lch and check_direction(lch, hx, lambda h: h.lefts):\n",
    "        return None\n",
    "    hx = hx[::-1]\n",
    "\n",
    "    # The path from the lowest common head to y\n",
    "    hy = hy[lch_idx+1:]\n",
    "    if lch and check_direction(lch, hy, lambda h: h.rights):\n",
    "        return None\n",
    "#     print (x, y, hx, hy)\n",
    "#     return (x, hx, lch, hy, y)\n",
    "\n",
    "\n",
    "def heads(token):\n",
    "    \"\"\"\n",
    "    Return the heads of a token, from the root down to immediate head\n",
    "    :param token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    t = token\n",
    "    hs = []\n",
    "    while t != t.head:\n",
    "        t = t.head\n",
    "        hs.append(t)\n",
    "    return hs[::-1]\n",
    "\n",
    "\n",
    "def check_direction(lch, hs, f_dir):\n",
    "    \"\"\"\n",
    "    Make sure that the path between the term and the lowest common head is in a certain direction\n",
    "    :param lch: the lowest common head\n",
    "    :param hs: the path from the lowest common head to the term\n",
    "    :param f_dir: function of direction\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return any(modifier not in f_dir(head) for head, modifier in zip([lch] + hs[:-1], hs))\n",
    "\n",
    "def edge_to_string(token, is_head=False):\n",
    "    \"\"\"\n",
    "    Converts the token to an edge string representation\n",
    "    :param token: the token\n",
    "    :return: the edge string\n",
    "    \"\"\"\n",
    "    t = token\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        t = token.root\n",
    "\n",
    "    return '/'.join([token_to_lemma(token), t.pos_, t.dep_ if t.dep_ != '' and not is_head else 'ROOT'])\n",
    "\n",
    "\n",
    "def argument_to_string(token, edge_name):\n",
    "    \"\"\"\n",
    "    Converts the argument token (X or Y) to an edge string representation\n",
    "    :param token: the X or Y token\n",
    "    :param edge_name: 'X' or 'Y'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        token = token.root\n",
    "\n",
    "    return '/'.join([edge_name, token.pos_, token.dep_ if token.dep_ != '' else 'ROOT'])\n",
    "\n",
    "\n",
    "def direction(dir):\n",
    "    \"\"\"\n",
    "    Print the direction of the edge\n",
    "    :param dir: the direction\n",
    "    :return: a string representation of the direction\n",
    "    \"\"\"\n",
    "    # Up to the head\n",
    "    if dir == UP:\n",
    "        return '>'\n",
    "    # Down from the head\n",
    "    elif dir == DOWN:\n",
    "        return '<'\n",
    "\n",
    "\n",
    "def token_to_string(token):\n",
    "    \"\"\"\n",
    "    Convert the token to string representation\n",
    "    :param token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        return ' '.join([t.string.strip().lower() for t in token])\n",
    "    else:\n",
    "        return token.string.strip().lower()\n",
    "\n",
    "\n",
    "def token_to_lemma(token):\n",
    "    \"\"\"\n",
    "    Convert the token to string representation\n",
    "    :param token: the token\n",
    "    :return: string representation of the token\n",
    "    \"\"\"\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        return token_to_string(token)\n",
    "    else:\n",
    "        return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def clean_path(allParams):\n",
    "    \"\"\"\n",
    "    Filter out long paths and pretty print the short ones\n",
    "    :return: the string representation of the path\n",
    "    \"\"\"\n",
    "    (set_x, x, hx, lch, hy, y, set_y) = allParams\n",
    "    set_path_x = []\n",
    "    set_path_y = []\n",
    "    lch_lst = []\n",
    "\n",
    "    if set_x:\n",
    "        set_path_x = [edge_to_string(set_x) + direction(DOWN)]\n",
    "    if set_y:\n",
    "        set_path_y = [direction(UP) + edge_to_string(set_y)]\n",
    "\n",
    "    # X is the head\n",
    "    if isinstance(x, spacy.tokens.token.Token) and lch == x:\n",
    "        dir_x = ''\n",
    "        dir_y = direction(DOWN)\n",
    "    # Y is the head\n",
    "    elif isinstance(y, spacy.tokens.token.Token) and lch == y:\n",
    "        dir_x = direction(UP)\n",
    "        dir_y = ''\n",
    "    # X and Y are not heads\n",
    "    else:\n",
    "        lch_lst = [edge_to_string(lch, is_head=True)] if lch else []\n",
    "        dir_x = direction(UP)\n",
    "        dir_y = direction(DOWN)\n",
    "\n",
    "    len_path = len(hx) + len(hy) + len(set_path_x) + len(set_path_y) + len(lch_lst)\n",
    "\n",
    "    if len_path <= MAX_PATH_LEN:\n",
    "        cleaned_path = '_'.join(set_path_x + [argument_to_string(x, 'X') + dir_x] +\n",
    "                                [edge_to_string(token) + direction(UP) for token in hx] +\n",
    "                                lch_lst +\n",
    "                                [direction(DOWN) + edge_to_string(token) for token in hy] +\n",
    "                                [dir_y + argument_to_string(y, 'Y')] + set_path_y)\n",
    "        return token_to_string(x), token_to_string(y), cleaned_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Constants\n",
    "MAX_PATH_LEN = 4\n",
    "UP = 1\n",
    "DOWN = 2\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "in_file = \"../junk/temp_split_0.txt\"\n",
    "out_file = \"../junk/temp_PARSED\"\n",
    "# in_file = \"ISO_IEC_27001/ISO IEC 27018-2014.txt\"\n",
    "# out_file = \"Security_file_1.txt\"\n",
    "with codecs.open(in_file, 'r', 'utf-8') as f_in:\n",
    "    with codecs.open(out_file, 'w', 'utf-8') as f_out:\n",
    "\n",
    "        # Read the next paragraph\n",
    "        for (i,paragraph) in enumerate(f_in):\n",
    "            # Skip empty lines\n",
    "            paragraph = paragraph.replace(\"'''\", '').strip()\n",
    "            if len(paragraph) == 0:\n",
    "                continue\n",
    "\n",
    "            parsed_par = nlp(str(paragraph))\n",
    "            # Parse each sentence separately\n",
    "            for sent in parsed_par.sents:\n",
    "                dependency_paths = parse_sentence(sent,nlp)\n",
    "                if len(dependency_paths) > 0:\n",
    "                    print ('\\n'.join([str(path) for path in dependency_paths]), file=f_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main line\n",
      "module name: __main__\n",
      "parent process: 53357\n",
      "process id: 53369\n",
      "function f\n",
      "module name: __main__\n",
      "parent process: 53369\n",
      "process id: 59495\n",
      "hello bob\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "def info(title):\n",
    "    print (title)\n",
    "    print ('module name:', __name__)\n",
    "    if hasattr(os, 'getppid'):  # only available on Unix\n",
    "        print ('parent process:', os.getppid())\n",
    "    print ('process id:', os.getpid())\n",
    "\n",
    "def f(name):\n",
    "    info('function f')\n",
    "    print ('hello', name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    info('main line')\n",
    "    p = Process(target=f, args=('bob',))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
