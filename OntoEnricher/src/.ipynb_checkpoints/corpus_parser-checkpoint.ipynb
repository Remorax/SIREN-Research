{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsddb3 import btopen\n",
    "import bcolz, pickle, os, sys, shelve, time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "train_file = \"../files/dataset/train.tsv\"\n",
    "test_file = \"../files/dataset/test.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat\"\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "use_embeddings = \"../files/embeddings.pt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "# relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"/data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\"\n",
    "op_file = \"dataset_parsed_wiki2vec.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset for  /data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\n",
      "Successful hits:  138959 Failed hits:  0\n",
      "Parsed /data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\n"
     ]
    }
   ],
   "source": [
    "success, failed = [], []\n",
    "def id_to_entity(db, entity_id):\n",
    "    try:\n",
    "        entity = db[str(entity_id)]    \n",
    "    except:\n",
    "        entity = db[str(entity_id).decode(\"utf-8\")]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    try:\n",
    "        entity = db[str(entity_id)]\n",
    "    except:\n",
    "        entity = db[str(entity_id).decode(\"utf-8\")]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    if entity in db:\n",
    "        success.append(entity)\n",
    "        try:\n",
    "            return int(db[entity])\n",
    "        except:\n",
    "            return int(db[entity.decode(\"utf-8\")])\n",
    "    closest_entity = resolved.get(entity, \"\")[0]\n",
    "    if closest_entity:\n",
    "        return int(db[closest_entity])\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = vectors\n",
    "    except:\n",
    "        embeddings, word2idx = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "\n",
    "\n",
    "def create_embeddings():\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word, vector = line[0], line[1:]\n",
    "            words.append(word)\n",
    "            vectors.append(np.array(vector).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    vectors = vectors.reshape((-1, EMBEDDING_DIM))\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "\n",
    "    return vectors, word2idx\n",
    "\n",
    "try:\n",
    "    word2id_db = shelve.open(prefix + \"_word_to_id_dict.db\", 'r')\n",
    "except:\n",
    "    print (prefix)\n",
    "    raise\n",
    "id2word_db = shelve.open(prefix + \"_id_to_word_dict.db\", \"r\")\n",
    "path2id_db = shelve.open(prefix + \"_path_to_id_dict.db\", \"r\")\n",
    "id2path_db = shelve.open(prefix + \"_id_to_path_dict.db\", \"r\")\n",
    "relations_db = shelve.open(prefix + \"_relations_map.db\", \"r\")\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = entity_to_id(word2id_db, tup[0]), entity_to_id(word2id_db, tup[1])\n",
    "    # paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "    # x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "    # y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "    # path_count_dict = { id_to_path(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    paths_xy = list(extract_paths(relations_db,x,y).items())\n",
    "    paths_yx = list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict = { id_to_path(id2path_db, path) : freq for (path, freq) in paths_xy }\n",
    "    path_count_dict.update({ id_to_path(id2path_db, path).replace(\"X/\", '@@@').replace('Y/', 'X/').replace('@@@', 'Y/') : freq for (path, freq) in paths_yx })\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    print (\"Parsing dataset for \", prefix)\n",
    "\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    embed_indices = [(emb_indexer.get(x,0), emb_indexer.get(y,0)) for (x,y) in dataset]\n",
    "\n",
    "    return embed_indices, paths\n",
    "\n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys()) + list(test_instances.keys()) + list(test_knocked.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values()) + list(test_instances.values()) + list(test_knocked.values())\n",
    "\n",
    "mappingDict = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "f.close()\n",
    "\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "f.close()\n",
    "\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.hidden_dim = HIDDEN_DIM + 2 * EMBEDDING_DIM\n",
    "        self.input_dim = POS_DIM + DEP_DIM + EMBEDDING_DIM + DIR_DIM\n",
    "        self.W = nn.Linear(NUM_RELATIONS, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(len(embeddings), EMBEDDING_DIM)\n",
    "        self.word_embeddings.load_state_dict({'weight': torch.from_numpy(np.array(embeddings))})\n",
    "        self.word_embeddings.require_grad = False\n",
    "        \n",
    "        self.pos_embeddings = nn.Embedding(len(pos_indexer), POS_DIM)\n",
    "        self.dep_embeddings = nn.Embedding(len(dep_indexer), DEP_DIM)\n",
    "        self.dir_embeddings = nn.Embedding(len(dir_indexer), DIR_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, NUM_LAYERS)\n",
    "    \n",
    "    def embed_path(self, elem):\n",
    "        path, count = elem\n",
    "        if path in self.cache:\n",
    "            return cache[path] * count\n",
    "        lstm_inp = []\n",
    "        for edge in path:\n",
    "            inputs = [torch.Tensor([[el]]) for el in edge]\n",
    "            word_embed = self.dropout_layer(self.word_embeddings(inputs[0]))\n",
    "            pos_embed = self.dropout_layer(self.pos_embeddings(inputs[1]))\n",
    "            dep_embed = self.dropout_layer(self.dep_embeddings(inputs[2]))\n",
    "            dir_embed = self.dropout_layer(self.dir_embeddings(inputs[3]))\n",
    "            embeds = np.concatenate((word_embed, pos_embed, dep_embed, dir_embed))\n",
    "            lstm_inp.append(embeds)\n",
    "        output, _ = self.lstm(lstm_inp)\n",
    "        cache[path] = output\n",
    "\n",
    "        return output * count\n",
    "    \n",
    "    def forward(self, data, emb_indexer):\n",
    "        if not data:\n",
    "            data[NULL_PATH] = 1\n",
    "        print (\"Data: \", data)\n",
    "        num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = [np.sum([self.embed_path(path) for path in paths.items()]) for paths in data]\n",
    "        print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        h = [np.concatenate((self.word_embeddings(elem[0]), h[i], self.word_embeddings(elem[1]))) for i,emb in enumerate(emb_indexer)]\n",
    "        return self.softmax(self.W(h))\n",
    "\n",
    "HIDDEN_DIM = 60\n",
    "NUM_LAYERS = 2\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "batch_size = min(batch_size, dataset_size)\n",
    "num_batches = int(ceil(dataset_size/batch_size))\n",
    "\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "lstm = LSTM()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss, epoch_idx = 0, np.random.permutation(dataset_size)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch = epoch_idx[batch_start:batch_end]\n",
    "        \n",
    "        data, labels, embeddings_idx = x_train[batch], y_train[batch], embed_indices_train[batch]\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = lstm(data, embeddings_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= dataset_size\n",
    "    print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch + 1, num_epochs, total_loss))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictedLabels = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = lstm(data)\n",
    "        print (outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictedLabels.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "e = nn.Embedding(3, 3)\n",
    "ls = [[0, 1, 2], [3,4,5], [6,7,8]]\n",
    "ls = np.array([np.array(el) for el in ls])\n",
    "e.load_state_dict({'weight': torch.from_numpy(ls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = np.array([np.sum([self.embed_path(path) for path in paths.items()]) for paths in data])\n",
    "        #print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        print (h.shape)\n",
    "        h = [np.concatenate((self.word_embeddings(emb[0]), h[i], self.word_embeddings(emb[1]))) for i,emb in enumerate(emb_indexer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randn(1,4)\n",
    "torch.Tensor([[1]]).shape\n",
    "# torch.cat((h, t.view(1,-1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputt = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(inputt, target)\n",
    "output.backward()\n",
    "print (output, inputt, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took me 16.031342029571533 seconds to extract USE embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crateva greveana flowers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_word_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wansecure firewall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Values and keys obtained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim=-1000, closest_word=\"\"):\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    closest_word = \"\"\n",
    "    with shelve.open(use_embeddings, 'c') as db:\n",
    "        for i, w in enumerate(word_embeddings):\n",
    "            db[words[i]] = w\n",
    "        closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[closest_word_idx]\n",
    "        del word_embeddings\n",
    "    del db\n",
    "    return closest_word, max_sim\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "    if os.path.isfile(use_embeddings):\n",
    "        with shelve.open(use_embeddings, 'r') as db:\n",
    "            embeds = np.array(list(db.values()))\n",
    "            words = np.array(list(db.keys()))\n",
    "            print (\"Values and keys obtained\", time.time()-a)\n",
    "            sim_mat = awesome_cossim_topn(coo_matrix(embeds, dtype=np.float64), coo_matrix(word_to_compare.T, dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250)\n",
    "            print (\"Sim mat calculated\", time.time()-a)\n",
    "            closest_word_idx = np.argmax(sim_mat)\n",
    "            print (\"idx gotten\", time.time()-a)\n",
    "            closest_word = words[closest_word_idx]\n",
    "    else:\n",
    "        words = list(word2id_db.keys())\n",
    "        print (\"Obtained list of words\")\n",
    "        len_part = 100000\n",
    "        max_sim = -1000\n",
    "        n_parts = ceil(len(words)/len_part)\n",
    "        closest_word = \"\"\n",
    "        for i in range(n_parts):\n",
    "            words_part = words[i*len_part:(i+1)*len_part]\n",
    "            closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "\n",
    "org_names = names['buyer'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "\n",
    "\n",
    "print('All 3-grams in \"Department\":')\n",
    "print(extract_ngrams('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "\n",
    "def extract_ngrams(string, n=3):\n",
    "    string = fix_text(string).encode(\"ascii\", errors=\"ignore\").decode().lower() # fix text\n",
    "    string = string.replace('&', 'and').replace(',', ' ').replace('-', ' ').title()\n",
    "    string = re.sub('[' + re.escape(''.join(chars_to_remove)) + ']', '', string)\n",
    "    string = ' ' + re.sub(' +',' ',string).strip() + ' '\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    ngrams = [''.join(ngram) for ngram in ngrams]\n",
    "    return ngrams\n",
    "\n",
    "word_to_match = \"margherita pizza\"\n",
    "words = list(word2id_db.keys())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(words + [word_to_match])\n",
    "\n",
    "# d = awesome_cossim_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    print (sparserows)\n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "words[np.argmax(d)]\n",
    "print (\"time: \", start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_db_new = shelve.open(prefix + \"_relations_map.db\", \"c\")\n",
    "for k, v in relations_db.items():\n",
    "    relations_db_new[\"###\".join(k.split(\"_\"))] = v\n",
    "relations_db_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"\\t\".join(l.split(\"\\t\")[1:-1]) for l in open(\"../junk/security_dataset.tsv\",\"r\").read().split(\"\\n\")[1:]]\n",
    "open(\"../files/dataset/dataset.tsv\",\"w\").write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open(use_embeddings, 'r') as db:    \n",
    "    allitems = list(db.items())\n",
    "    emb = [el[1] for el in allitems]\n",
    "    wds = [el[0] for el in allitems]\n",
    "    file = open(\"../files/embeddings_list.pkl\", \"wb\")\n",
    "    pickle.dump(allitems, file)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22912765"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab peter wyche (diplomat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab acoma-zuni section\n",
      "Original word not in vocab madan-harini\n",
      "Original word not in vocab trust no one (internet security)\n",
      "Original word not in vocab international tibet independence movement\n",
      "Original word not in vocab isobase\n",
      "Original word not in vocab human computer interaction (security)\n",
      "Original word not in vocab poetas de karaoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab ipa pulmonic consonant chart with audio\n",
      "Original word not in vocab lego clutch powers: bad hair day\n",
      "Original word not in vocab aed (non-profit)\n",
      "Original word not in vocab quilmes airport\n",
      "Original word not in vocab yendegaia airport\n",
      "Original word not in vocab the pack a.d.\n",
      "Original word not in vocab harvie-watt baronets\n",
      "Original word not in vocab sharp actius rd3d notebook\n",
      "Original word not in vocab big beach boutique ii - the movie\n",
      "Original word not in vocab privacy by design\n",
      "Original word not in vocab motorola devour\n",
      "Original word not in vocab piracy act\n",
      "Original word not in vocab starter ring gear\n",
      "Original word not in vocab antonio sánchez (puerto rican host)\n",
      "Original word not in vocab electronic logbook\n",
      "Original word not in vocab greg burke (journalist)\n",
      "Original word not in vocab deaths in november 2013\n",
      "Original word not in vocab hp mini 311\n",
      "Original word not in vocab confederation of indigenous nationalities of the ecuadorian amazon\n",
      "Original word not in vocab url subscription architecture\n",
      "Original word not in vocab snowballers entertainment\n",
      "Original word not in vocab basic strategic arts program\n",
      "Original word not in vocab mars (ticket reservation system)\n",
      "Original word not in vocab matija kristić\n",
      "Original word not in vocab edward graham lee\n",
      "Original word not in vocab rebellion of the three guards\n",
      "Original word not in vocab the recipe for gertrude\n",
      "Original word not in vocab quiet pc\n",
      "Original word not in vocab russian amateur radio union\n",
      "1 done\n",
      "Original word not in vocab core strategy document\n",
      "Original word not in vocab mutukula airport\n",
      "2 done\n",
      "Original word not in vocab magnus l. kpakol\n",
      "1 done\n",
      "Original word not in vocab strangers (malibu comics)\n",
      "2 done\n",
      "3 done\n",
      "Original word not in vocab cary baronets\n",
      "3 done\n",
      "Original word not in vocab andrew wood (diplomat)\n",
      "4 done\n",
      "Original word not in vocab lead petty officer\n",
      "4 done\n",
      "Original word not in vocab corps of military police (india)\n",
      "5 done\n",
      "Original word not in vocab siniša radanović\n",
      "5 done\n",
      "Original word not in vocab oatmeal cookie\n",
      "6 done\n",
      "Original word not in vocab academic research alliance\n",
      "6 done\n",
      "1 done\n",
      "Original word not in vocab joseph e. duncan iii\n",
      "7 done\n",
      "7 done\n",
      "Original word not in vocab lideta army airport\n",
      "2 done\n",
      "8 done\n",
      "8 done\n",
      "3 done\n",
      "9 done\n",
      "9 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "10 done\n",
      "10 done\n",
      "7 done\n",
      "11 done\n",
      "11 done\n",
      "12 done\n",
      "12 done\n",
      "8 done\n",
      "13 done\n",
      "13 done\n",
      "1 done\n",
      "9 done\n",
      "14 done\n",
      "2 done\n",
      "14 done\n",
      "1 done\n",
      "15 done\n",
      "3 done\n",
      "15 done\n",
      "10 done\n",
      "16 done\n",
      "2 done\n",
      "11 done\n",
      "4 done\n",
      "16 done\n",
      "3 done\n",
      "12 done\n",
      "17 done\n",
      "5 done\n",
      "4 done\n",
      "17 done\n",
      "13 done\n",
      "6 done\n",
      "18 done\n",
      "5 done\n",
      "18 done\n",
      "14 done\n",
      "6 done\n",
      "19 done\n",
      "7 done\n",
      "19 done\n",
      "15 done\n",
      "7 done\n",
      "20 done\n",
      "20 done\n",
      "16 done\n",
      "8 done\n",
      "21 done\n",
      "8 done\n",
      "21 done\n",
      "17 done\n",
      "9 done\n",
      "22 done\n",
      "22 done\n",
      "9 done\n",
      "18 done\n",
      "23 done\n",
      "23 done\n",
      "19 done\n",
      "10 done\n",
      "24 done\n",
      "10 done\n",
      "24 done\n",
      "20 done\n",
      "11 done\n",
      "25 done\n",
      "11 done\n",
      "25 done\n",
      "21 done\n",
      "12 done\n",
      "12 done\n",
      "26 done\n",
      "13 done\n",
      "26 done\n",
      "13 done\n",
      "22 done\n",
      "27 done\n",
      "14 done\n",
      "27 done\n",
      "14 done\n",
      "28 done\n",
      "15 done\n",
      "23 done\n",
      "28 done\n",
      "16 done\n",
      "15 done\n",
      "29 done\n",
      "24 done\n",
      "17 done\n",
      "16 done\n",
      "29 done\n",
      "30 done\n",
      "25 done\n",
      "18 done\n",
      "30 done\n",
      "17 done\n",
      "26 done\n",
      "19 done\n",
      "18 done\n",
      "27 done\n",
      "31 done\n",
      "20 done\n",
      "31 done\n",
      "19 done\n",
      "28 done\n",
      "32 done\n",
      "21 done\n",
      "20 done\n",
      "32 done\n",
      "33 done\n",
      "29 done\n",
      "22 done\n",
      "33 done\n",
      "21 done\n",
      "34 done\n",
      "30 done\n",
      "34 done\n",
      "23 done\n",
      "22 done\n",
      "35 done\n",
      "24 done\n",
      "35 done\n",
      "36 done\n",
      "23 done\n",
      "25 done\n",
      "31 done\n",
      "36 done\n",
      "37 done\n",
      "37 done\n",
      "24 done\n",
      "26 done\n",
      "32 done\n",
      "38 done\n",
      "38 done\n",
      "25 done\n",
      "27 done\n",
      "33 done\n",
      "39 done\n",
      "39 done\n",
      "28 done\n",
      "26 done\n",
      "34 done\n",
      "40 done\n",
      "40 done\n",
      "29 done\n",
      "27 done\n",
      "41 done\n",
      "35 done\n",
      "30 done\n",
      "41 done\n",
      "28 done\n",
      "42 done\n",
      "36 done\n",
      "42 done\n",
      "29 done\n",
      "43 done\n",
      "37 done\n",
      "31 done\n",
      "43 done\n",
      "44 done\n",
      "30 done\n",
      "38 done\n",
      "44 done\n",
      "45 done\n",
      "32 done\n",
      "39 done\n",
      "45 done\n",
      "33 done\n",
      "46 done\n",
      "31 done\n",
      "40 done\n",
      "34 done\n",
      "46 done\n",
      "47 done\n",
      "32 done\n",
      "47 done\n",
      "35 done\n",
      "33 done\n",
      "41 done\n",
      "48 done\n",
      "48 done\n",
      "34 done\n",
      "36 done\n",
      "42 done\n",
      "49 done\n",
      "49 done\n",
      "37 done\n",
      "43 done\n",
      "35 done\n",
      "50 done\n",
      "44 done\n",
      "50 done\n",
      "38 done\n",
      "36 done\n",
      "51 done\n",
      "51 done\n",
      "45 done\n",
      "39 done\n",
      "37 done\n",
      "52 done\n",
      "52 done\n",
      "46 done\n",
      "53 done\n",
      "38 done\n",
      "40 done\n",
      "53 done\n",
      "47 done\n",
      "54 done\n",
      "39 done\n",
      "41 done\n",
      "54 done\n",
      "48 done\n",
      "55 done\n",
      "42 done\n",
      "55 done\n",
      "40 done\n",
      "49 done\n",
      "56 done\n",
      "43 done\n",
      "50 done\n",
      "41 done\n",
      "57 done\n",
      "56 done\n",
      "44 done\n",
      "51 done\n",
      "57 done\n",
      "58 done\n",
      "42 done\n",
      "45 done\n",
      "58 done\n",
      "52 done\n",
      "59 done\n",
      "43 done\n",
      "46 done\n",
      "59 done\n",
      "53 done\n",
      "44 done\n",
      "60 done\n",
      "47 done\n",
      "60 done\n",
      "54 done\n",
      "45 done\n",
      "61 done\n",
      "48 done\n",
      "55 done\n",
      "61 done\n",
      "62 done\n",
      "46 done\n",
      "49 done\n",
      "62 done\n",
      "56 done\n",
      "63 done\n",
      "50 done\n",
      "47 done\n",
      "57 done\n",
      "63 done\n",
      "64 done\n",
      "51 done\n",
      "48 done\n",
      "58 done\n",
      "64 done\n",
      "52 done\n",
      "49 done\n",
      "65 done\n",
      "59 done\n",
      "50 done\n",
      "65 done\n",
      "53 done\n",
      "66 done\n",
      "60 done\n",
      "51 done\n",
      "54 done\n",
      "67 done\n",
      "66 done\n",
      "52 done\n",
      "61 done\n",
      "55 done\n",
      "68 done\n",
      "67 done\n",
      "53 done\n",
      "62 done\n",
      "56 done\n",
      "68 done\n",
      "69 done\n",
      "54 done\n",
      "63 done\n",
      "57 done\n",
      "69 done\n",
      "70 done\n",
      "55 done\n",
      "64 done\n",
      "58 done\n",
      "70 done\n",
      "71 done\n",
      "56 done\n",
      "65 done\n",
      "72 done\n",
      "71 done\n",
      "59 done\n",
      "57 done\n",
      "66 done\n",
      "72 done\n",
      "73 done\n",
      "60 done\n",
      "58 done\n",
      "67 done\n",
      "73 done\n",
      "61 done\n",
      "74 done\n",
      "59 done\n",
      "68 done\n",
      "62 done\n",
      "74 done\n",
      "75 done\n",
      "60 done\n",
      "69 done\n",
      "63 done\n",
      "75 done\n",
      "76 done\n",
      "64 done\n",
      "61 done\n",
      "70 done\n",
      "76 done\n",
      "77 done\n",
      "62 done\n",
      "71 done\n",
      "65 done\n",
      "77 done\n",
      "72 done\n",
      "78 done\n",
      "63 done\n",
      "78 done\n",
      "66 done\n",
      "73 done\n",
      "79 done\n",
      "64 done\n",
      "79 done\n",
      "67 done\n",
      "74 done\n",
      "80 done\n",
      "65 done\n",
      "68 done\n",
      "80 done\n",
      "75 done\n",
      "69 done\n",
      "81 done\n",
      "81 done\n",
      "66 done\n",
      "70 done\n",
      "76 done\n",
      "67 done\n",
      "82 done\n",
      "71 done\n",
      "82 done\n",
      "68 done\n",
      "77 done\n",
      "83 done\n",
      "72 done\n",
      "83 done\n",
      "69 done\n",
      "84 done\n",
      "78 done\n",
      "73 done\n",
      "84 done\n",
      "70 done\n",
      "79 done\n",
      "74 done\n",
      "71 done\n",
      "85 done\n",
      "85 done\n",
      "80 done\n",
      "72 done\n",
      "75 done\n",
      "86 done\n",
      "86 done\n",
      "81 done\n",
      "76 done\n",
      "73 done\n",
      "87 done\n",
      "87 done\n",
      "74 done\n",
      "77 done\n",
      "88 done\n",
      "82 done\n",
      "88 done\n",
      "89 done\n",
      "78 done\n",
      "75 done\n",
      "83 done\n",
      "89 done\n",
      "90 done\n",
      "76 done\n",
      "79 done\n",
      "90 done\n",
      "84 done\n",
      "80 done\n",
      "77 done\n",
      "91 done\n",
      "91 done\n",
      "85 done\n",
      "92 done\n",
      "81 done\n",
      "78 done\n",
      "92 done\n",
      "93 done\n",
      "86 done\n",
      "79 done\n",
      "93 done\n",
      "82 done\n",
      "87 done\n",
      "83 done\n",
      "80 done\n",
      "94 done\n",
      "94 done\n",
      "88 done\n",
      "95 done\n",
      "84 done\n",
      "95 done\n",
      "81 done\n",
      "89 done\n",
      "96 done\n",
      "96 done\n",
      "90 done\n",
      "82 done\n",
      "85 done\n",
      "97 done\n",
      "97 done\n",
      "91 done\n",
      "83 done\n",
      "86 done\n",
      "98 done\n",
      "98 done\n",
      "92 done\n",
      "84 done\n",
      "87 done\n",
      "93 done\n",
      "88 done\n",
      "99 done\n",
      "99 done\n",
      "89 done\n",
      "85 done\n",
      "100 done\n",
      "100 done\n",
      "94 done\n",
      "90 done\n",
      "86 done\n",
      "101 done\n",
      "101 done\n",
      "95 done\n",
      "91 done\n",
      "87 done\n",
      "92 done\n",
      "102 done\n",
      "102 done\n",
      "96 done\n",
      "88 done\n",
      "93 done\n",
      "103 done\n",
      "89 done\n",
      "97 done\n",
      "103 done\n",
      "94 done\n",
      "90 done\n",
      "104 done\n",
      "98 done\n",
      "104 done\n",
      "95 done\n",
      "105 done\n",
      "99 done\n",
      "91 done\n",
      "106 done\n",
      "96 done\n",
      "100 done\n",
      "105 done\n",
      "92 done\n",
      "97 done\n",
      "101 done\n",
      "106 done\n",
      "107 done\n",
      "93 done\n",
      "102 done\n",
      "108 done\n",
      "98 done\n",
      "107 done\n",
      "94 done\n",
      "103 done\n",
      "109 done\n",
      "108 done\n",
      "95 done\n",
      "99 done\n",
      "104 done\n",
      "100 done\n",
      "109 done\n",
      "96 done\n",
      "110 done\n",
      "105 done\n",
      "97 done\n",
      "101 done\n",
      "111 done\n",
      "106 done\n",
      "110 done\n",
      "102 done\n",
      "112 done\n",
      "98 done\n",
      "113 done\n",
      "111 done\n",
      "107 done\n",
      "103 done\n",
      "99 done\n",
      "114 done\n",
      "108 done\n",
      "112 done\n",
      "100 done\n",
      "115 done\n",
      "109 done\n",
      "104 done\n",
      "113 done\n",
      "101 done\n",
      "116 done\n",
      "105 done\n",
      "114 done\n",
      "102 done\n",
      "110 done\n",
      "106 done\n",
      "115 done\n",
      "117 done\n",
      "111 done\n",
      "103 done\n",
      "107 done\n",
      "116 done\n",
      "118 done\n",
      "104 done\n",
      "108 done\n",
      "112 done\n",
      "113 done\n",
      "105 done\n",
      "109 done\n",
      "119 done\n",
      "117 done\n",
      "114 done\n",
      "106 done\n",
      "120 done\n",
      "118 done\n",
      "115 done\n",
      "110 done\n",
      "107 done\n",
      "121 done\n",
      "111 done\n",
      "119 done\n",
      "116 done\n",
      "108 done\n",
      "122 done\n",
      "120 done\n",
      "112 done\n",
      "109 done\n",
      "123 done\n",
      "113 done\n",
      "121 done\n",
      "117 done\n",
      "124 done\n",
      "110 done\n",
      "114 done\n",
      "122 done\n",
      "118 done\n",
      "111 done\n",
      "125 done\n",
      "115 done\n",
      "123 done\n",
      "112 done\n",
      "119 done\n",
      "126 done\n",
      "116 done\n",
      "124 done\n",
      "113 done\n",
      "120 done\n",
      "127 done\n",
      "125 done\n",
      "114 done\n",
      "121 done\n",
      "128 done\n",
      "117 done\n",
      "122 done\n",
      "115 done\n",
      "118 done\n",
      "129 done\n",
      "126 done\n",
      "123 done\n",
      "130 done\n",
      "127 done\n",
      "116 done\n",
      "119 done\n",
      "124 done\n",
      "131 done\n",
      "128 done\n",
      "120 done\n",
      "129 done\n",
      "117 done\n",
      "125 done\n",
      "132 done\n",
      "121 done\n",
      "118 done\n",
      "130 done\n",
      "133 done\n",
      "122 done\n",
      "126 done\n",
      "131 done\n",
      "134 done\n",
      "119 done\n",
      "123 done\n",
      "127 done\n",
      "135 done\n",
      "132 done\n",
      "124 done\n",
      "120 done\n",
      "128 done\n",
      "136 done\n",
      "133 done\n",
      "129 done\n",
      "121 done\n",
      "125 done\n",
      "134 done\n",
      "130 done\n",
      "122 done\n",
      "126 done\n",
      "137 done\n",
      "135 done\n",
      "123 done\n",
      "131 done\n",
      "127 done\n",
      "136 done\n",
      "138 done\n",
      "124 done\n",
      "132 done\n",
      "139 done\n",
      "128 done\n",
      "125 done\n",
      "140 done\n",
      "133 done\n",
      "129 done\n",
      "137 done\n",
      "138 done\n",
      "130 done\n",
      "134 done\n",
      "126 done\n",
      "141 done\n",
      "139 done\n",
      "135 done\n",
      "127 done\n",
      "142 done\n",
      "131 done\n",
      "140 done\n",
      "136 done\n",
      "143 done\n",
      "128 done\n",
      "141 done\n",
      "132 done\n",
      "142 done\n",
      "144 done\n",
      "129 done\n",
      "133 done\n",
      "137 done\n",
      "130 done\n",
      "145 done\n",
      "143 done\n",
      "134 done\n",
      "138 done\n",
      "146 done\n",
      "135 done\n",
      "144 done\n",
      "131 done\n",
      "147 done\n",
      "139 done\n",
      "148 done\n",
      "136 done\n",
      "145 done\n",
      "149 done\n",
      "132 done\n",
      "140 done\n",
      "150 done\n",
      "146 done\n",
      "133 done\n",
      "147 done\n",
      "151 done\n",
      "141 done\n",
      "137 done\n",
      "134 done\n",
      "148 done\n",
      "142 done\n",
      "152 done\n",
      "135 done\n",
      "138 done\n",
      "153 done\n",
      "149 done\n",
      "136 done\n",
      "154 done\n",
      "143 done\n",
      "139 done\n",
      "150 done\n",
      "155 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "151 done\n",
      "145 done\n",
      "152 done\n",
      "137 done\n",
      "157 done\n",
      "141 done\n",
      "153 done\n",
      "146 done\n",
      "158 done\n",
      "138 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 done\n",
      "154 done\n",
      "147 done\n",
      "159 done\n",
      "139 done\n",
      "143 done\n",
      "155 done\n",
      "148 done\n",
      "160 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "161 done\n",
      "149 done\n",
      "145 done\n",
      "157 done\n",
      "141 done\n",
      "150 done\n",
      "146 done\n",
      "142 done\n",
      "158 done\n",
      "162 done\n",
      "151 done\n",
      "147 done\n",
      "159 done\n",
      "143 done\n",
      "152 done\n",
      "148 done\n",
      "163 done\n",
      "160 done\n",
      "144 done\n",
      "149 done\n",
      "153 done\n",
      "161 done\n",
      "145 done\n",
      "150 done\n",
      "146 done\n",
      "154 done\n",
      "164 done\n",
      "151 done\n",
      "147 done\n",
      "155 done\n",
      "148 done\n",
      "152 done\n",
      "156 done\n",
      "165 done\n",
      "162 done\n",
      "149 done\n",
      "157 done\n",
      "166 done\n",
      "163 done\n",
      "153 done\n",
      "150 done\n",
      "158 done\n",
      "154 done\n",
      "151 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "164 done\n",
      "152 done\n",
      "168 done\n",
      "156 done\n",
      "160 done\n",
      "165 done\n",
      "153 done\n",
      "169 done\n",
      "161 done\n",
      "157 done\n",
      "166 done\n",
      "170 done\n",
      "154 done\n",
      "158 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "171 done\n",
      "168 done\n",
      "156 done\n",
      "162 done\n",
      "160 done\n",
      "172 done\n",
      "169 done\n",
      "157 done\n",
      "161 done\n",
      "163 done\n",
      "173 done\n",
      "170 done\n",
      "158 done\n",
      "171 done\n",
      "159 done\n",
      "174 done\n",
      "172 done\n",
      "162 done\n",
      "160 done\n",
      "175 done\n",
      "164 done\n",
      "161 done\n",
      "173 done\n",
      "163 done\n",
      "176 done\n",
      "165 done\n",
      "174 done\n",
      "166 done\n",
      "177 done\n",
      "175 done\n",
      "178 done\n",
      "164 done\n",
      "167 done\n",
      "162 done\n",
      "176 done\n",
      "168 done\n",
      "165 done\n",
      "179 done\n",
      "163 done\n",
      "169 done\n",
      "166 done\n",
      "177 done\n",
      "170 done\n",
      "180 done\n",
      "178 done\n",
      "167 done\n",
      "181 done\n",
      "164 done\n",
      "171 done\n",
      "179 done\n",
      "168 done\n",
      "172 done\n",
      "165 done\n",
      "182 done\n",
      "169 done\n",
      "166 done\n",
      "180 done\n",
      "170 done\n",
      "173 done\n",
      "183 done\n",
      "181 done\n",
      "171 done\n",
      "174 done\n",
      "167 done\n",
      "184 done\n",
      "172 done\n",
      "182 done\n",
      "175 done\n",
      "168 done\n",
      "185 done\n",
      "173 done\n",
      "176 done\n",
      "183 done\n",
      "169 done\n",
      "186 done\n",
      "174 done\n",
      "170 done\n",
      "184 done\n",
      "177 done\n",
      "187 done\n",
      "171 done\n",
      "175 done\n",
      "185 done\n",
      "178 done\n",
      "188 done\n",
      "172 done\n",
      "176 done\n",
      "186 done\n",
      "179 done\n",
      "189 done\n",
      "177 done\n",
      "187 done\n",
      "173 done\n",
      "180 done\n",
      "190 done\n",
      "174 done\n",
      "178 done\n",
      "188 done\n",
      "181 done\n",
      "189 done\n",
      "179 done\n",
      "191 done\n",
      "175 done\n",
      "182 done\n",
      "192 done\n",
      "180 done\n",
      "190 done\n",
      "176 done\n",
      "193 done\n",
      "183 done\n",
      "181 done\n",
      "194 done\n",
      "177 done\n",
      "184 done\n",
      "195 done\n",
      "191 done\n",
      "182 done\n",
      "178 done\n",
      "185 done\n",
      "192 done\n",
      "196 done\n",
      "183 done\n",
      "179 done\n",
      "186 done\n",
      "193 done\n",
      "197 done\n",
      "184 done\n",
      "180 done\n",
      "187 done\n",
      "198 done\n",
      "194 done\n",
      "185 done\n",
      "181 done\n",
      "199 done\n",
      "188 done\n",
      "186 done\n",
      "195 done\n",
      "182 done\n",
      "187 done\n",
      "189 done\n",
      "200 done\n",
      "196 done\n",
      "183 done\n",
      "188 done\n",
      "190 done\n",
      "197 done\n",
      "201 done\n",
      "184 done\n",
      "189 done\n",
      "198 done\n",
      "202 done\n",
      "190 done\n",
      "185 done\n",
      "191 done\n",
      "199 done\n",
      "191 done\n",
      "203 done\n",
      "186 done\n",
      "192 done\n",
      "200 done\n",
      "192 done\n",
      "204 done\n",
      "193 done\n",
      "187 done\n",
      "201 done\n",
      "193 done\n",
      "194 done\n",
      "188 done\n",
      "205 done\n",
      "194 done\n",
      "202 done\n",
      "189 done\n",
      "206 done\n",
      "195 done\n",
      "195 done\n",
      "207 done\n",
      "190 done\n",
      "203 done\n",
      "196 done\n",
      "204 done\n",
      "196 done\n",
      "208 done\n",
      "197 done\n",
      "197 done\n",
      "191 done\n",
      "209 done\n",
      "198 done\n",
      "205 done\n",
      "198 done\n",
      "210 done\n",
      "192 done\n",
      "199 done\n",
      "206 done\n",
      "199 done\n",
      "200 done\n",
      "193 done\n",
      "211 done\n",
      "207 done\n",
      "200 done\n",
      "194 done\n",
      "208 done\n",
      "201 done\n",
      "201 done\n",
      "195 done\n",
      "212 done\n",
      "202 done\n",
      "209 done\n",
      "202 done\n",
      "213 done\n",
      "196 done\n",
      "203 done\n",
      "210 done\n",
      "214 done\n",
      "197 done\n",
      "203 done\n",
      "204 done\n",
      "215 done\n",
      "211 done\n",
      "198 done\n",
      "204 done\n",
      "205 done\n",
      "216 done\n",
      "206 done\n",
      "199 done\n",
      "212 done\n",
      "217 done\n",
      "207 done\n",
      "205 done\n",
      "218 done\n",
      "200 done\n",
      "213 done\n",
      "206 done\n",
      "208 done\n",
      "219 done\n",
      "214 done\n",
      "209 done\n",
      "207 done\n",
      "201 done\n",
      "215 done\n",
      "220 done\n",
      "210 done\n",
      "208 done\n",
      "221 done\n",
      "202 done\n",
      "216 done\n",
      "209 done\n",
      "211 done\n",
      "203 done\n",
      "222 done\n",
      "217 done\n",
      "204 done\n",
      "218 done\n",
      "210 done\n",
      "212 done\n",
      "223 done\n",
      "205 done\n",
      "213 done\n",
      "211 done\n",
      "219 done\n",
      "206 done\n",
      "214 done\n",
      "224 done\n",
      "220 done\n",
      "212 done\n",
      "215 done\n",
      "225 done\n",
      "207 done\n",
      "221 done\n",
      "213 done\n",
      "216 done\n",
      "208 done\n",
      "214 done\n",
      "226 done\n",
      "222 done\n",
      "217 done\n",
      "209 done\n",
      "215 done\n",
      "227 done\n",
      "218 done\n",
      "223 done\n",
      "210 done\n",
      "216 done\n",
      "228 done\n",
      "219 done\n",
      "211 done\n",
      "217 done\n",
      "229 done\n",
      "224 done\n",
      "220 done\n",
      "225 done\n",
      "230 done\n",
      "218 done\n",
      "221 done\n",
      "212 done\n",
      "231 done\n",
      "219 done\n",
      "226 done\n",
      "222 done\n",
      "213 done\n",
      "232 done\n",
      "227 done\n",
      "220 done\n",
      "214 done\n",
      "223 done\n",
      "233 done\n",
      "221 done\n",
      "215 done\n",
      "228 done\n",
      "234 done\n",
      "222 done\n",
      "216 done\n",
      "224 done\n",
      "229 done\n",
      "235 done\n",
      "217 done\n",
      "225 done\n",
      "223 done\n",
      "236 done\n",
      "218 done\n",
      "230 done\n",
      "226 done\n",
      "237 done\n",
      "231 done\n",
      "219 done\n",
      "227 done\n",
      "224 done\n",
      "232 done\n",
      "238 done\n",
      "220 done\n",
      "225 done\n",
      "228 done\n",
      "233 done\n",
      "229 done\n",
      "239 done\n",
      "221 done\n",
      "226 done\n",
      "234 done\n",
      "230 done\n",
      "222 done\n",
      "240 done\n",
      "231 done\n",
      "227 done\n",
      "235 done\n",
      "241 done\n",
      "223 done\n",
      "232 done\n",
      "236 done\n",
      "228 done\n",
      "237 done\n",
      "242 done\n",
      "233 done\n",
      "224 done\n",
      "229 done\n",
      "243 done\n",
      "238 done\n",
      "234 done\n",
      "225 done\n",
      "230 done\n",
      "235 done\n",
      "244 done\n",
      "239 done\n",
      "226 done\n",
      "231 done\n",
      "236 done\n",
      "227 done\n",
      "240 done\n",
      "232 done\n",
      "237 done\n",
      "245 done\n",
      "241 done\n",
      "238 done\n",
      "228 done\n",
      "233 done\n",
      "246 done\n",
      "242 done\n",
      "234 done\n",
      "229 done\n",
      "247 done\n",
      "239 done\n",
      "235 done\n",
      "243 done\n",
      "240 done\n",
      "248 done\n",
      "230 done\n",
      "236 done\n",
      "249 done\n",
      "241 done\n",
      "244 done\n",
      "231 done\n",
      "237 done\n",
      "250 done\n",
      "232 done\n",
      "242 done\n",
      "238 done\n",
      "245 done\n",
      "251 done\n",
      "233 done\n",
      "243 done\n",
      "246 done\n",
      "239 done\n",
      "252 done\n",
      "234 done\n",
      "247 done\n",
      "244 done\n",
      "240 done\n",
      "235 done\n",
      "253 done\n",
      "248 done\n",
      "241 done\n",
      "236 done\n",
      "249 done\n",
      "254 done\n",
      "245 done\n",
      "237 done\n",
      "250 done\n",
      "242 done\n",
      "255 done\n",
      "246 done\n",
      "238 done\n",
      "243 done\n",
      "256 done\n",
      "247 done\n",
      "251 done\n",
      "239 done\n",
      "248 done\n",
      "252 done\n",
      "244 done\n",
      "257 done\n",
      "240 done\n",
      "249 done\n",
      "253 done\n",
      "258 done\n",
      "241 done\n",
      "250 done\n",
      "245 done\n",
      "259 done\n",
      "254 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "242 done\n",
      "255 done\n",
      "261 done\n",
      "252 done\n",
      "243 done\n",
      "247 done\n",
      "256 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "244 done\n",
      "257 done\n",
      "258 done\n",
      "249 done\n",
      "263 done\n",
      "254 done\n",
      "264 done\n",
      "250 done\n",
      "259 done\n",
      "245 done\n",
      "255 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "256 done\n",
      "265 done\n",
      "261 done\n",
      "252 done\n",
      "247 done\n",
      "257 done\n",
      "266 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "258 done\n",
      "267 done\n",
      "249 done\n",
      "263 done\n",
      "259 done\n",
      "268 done\n",
      "254 done\n",
      "250 done\n",
      "264 done\n",
      "260 done\n",
      "269 done\n",
      "255 done\n",
      "251 done\n",
      "270 done\n",
      "261 done\n",
      "256 done\n",
      "265 done\n",
      "252 done\n",
      "271 done\n",
      "262 done\n",
      "257 done\n",
      "266 done\n",
      "253 done\n",
      "272 done\n",
      "258 done\n",
      "263 done\n",
      "267 done\n",
      "264 done\n",
      "273 done\n",
      "259 done\n",
      "254 done\n",
      "268 done\n",
      "274 done\n",
      "260 done\n",
      "255 done\n",
      "269 done\n",
      "265 done\n",
      "261 done\n",
      "256 done\n",
      "275 done\n",
      "270 done\n",
      "266 done\n",
      "262 done\n",
      "257 done\n",
      "267 done\n",
      "276 done\n",
      "271 done\n",
      "258 done\n",
      "268 done\n",
      "263 done\n",
      "277 done\n",
      "272 done\n",
      "269 done\n",
      "259 done\n",
      "264 done\n",
      "278 done\n",
      "273 done\n",
      "270 done\n",
      "260 done\n",
      "279 done\n",
      "274 done\n",
      "261 done\n",
      "265 done\n",
      "271 done\n",
      "280 done\n",
      "272 done\n",
      "275 done\n",
      "262 done\n",
      "281 done\n",
      "266 done\n",
      "273 done\n",
      "276 done\n",
      "263 done\n",
      "282 done\n",
      "267 done\n",
      "277 done\n",
      "274 done\n",
      "264 done\n",
      "268 done\n",
      "283 done\n",
      "278 done\n",
      "275 done\n",
      "269 done\n",
      "284 done\n",
      "279 done\n",
      "276 done\n",
      "265 done\n",
      "280 done\n",
      "277 done\n",
      "270 done\n",
      "285 done\n",
      "266 done\n",
      "281 done\n",
      "278 done\n",
      "271 done\n",
      "267 done\n",
      "286 done\n",
      "282 done\n",
      "272 done\n",
      "279 done\n",
      "268 done\n",
      "287 done\n",
      "283 done\n",
      "280 done\n",
      "269 done\n",
      "273 done\n",
      "288 done\n",
      "270 done\n",
      "284 done\n",
      "281 done\n",
      "274 done\n",
      "282 done\n",
      "271 done\n",
      "289 done\n",
      "285 done\n",
      "275 done\n",
      "272 done\n",
      "283 done\n",
      "286 done\n",
      "276 done\n",
      "290 done\n",
      "284 done\n",
      "277 done\n",
      "273 done\n",
      "291 done\n",
      "287 done\n",
      "285 done\n",
      "278 done\n",
      "274 done\n",
      "288 done\n",
      "292 done\n",
      "286 done\n",
      "279 done\n",
      "275 done\n",
      "293 done\n",
      "289 done\n",
      "280 done\n",
      "287 done\n",
      "276 done\n",
      "290 done\n",
      "281 done\n",
      "288 done\n",
      "277 done\n",
      "294 done\n",
      "291 done\n",
      "282 done\n",
      "278 done\n",
      "295 done\n",
      "289 done\n",
      "292 done\n",
      "283 done\n",
      "279 done\n",
      "296 done\n",
      "284 done\n",
      "293 done\n",
      "280 done\n",
      "290 done\n",
      "297 done\n",
      "285 done\n",
      "281 done\n",
      "291 done\n",
      "298 done\n",
      "294 done\n",
      "282 done\n",
      "286 done\n",
      "295 done\n",
      "292 done\n",
      "283 done\n",
      "296 done\n",
      "299 done\n",
      "287 done\n",
      "284 done\n",
      "293 done\n",
      "297 done\n",
      "288 done\n",
      "300 done\n",
      "285 done\n",
      "301 done\n",
      "298 done\n",
      "294 done\n",
      "289 done\n",
      "286 done\n",
      "302 done\n",
      "295 done\n",
      "299 done\n",
      "303 done\n",
      "290 done\n",
      "287 done\n",
      "296 done\n",
      "300 done\n",
      "291 done\n",
      "304 done\n",
      "301 done\n",
      "288 done\n",
      "297 done\n",
      "292 done\n",
      "305 done\n",
      "302 done\n",
      "289 done\n",
      "298 done\n",
      "306 done\n",
      "293 done\n",
      "303 done\n",
      "290 done\n",
      "307 done\n",
      "299 done\n",
      "291 done\n",
      "304 done\n",
      "294 done\n",
      "300 done\n",
      "308 done\n",
      "295 done\n",
      "305 done\n",
      "309 done\n",
      "292 done\n",
      "301 done\n",
      "310 done\n",
      "296 done\n",
      "302 done\n",
      "306 done\n",
      "293 done\n",
      "311 done\n",
      "297 done\n",
      "307 done\n",
      "303 done\n",
      "298 done\n",
      "308 done\n",
      "312 done\n",
      "294 done\n",
      "304 done\n",
      "309 done\n",
      "295 done\n",
      "313 done\n",
      "299 done\n",
      "305 done\n",
      "310 done\n",
      "296 done\n",
      "300 done\n",
      "314 done\n",
      "306 done\n",
      "311 done\n",
      "297 done\n",
      "301 done\n",
      "307 done\n",
      "315 done\n",
      "298 done\n",
      "312 done\n",
      "302 done\n",
      "308 done\n",
      "316 done\n",
      "313 done\n",
      "303 done\n",
      "309 done\n",
      "299 done\n",
      "317 done\n",
      "314 done\n",
      "310 done\n",
      "300 done\n",
      "318 done\n",
      "304 done\n",
      "301 done\n",
      "311 done\n",
      "315 done\n",
      "305 done\n",
      "302 done\n",
      "316 done\n",
      "312 done\n",
      "319 done\n",
      "306 done\n",
      "303 done\n",
      "317 done\n",
      "313 done\n",
      "307 done\n",
      "320 done\n",
      "308 done\n",
      "318 done\n",
      "304 done\n",
      "309 done\n",
      "314 done\n",
      "305 done\n",
      "310 done\n",
      "319 done\n",
      "311 done\n",
      "315 done\n",
      "321 done\n",
      "306 done\n",
      "316 done\n",
      "322 done\n",
      "312 done\n",
      "307 done\n",
      "320 done\n",
      "313 done\n",
      "317 done\n",
      "323 done\n",
      "308 done\n",
      "318 done\n",
      "321 done\n",
      "309 done\n",
      "324 done\n",
      "314 done\n",
      "322 done\n",
      "310 done\n",
      "315 done\n",
      "325 done\n",
      "311 done\n",
      "319 done\n",
      "323 done\n",
      "326 done\n",
      "316 done\n",
      "324 done\n",
      "327 done\n",
      "312 done\n",
      "317 done\n",
      "320 done\n",
      "325 done\n",
      "313 done\n",
      "318 done\n",
      "328 done\n",
      "326 done\n",
      "321 done\n",
      "314 done\n",
      "327 done\n",
      "329 done\n",
      "319 done\n",
      "322 done\n",
      "315 done\n",
      "328 done\n",
      "330 done\n",
      "323 done\n",
      "316 done\n",
      "320 done\n",
      "329 done\n",
      "324 done\n",
      "331 done\n",
      "317 done\n",
      "321 done\n",
      "330 done\n",
      "325 done\n",
      "332 done\n",
      "318 done\n",
      "322 done\n",
      "326 done\n",
      "333 done\n",
      "331 done\n",
      "323 done\n",
      "327 done\n",
      "334 done\n",
      "332 done\n",
      "319 done\n",
      "324 done\n",
      "333 done\n",
      "328 done\n",
      "335 done\n",
      "334 done\n",
      "325 done\n",
      "320 done\n",
      "336 done\n",
      "329 done\n",
      "326 done\n",
      "335 done\n",
      "337 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 done\n",
      "321 done\n",
      "336 done\n",
      "327 done\n",
      "338 done\n",
      "322 done\n",
      "337 done\n",
      "328 done\n",
      "331 done\n",
      "323 done\n",
      "339 done\n",
      "338 done\n",
      "332 done\n",
      "324 done\n",
      "329 done\n",
      "340 done\n",
      "339 done\n",
      "325 done\n",
      "333 done\n",
      "340 done\n",
      "330 done\n",
      "326 done\n",
      "341 done\n",
      "334 done\n",
      "327 done\n",
      "341 done\n",
      "331 done\n",
      "335 done\n",
      "342 done\n",
      "332 done\n",
      "336 done\n",
      "328 done\n",
      "343 done\n",
      "342 done\n",
      "337 done\n",
      "333 done\n",
      "344 done\n",
      "343 done\n",
      "329 done\n",
      "345 done\n",
      "338 done\n",
      "344 done\n",
      "334 done\n",
      "346 done\n",
      "330 done\n",
      "339 done\n",
      "345 done\n",
      "335 done\n",
      "347 done\n",
      "340 done\n",
      "336 done\n",
      "331 done\n",
      "348 done\n",
      "346 done\n",
      "337 done\n",
      "347 done\n",
      "332 done\n",
      "341 done\n",
      "349 done\n",
      "348 done\n",
      "338 done\n",
      "333 done\n",
      "350 done\n",
      "342 done\n",
      "349 done\n",
      "334 done\n",
      "339 done\n",
      "351 done\n",
      "343 done\n",
      "350 done\n",
      "340 done\n",
      "335 done\n",
      "344 done\n",
      "351 done\n",
      "336 done\n",
      "352 done\n",
      "345 done\n",
      "341 done\n",
      "353 done\n",
      "337 done\n",
      "346 done\n",
      "352 done\n",
      "354 done\n",
      "342 done\n",
      "347 done\n",
      "338 done\n",
      "353 done\n",
      "355 done\n",
      "348 done\n",
      "343 done\n",
      "339 done\n",
      "356 done\n",
      "354 done\n",
      "344 done\n",
      "349 done\n",
      "340 done\n",
      "355 done\n",
      "345 done\n",
      "357 done\n",
      "350 done\n",
      "356 done\n",
      "346 done\n",
      "341 done\n",
      "358 done\n",
      "351 done\n",
      "347 done\n",
      "357 done\n",
      "359 done\n",
      "348 done\n",
      "360 done\n",
      "358 done\n",
      "342 done\n",
      "361 done\n",
      "352 done\n",
      "359 done\n",
      "349 done\n",
      "343 done\n",
      "360 done\n",
      "344 done\n",
      "353 done\n",
      "362 done\n",
      "350 done\n",
      "361 done\n",
      "345 done\n",
      "351 done\n",
      "363 done\n",
      "354 done\n",
      "362 done\n",
      "346 done\n",
      "364 done\n",
      "355 done\n",
      "363 done\n",
      "347 done\n",
      "365 done\n",
      "356 done\n",
      "364 done\n",
      "352 done\n",
      "348 done\n",
      "366 done\n",
      "365 done\n",
      "357 done\n",
      "353 done\n",
      "349 done\n",
      "366 done\n",
      "358 done\n",
      "367 done\n",
      "354 done\n",
      "367 done\n",
      "350 done\n",
      "359 done\n",
      "355 done\n",
      "368 done\n",
      "351 done\n",
      "360 done\n",
      "356 done\n",
      "368 done\n",
      "369 done\n",
      "361 done\n",
      "369 done\n",
      "357 done\n",
      "370 done\n",
      "352 done\n",
      "362 done\n",
      "370 done\n",
      "371 done\n",
      "358 done\n",
      "371 done\n",
      "353 done\n",
      "363 done\n",
      "372 done\n",
      "359 done\n",
      "372 done\n",
      "354 done\n",
      "364 done\n",
      "360 done\n",
      "355 done\n",
      "373 done\n",
      "373 done\n",
      "365 done\n",
      "356 done\n",
      "361 done\n",
      "374 done\n",
      "374 done\n",
      "357 done\n",
      "362 done\n",
      "366 done\n",
      "375 done\n",
      "375 done\n",
      "358 done\n",
      "363 done\n",
      "367 done\n",
      "359 done\n",
      "364 done\n",
      "376 done\n",
      "368 done\n",
      "360 done\n",
      "376 done\n",
      "365 done\n",
      "369 done\n",
      "361 done\n",
      "377 done\n",
      "370 done\n",
      "366 done\n",
      "377 done\n",
      "362 done\n",
      "378 done\n",
      "371 done\n",
      "379 done\n",
      "378 done\n",
      "367 done\n",
      "363 done\n",
      "372 done\n",
      "379 done\n",
      "380 done\n",
      "364 done\n",
      "368 done\n",
      "380 done\n",
      "373 done\n",
      "381 done\n",
      "365 done\n",
      "369 done\n",
      "381 done\n",
      "374 done\n",
      "382 done\n",
      "370 done\n",
      "382 done\n",
      "366 done\n",
      "371 done\n",
      "383 done\n",
      "375 done\n",
      "383 done\n",
      "367 done\n",
      "372 done\n",
      "384 done\n",
      "384 done\n",
      "368 done\n",
      "373 done\n",
      "385 done\n",
      "369 done\n",
      "376 done\n",
      "385 done\n",
      "374 done\n",
      "386 done\n",
      "370 done\n",
      "386 done\n",
      "377 done\n",
      "371 done\n",
      "375 done\n",
      "387 done\n",
      "387 done\n",
      "378 done\n",
      "372 done\n",
      "379 done\n",
      "388 done\n",
      "388 done\n",
      "373 done\n",
      "380 done\n",
      "376 done\n",
      "389 done\n",
      "389 done\n",
      "374 done\n",
      "381 done\n",
      "390 done\n",
      "390 done\n",
      "377 done\n",
      "375 done\n",
      "391 done\n",
      "382 done\n",
      "391 done\n",
      "378 done\n",
      "383 done\n",
      "379 done\n",
      "392 done\n",
      "392 done\n",
      "384 done\n"
     ]
    }
   ],
   "source": [
    "# words_sample = [\"pizza hut\", \"burger king\", \"south africa\", \"nasa\"]\n",
    "# del og_dict\n",
    "def calculate_sim(words, word1, max_sim, closest_word):\n",
    "    t = time.time()\n",
    "    i = 0\n",
    "    for word2 in words:\n",
    "        try:\n",
    "            sim = wiki2vec.similarity(\"_\".join(word1.lower().split()), \"_\".join(word2.split()))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                closest_word = word2\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    print (\"Original word: \", word1, \"Closest Word: \", closest_word)\n",
    "    print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-a))\n",
    "    sys.stdout.flush()\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_w2v(word1):\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    if word1 not in wiki2vec.wv.vocab:\n",
    "        print (\"Original word not in vocab\", word1)\n",
    "        return (closest_word, max_sim)\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, word1, max_sim, closest_word)\n",
    "    return word1, closest_word          \n",
    "\n",
    "a = time.time()\n",
    "\n",
    "# closest_word = closest_word_w2v(\"margherita pizza\")\n",
    "\n",
    "# closest_word_w2v(\"nelson mandela\")\n",
    "\n",
    "resolved = dict()\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    for res in executor.map(closest_word_w2v, failed):\n",
    "        resolved[res[0]] = res[1]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# w2v = KeyedVectors.load_word2vec_format(\"~/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(\"/home/vlead/enwiki_20180420_win10_300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "og_dict = deepcopy(wiki2vec.wv.vocab)\n",
    "for k in og_dict:\n",
    "    if \"/\" in k:\n",
    "        wiki2vec.wv.vocab[k.split(\"/\")[1].lower()] = wiki2vec.wv.vocab[k]\n",
    "        del wiki2vec.wv.vocab[k]\n",
    "del og_dict\n",
    "f = open(\"w2v_data\", \"wb\")\n",
    "pickle.dump([words, failed], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005288124084472656\n",
      "0.00016236305236816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"abrkadabra\" in w2v.wv\n",
    "print (time.time()-a)\n",
    "a = time.time()\n",
    "try:\n",
    "    w2v.similarity(\"margherita_pizza\", \"abrkadabra\")\n",
    "except:    \n",
    "    print (time.time()-a)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2id_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0884,  0.2092, -0.1895, -0.1527, -0.0978,  0.0378, -0.1611,\n",
       "        0.0245,  0.0549, -0.2892,  0.0931, -0.3243, -0.2276, -0.0727,\n",
       "        0.0521, -0.2883, -0.0754, -0.0059, -0.0705, -0.3562, -0.1019,\n",
       "        0.0847,  0.111 ,  0.0049, -0.3304, -0.2235,  0.1369, -0.1037,\n",
       "       -0.0751, -0.3887,  0.1092, -0.1504,  0.0167,  0.0217,  0.0204,\n",
       "        0.064 , -0.2647,  0.3114, -0.0973,  0.1509, -0.2116, -0.0882,\n",
       "        0.1436, -0.2557,  0.23  ,  0.1662,  0.04  , -0.1121,  0.0426,\n",
       "       -0.179 , -0.0356, -0.1443, -0.2153, -0.1841, -0.2113, -0.1561,\n",
       "        0.258 , -0.0593, -0.1704, -0.0394, -0.0992, -0.1615,  0.0623,\n",
       "       -0.1708, -0.1204,  0.2041,  0.173 , -0.3095, -0.0589, -0.0366,\n",
       "        0.0084, -0.2201, -0.3896, -0.2086,  0.323 , -0.0779, -0.1028,\n",
       "        0.0626,  0.2596,  0.0631,  0.18  ,  0.1857,  0.3112,  0.0103,\n",
       "        0.2184, -0.102 ,  0.0504,  0.0907,  0.2355,  0.2216,  0.0125,\n",
       "        0.0075,  0.0846, -0.1534,  0.4137,  0.0309, -0.2167, -0.0785,\n",
       "       -0.0552,  0.1009,  0.2382,  0.0789, -0.0333, -0.2412, -0.1341,\n",
       "        0.0201,  0.2794,  0.0011,  0.0298, -0.1577, -0.1338, -0.2247,\n",
       "        0.0086, -0.1434,  0.1252,  0.1057, -0.0273, -0.1806,  0.07  ,\n",
       "       -0.0414, -0.2173, -0.1507, -0.1246, -0.006 , -0.3306,  0.2111,\n",
       "       -0.1423,  0.0107, -0.1104, -0.1613,  0.4693,  0.0907, -0.1883,\n",
       "        0.0796, -0.0664, -0.0051, -0.2936, -0.146 ,  0.3968,  0.126 ,\n",
       "       -0.003 ,  0.0855,  0.2327,  0.0266, -0.1693, -0.1261,  0.1542,\n",
       "       -0.062 ,  0.0307,  0.0056, -0.274 ,  0.1674, -0.0414,  0.2391,\n",
       "       -0.2105, -0.1144,  0.0929, -0.12  ,  0.1924, -0.0865, -0.0856,\n",
       "        0.1252,  0.1665, -0.2329, -0.0251, -0.0148,  0.0345, -0.188 ,\n",
       "       -0.0538,  0.0334,  0.1629, -0.1095, -0.1309, -0.0365, -0.2053,\n",
       "        0.0962,  0.5739, -0.1491,  0.1356,  0.1067,  0.2074,  0.1196,\n",
       "        0.0801,  0.2219, -0.1299,  0.0929, -0.2259, -0.1049, -0.3055,\n",
       "       -0.1043, -0.0748,  0.1801, -0.35  ,  0.0795,  0.2982, -0.049 ,\n",
       "        0.1819, -0.3701,  0.1377, -0.0465,  0.1129, -0.2537,  0.1466,\n",
       "       -0.032 , -0.005 , -0.0516,  0.0883, -0.1519, -0.1497, -0.0941,\n",
       "       -0.1741,  0.1991, -0.1138, -0.0219, -0.122 , -0.0123, -0.0136,\n",
       "        0.2865, -0.3002, -0.0241,  0.0726,  0.0654, -0.235 , -0.1227,\n",
       "       -0.2656, -0.2147, -0.1193,  0.011 , -0.3328,  0.3237,  0.0375,\n",
       "        0.1942,  0.0471, -0.0605,  0.0011,  0.0636, -0.03  , -0.1148,\n",
       "       -0.0559, -0.1026, -0.3166,  0.3044,  0.1773,  0.2265,  0.1125,\n",
       "       -0.114 ,  0.1113,  0.0789, -0.2937, -0.2036, -0.2807,  0.0188,\n",
       "        0.0573,  0.0679,  0.1155,  0.1796, -0.1041,  0.1025, -0.0483,\n",
       "        0.0043,  0.1314,  0.0846,  0.1881,  0.1721,  0.463 , -0.1782,\n",
       "       -0.1547,  0.2518,  0.0634,  0.2856,  0.6526, -0.3789,  0.1362,\n",
       "        0.1545,  0.0719, -0.1429, -0.1224,  0.0836,  0.0851, -0.0349,\n",
       "       -0.2552, -0.2929, -0.183 ,  0.2112, -0.0847, -0.148 , -0.1365,\n",
       "       -0.1245,  0.3218,  0.0563,  0.245 ,  0.0132, -0.1204,  0.0391,\n",
       "       -0.1132, -0.0707,  0.2167, -0.0322, -0.1085,  0.0054],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2vec[\"january\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"resolved\", \"rb\")\n",
    "resolved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peter wyche (diplomat)': ('', -1000),\n",
       " 'acoma-zuni section': ('', -1000),\n",
       " 'madan-harini': ('', -1000),\n",
       " 'trust no one (internet security)': ('', -1000),\n",
       " 'ipa pulmonic consonant chart with audio': ('', -1000),\n",
       " 'isobase': ('', -1000),\n",
       " 'international tibet independence movement': ('', -1000),\n",
       " 'human computer interaction (security)': ('', -1000),\n",
       " 'poetas de karaoke': ('', -1000),\n",
       " 'lego clutch powers: bad hair day': ('', -1000),\n",
       " 'yendegaia airport': ('', -1000),\n",
       " 'aed (non-profit)': ('', -1000),\n",
       " 'quilmes airport': ('', -1000),\n",
       " 'the pack a.d.': ('', -1000),\n",
       " 'harvie-watt baronets': ('', -1000),\n",
       " 'sharp actius rd3d notebook': ('', -1000),\n",
       " 'big beach boutique ii - the movie': ('', -1000),\n",
       " 'privacy by design': ('', -1000),\n",
       " 'motorola devour': ('', -1000),\n",
       " 'piracy act': ('', -1000),\n",
       " 'antonio sánchez (puerto rican host)': ('', -1000),\n",
       " 'yinzcam': ('nanorex', 0.69111955),\n",
       " 'starter ring gear': ('', -1000),\n",
       " 'electronic logbook': ('', -1000),\n",
       " 'greg burke (journalist)': ('', -1000),\n",
       " 'deaths in november 2013': ('', -1000),\n",
       " 'confederation of indigenous nationalities of the ecuadorian amazon': ('',\n",
       "  -1000),\n",
       " 'hp mini 311': ('', -1000),\n",
       " 'url subscription architecture': ('', -1000),\n",
       " 'snowballers entertainment': ('', -1000),\n",
       " 'basic strategic arts program': ('', -1000),\n",
       " 'mars (ticket reservation system)': ('', -1000),\n",
       " 'edward graham lee': ('', -1000),\n",
       " 'matija kristić': ('', -1000),\n",
       " 'non-prophets': ('prolyphic', 0.6965268),\n",
       " 'rebellion of the three guards': ('', -1000),\n",
       " 'the recipe for gertrude': ('', -1000),\n",
       " 'russian amateur radio union': ('', -1000),\n",
       " 'quiet pc': ('', -1000),\n",
       " 'prince-abbot': ('hochstift', 0.6136858),\n",
       " 'core strategy document': ('', -1000),\n",
       " 'mutukula airport': ('', -1000),\n",
       " 'magnus l. kpakol': ('', -1000),\n",
       " 'strangers (malibu comics)': ('', -1000),\n",
       " 'cary baronets': ('', -1000),\n",
       " 'andrew wood (diplomat)': ('', -1000),\n",
       " 'lead petty officer': ('', -1000),\n",
       " 'corps of military police (india)': ('', -1000),\n",
       " 'siniša radanović': ('', -1000),\n",
       " 'academic research alliance': ('', -1000),\n",
       " 'oatmeal cookie': ('', -1000),\n",
       " 'joseph e. duncan iii': ('', -1000),\n",
       " '3650': ('3800', 0.62255555),\n",
       " 'lideta army airport': ('', -1000),\n",
       " '(isc)²': ('isc2', 0.79769313),\n",
       " 'constitution of the state of guanajuato': ('', -1000),\n",
       " 'aldo leopold legacy trail system': ('', -1000),\n",
       " 'the adventures of jimmy neutron: boy genius': ('', -1000),\n",
       " 'perfectionism (psychology)': ('', -1000),\n",
       " 'james keene (writer)': ('', -1000),\n",
       " 'hussam al sayed': ('', -1000),\n",
       " 'germán olano airport': ('', -1000),\n",
       " 'dano-swedish war of 1808-09': ('', -1000),\n",
       " '1260 (computer virus)': ('', -1000),\n",
       " 'lynn boden': ('', -1000),\n",
       " 'looking glass workshop': ('', -1000),\n",
       " 'endicott pear tree': ('', -1000),\n",
       " 'cascade storage system': ('', -1000),\n",
       " 'diamond is unbreakable': ('', -1000),\n",
       " '1964 (emulator)': ('', -1000),\n",
       " 'eurasmus': ('', -1000),\n",
       " 'national training system (australia)': ('', -1000),\n",
       " 'valerie aurora': ('', -1000),\n",
       " 'co-founder': ('', -1000),\n",
       " 'stucley baronets': ('', -1000),\n",
       " 'meta (academic company)': ('', -1000),\n",
       " 'robert g. wright': ('', -1000),\n",
       " '19 you me': ('', -1000),\n",
       " 'master of business informatics': ('', -1000),\n",
       " 'edward llewellyn (conservative adviser)': ('', -1000),\n",
       " 'andre barcat': ('', -1000),\n",
       " 'italian electronic identity card': ('', -1000),\n",
       " 'dappen glass': ('', -1000),\n",
       " 'camp delta standard operating procedures': ('', -1000),\n",
       " 'jonas acquistapace': ('', -1000),\n",
       " 'naca score': ('', -1000),\n",
       " 'hayworth hicks': ('', -1000),\n",
       " 'category of sets': ('', -1000),\n",
       " 'everly (group)': ('', -1000),\n",
       " 'patricia gudiel': ('', -1000),\n",
       " 'freedom records': ('', -1000),\n",
       " 'skipjack (cipher)': ('', -1000),\n",
       " 'program resources and outcome management information system': ('', -1000),\n",
       " 'off key melodies': ('', -1000),\n",
       " 'python syntax and semantics': ('', -1000),\n",
       " 'segundo durandal': ('', -1000),\n",
       " 'ventanas radioayuda airport': ('', -1000),\n",
       " 'parachute youth': ('', -1000),\n",
       " 'zhou li (diplomat)': ('', -1000),\n",
       " 'yoo byung-ok': ('', -1000),\n",
       " 'earl of northampton': ('', -1000),\n",
       " 'timeline of the war on terror': ('', -1000),\n",
       " 'i predict 1990': ('', -1000),\n",
       " 'fleet electronic warfare center': ('', -1000),\n",
       " 'list of rifftrax': ('', -1000),\n",
       " 'pettus baronets': ('', -1000),\n",
       " 'kailo airport': ('', -1000),\n",
       " 'limnonectes acanthi': ('', -1000),\n",
       " 'observatory for protection of cultural heritage in areas of crisis': ('',\n",
       "  -1000),\n",
       " 'earl of wessex': ('', -1000),\n",
       " 'army catering corps': ('', -1000),\n",
       " 'miss world poland': ('', -1000),\n",
       " 'canadian university society for intercollegiate debate': ('', -1000),\n",
       " 'thinkpad 8': ('', -1000),\n",
       " 'sandro tsveiba': ('', -1000),\n",
       " 'jetking infotrain': ('', -1000),\n",
       " 'fritz andré': ('', -1000),\n",
       " 'mister global indonesia': ('', -1000),\n",
       " 'network telescope': ('', -1000),\n",
       " 'justin britt': ('', -1000),\n",
       " 'world archery rankings': ('', -1000),\n",
       " 'arp spoofing': ('', -1000),\n",
       " 'stephania japonica': ('', -1000),\n",
       " 'scoot.com': ('', -1000),\n",
       " 'leonardo world (canada)': ('', -1000),\n",
       " 'novogamer': ('', -1000),\n",
       " 'tomioka tessai': ('', -1000),\n",
       " 'buenos aires bid for the 2018 summer youth olympics': ('', -1000),\n",
       " 'bridge 9 records': ('', -1000),\n",
       " 'internal passport': ('', -1000),\n",
       " 'phyllomedusa atelopoides': ('', -1000),\n",
       " 'negros occidental national science high school': ('', -1000),\n",
       " 'dayton–wright brothers airport': ('', -1000),\n",
       " 'aasohtx': ('', -1000),\n",
       " 'vanish (computer science)': ('', -1000),\n",
       " 'arthur g. jones-williams': ('', -1000),\n",
       " 'morehouse memorial airport': ('', -1000),\n",
       " 'vi corps (united kingdom)': ('', -1000),\n",
       " 'mario méndez (panamanian footballer)': ('', -1000),\n",
       " 'miss uganda': ('', -1000),\n",
       " 'adepd': ('esdis', 0.801283),\n",
       " 'faed mustafa': ('', -1000),\n",
       " 'american canoe association': ('', -1000),\n",
       " 'osmothèque': ('jicky', 0.6577778),\n",
       " '’': ('', -1000),\n",
       " 'presbyterian peace fellowship': ('', -1000),\n",
       " 'registered tax return preparer': ('', -1000),\n",
       " 'discodeles bufoniformis': ('', -1000),\n",
       " 'chase (land)': ('', -1000),\n",
       " 'labour and socialist international': ('', -1000),\n",
       " 'odd child recordings': ('', -1000),\n",
       " 'toad (software)': ('', -1000),\n",
       " 'tatiana zaitseva': ('', -1000),\n",
       " 'hinche airport': ('', -1000),\n",
       " 'center for christian-democratic studies': ('', -1000),\n",
       " 'fugue (hash function)': ('', -1000),\n",
       " 'igor mintusov': ('', -1000),\n",
       " 'justice at the gate': ('', -1000),\n",
       " 'chase hilgenbrinck': ('', -1000),\n",
       " 'marshall ferguson': ('', -1000),\n",
       " 'national debt act': ('', -1000),\n",
       " 'kouassi kouadja': ('', -1000),\n",
       " 'jean-paul abalo': ('', -1000),\n",
       " 'clearcheckbook.com': ('', -1000),\n",
       " 'economic community of the great lakes countries': ('', -1000),\n",
       " 'talmiz ahmad': ('', -1000),\n",
       " 'robert watson (computer scientist)': ('', -1000),\n",
       " \"ha'il regional airport\": ('', -1000),\n",
       " 'burlington northern depot (amenia, north dakota)': ('', -1000),\n",
       " \"yoreh de'ah\": ('', -1000),\n",
       " 'fuckart & pimp': ('', -1000),\n",
       " 'roger jackling (diplomat)': ('', -1000),\n",
       " 'ambrosiy chachua': ('', -1000),\n",
       " 'mapa quinatzin': ('', -1000),\n",
       " 'lewis carroll society of north america': ('', -1000),\n",
       " 'lymanske airport': ('', -1000),\n",
       " 'bjørn skogmo': ('', -1000),\n",
       " 'charles w. blackwell': ('', -1000),\n",
       " 'santa rosa del sara airport': ('', -1000),\n",
       " 'asian liver center': ('', -1000),\n",
       " 'peter hayes (diplomat)': ('', -1000),\n",
       " 'kaidā_glyphs': ('shunbajunki', 0.6622865),\n",
       " 'orán airport': ('', -1000),\n",
       " 'g.i. joe: cobra strike': ('', -1000),\n",
       " 'yi jiang chun shui xiang dong liu': ('', -1000),\n",
       " 'wilfredo alvarado': ('', -1000),\n",
       " \"natashquan (lac de l'avion) water aerodrome\": ('', -1000),\n",
       " 'ranks and insignia of the national socialist motor corps': ('', -1000),\n",
       " 'science and public policy institute': ('', -1000),\n",
       " 'list of government mass surveillance projects': ('', -1000),\n",
       " 'road traffic control department': ('', -1000),\n",
       " 'lenovo ideapad u310': ('', -1000),\n",
       " 'sapo (computer)': ('', -1000),\n",
       " 'thorium energy alliance': ('', -1000),\n",
       " 'c. b. atkins & c. e. snow by special request': ('', -1000),\n",
       " \"women's caucus for art\": ('', -1000),\n",
       " 'albany municipal airport (oregon)': ('', -1000),\n",
       " 'association against bulgarian bandits': ('', -1000),\n",
       " 'kahrizak charity foundation': ('', -1000),\n",
       " 'ĺ\\xa0kocjan caves': ('', -1000),\n",
       " 'nuttall baronets': ('', -1000),\n",
       " 'binary black hole': ('', -1000),\n",
       " 'dynamic vibration absorber': ('', -1000),\n",
       " 'order of precedence in prince edward island': ('', -1000),\n",
       " 'association of women for action and research': ('', -1000),\n",
       " '1legcall': ('emoze', 0.8751701),\n",
       " 'carolina bird club': ('', -1000),\n",
       " 'sculpture in the parklands': ('', -1000),\n",
       " 'roswell spencer house': ('', -1000),\n",
       " 'notre-dame de chrétienté': ('', -1000),\n",
       " 'center island airport': ('', -1000),\n",
       " 'master tracks pro': ('', -1000),\n",
       " 'taichung mass rapid transit': ('', -1000),\n",
       " 'economy of iran': ('', -1000),\n",
       " 'dynamic duo (south korean duo)': ('', -1000),\n",
       " '3 skypephone s2': ('', -1000),\n",
       " 'wanna make you love me': ('', -1000),\n",
       " 'djurgårdens_if_boxningsförening': ('', -1000),\n",
       " 'marco förster': ('', -1000),\n",
       " 'ipad mini 3': ('', -1000),\n",
       " 'al-rasheed airport': ('', -1000),\n",
       " 'joe fields (safety)': ('', -1000),\n",
       " 'common year (greyhawk)': ('', -1000),\n",
       " 'libpath': ('', -1000),\n",
       " 'wonders & worries': ('', -1000),\n",
       " 'masih zahedi': ('', -1000),\n",
       " 'dos-11': ('', -1000),\n",
       " 'jundallah (iran)': ('', -1000),\n",
       " 'tunku besar': ('', -1000),\n",
       " 'certificate-based encryption': ('', -1000),\n",
       " 'rhacophorus yinggelingensis': ('', -1000),\n",
       " 'regions of albania': ('', -1000),\n",
       " 'maderas rainforest conservancy': ('', -1000),\n",
       " 'chet atkins in hollywood': ('', -1000),\n",
       " 'steven m. bellovin': ('', -1000),\n",
       " 'callulops dubius': ('', -1000),\n",
       " 'gerome sapp': ('', -1000),\n",
       " 'turpan jiaohe airport': ('', -1000),\n",
       " 'typhoon yuri (1991)': ('', -1000),\n",
       " 'miss france monde': ('', -1000),\n",
       " 'birtles & goble': ('', -1000),\n",
       " 'richard alvin neilson': ('', -1000),\n",
       " 'baron mildmay of flete': ('', -1000),\n",
       " 'black-spotted sticky frog': ('', -1000),\n",
       " 'earl of scarsdale': ('', -1000),\n",
       " 'rené trost': ('', -1000),\n",
       " 'fossil (software)': ('', -1000),\n",
       " 'patome': ('', -1000),\n",
       " 'svyatoslav shabanov': ('', -1000),\n",
       " 'ken mahoney': ('', -1000),\n",
       " 'invisible agent (record label)': ('', -1000),\n",
       " 'primitive ring': ('', -1000),\n",
       " 'all writs act': ('', -1000),\n",
       " \"dave leip's atlas of u.s. presidential elections\": ('', -1000),\n",
       " \"k'nex\": ('mecanum', 0.549265),\n",
       " 'approaches to prejudice reduction': ('', -1000),\n",
       " 'american independent institute': ('', -1000),\n",
       " 'public delivery': ('', -1000),\n",
       " 'modern art - the best of john foxx': ('', -1000),\n",
       " 'macgregor baronets': ('', -1000),\n",
       " '3cx phone system': ('', -1000),\n",
       " 'nicholas tandi dammen': ('', -1000),\n",
       " 'saeid janfada': ('', -1000),\n",
       " '14th ward of new orleans': ('', -1000),\n",
       " 'bodhi and friends': ('', -1000),\n",
       " 'bob newton (american football)': ('', -1000),\n",
       " 'advanced disc filing system': ('', -1000),\n",
       " 'central register of foreign nationals (germany)': ('', -1000),\n",
       " 'elena urkizu': ('', -1000),\n",
       " 'artem bobukh': ('', -1000),\n",
       " 'floyd baronets': ('', -1000),\n",
       " 'sertoca (record label)': ('', -1000),\n",
       " 'the art of yoga project': ('', -1000),\n",
       " 'eros and agape': ('', -1000),\n",
       " 'robert maxwell (producer)': ('', -1000),\n",
       " 'el mirador airport': ('', -1000),\n",
       " 'badorb.com': ('', -1000),\n",
       " 'edwards-moss baronets': ('', -1000),\n",
       " 'volodymyr kovalyuk': ('', -1000),\n",
       " 'seventh-day adventist church state council': ('', -1000),\n",
       " 'party of united pensioners of serbia': ('', -1000),\n",
       " 'gunstar super heroes': ('', -1000),\n",
       " 'hd 114762': ('', -1000),\n",
       " 'doichang frog': ('', -1000),\n",
       " 'luniemu airport': ('', -1000),\n",
       " 'cacosternum': ('poyntoni', 0.8458121),\n",
       " 'maciej stolarczyk': ('', -1000),\n",
       " 'turaiha': ('chhatriya', 0.7182206),\n",
       " 'tysyn hartman': ('', -1000),\n",
       " 'whiskercontrol': ('', -1000),\n",
       " 'los vilos airport': ('', -1000),\n",
       " 'minerva (cable system)': ('', -1000),\n",
       " 'transportation and land use coalition': ('', -1000),\n",
       " 'richard lavers': ('', -1000),\n",
       " 'fpq-16 parcs': ('', -1000),\n",
       " 'when the storm is over': ('', -1000),\n",
       " 'epsilon lupi': ('', -1000),\n",
       " '3wplayer': ('divocodec', 0.854064),\n",
       " 'cmx dvd': ('', -1000),\n",
       " 'fauziah mohd taib': ('', -1000),\n",
       " 'humane society of indianapolis': ('', -1000),\n",
       " 'button baronets': ('', -1000),\n",
       " 'society of satellite professionals international': ('', -1000),\n",
       " 'go (h2o album)': ('', -1000),\n",
       " 'heinrich wilhelm von werther': ('', -1000),\n",
       " 'wpvc-lp': ('', -1000),\n",
       " 'santa rosa airport (argentina)': ('', -1000),\n",
       " 'harford jones-brydges': ('', -1000),\n",
       " 'marion schultz': ('', -1000),\n",
       " 'roger l. ogden': ('', -1000),\n",
       " 'saxony-anhalt football association': ('', -1000),\n",
       " 'mike mcleod (gridiron football)': ('', -1000),\n",
       " 'thamkharka airport': ('', -1000),\n",
       " '4k (computer virus)': ('', -1000),\n",
       " 'identified (company)': ('', -1000),\n",
       " 'pichidangui airport': ('', -1000),\n",
       " 'karuna trust (india)': ('', -1000),\n",
       " 'soundplate': ('factmag', 0.761556),\n",
       " 'quackdown': ('', -1000),\n",
       " 'low pros': ('', -1000),\n",
       " 'steve macdonald (soccer)': ('', -1000),\n",
       " 'hd 112028': ('', -1000),\n",
       " 'tropical storm franklin (2005)': ('', -1000),\n",
       " 'yĺťsuke matsuoka': ('', -1000),\n",
       " 'the pope app': ('', -1000),\n",
       " 'oliver wardrop': ('', -1000),\n",
       " 'summit airport (delaware)': ('', -1000),\n",
       " 'whispering pines: live at the getaway': ('', -1000),\n",
       " 'blues control': ('', -1000),\n",
       " 'wsl world light heavyweight championship': ('', -1000),\n",
       " 'meteoalarm': ('', -1000),\n",
       " 'deputado luă': ('', -1000),\n",
       " \"shanghai people's press\": ('', -1000),\n",
       " 'ninite': ('syncbackpro', 0.8344648),\n",
       " 'united states navy seals in popular culture': ('', -1000),\n",
       " 'heresy records': ('', -1000),\n",
       " 'peter a. freeman': ('', -1000),\n",
       " 'zebrafish information network': ('', -1000),\n",
       " 'gopalapuram parthasarathy': ('', -1000),\n",
       " 'corner stone cues': ('', -1000),\n",
       " 'full time hobby': ('', -1000),\n",
       " 'kempile airport': ('', -1000),\n",
       " 'energy label': ('', -1000),\n",
       " 'w. e. upjohn institute for employment research': ('', -1000),\n",
       " 'surgeon (musician)': ('', -1000),\n",
       " 'airline ambassadors international': ('', -1000),\n",
       " 'andrej benedejäťiäť': ('', -1000),\n",
       " 'geos (8-bit operating system)': ('', -1000),\n",
       " 'federation of kerala associations in north america': ('', -1000),\n",
       " 'john andrew licharson': ('', -1000),\n",
       " \"savage's brook frog\": ('', -1000),\n",
       " 'baron clifford of chudleigh': ('', -1000),\n",
       " 'bobbi jo steadward': ('', -1000),\n",
       " \"carter's army\": ('', -1000),\n",
       " 'high treason in the united kingdom': ('', -1000),\n",
       " 'salvatore d. morgera': ('', -1000),\n",
       " 'mount vernon police department (new york)': ('', -1000),\n",
       " 'completeness (cryptography)': ('', -1000),\n",
       " 'art souterrain': ('', -1000),\n",
       " 'new york youth symphony': ('', -1000),\n",
       " 'anthony smith (safety)': ('', -1000),\n",
       " 'director-general': ('', -1000),\n",
       " 'hardscrabble farm': ('', -1000),\n",
       " 'gracixalus jinxiuensis': ('', -1000),\n",
       " 'milad zakipour': ('', -1000),\n",
       " 'baron knoop stradivarius': ('', -1000),\n",
       " 'karlsruhe model': ('', -1000),\n",
       " 'speaker of the west bengal legislative assembly': ('', -1000),\n",
       " 'task manager (windows)': ('', -1000),\n",
       " 'amir khosrow afshar': ('', -1000),\n",
       " \"na'ama cohen\": ('', -1000),\n",
       " 'negro string quartet': ('', -1000),\n",
       " 'anglo-persian agreement': ('', -1000),\n",
       " 'haystack (software)': ('', -1000),\n",
       " 'computer-assisted passenger prescreening system': ('', -1000),\n",
       " 'dmitri tikhiy': ('', -1000),\n",
       " 'anthony zboralski': ('', -1000),\n",
       " 'dominion alliance for the total suppression of the liquor traffic': ('',\n",
       "  -1000),\n",
       " 'meo (mobile phone company)': ('', -1000),\n",
       " 'denis baliĺˇ': ('', -1000),\n",
       " 'saizerais aerodrome': ('', -1000),\n",
       " 'mason county airport (west virginia)': ('', -1000),\n",
       " 'william fox-strangways, 4th earl of ilchester': ('', -1000),\n",
       " 'dedication and everlasting love to animals rescue': ('', -1000),\n",
       " 'mară': ('', -1000),\n",
       " \"university of lethbridge students' union\": ('', -1000),\n",
       " 'outbound laptop': ('', -1000),\n",
       " 'falconhead airport': ('', -1000),\n",
       " 'rustock botnet': ('', -1000),\n",
       " 'tarabuco municipality': ('', -1000),\n",
       " 'waldenburg railway': ('', -1000),\n",
       " 'queen sofă': ('', -1000),\n",
       " 'hylarana jimiensis': ('', -1000),\n",
       " 'morton i. abramowitz': ('', -1000),\n",
       " 'reaching across illinois library system': ('', -1000),\n",
       " 'kirkegata (levanger)': ('', -1000),\n",
       " 'two-tier system': ('', -1000),\n",
       " 'amethod public schools': ('', -1000),\n",
       " 'chathamattom': ('kadamattam', 0.760544),\n",
       " 'oblivious data structure': ('', -1000),\n",
       " 'atkins baronets': ('', -1000),\n",
       " 'major cities of europe it users group': ('', -1000),\n",
       " 'grind planets': ('', -1000),\n",
       " 'born again (comics)': ('', -1000),\n",
       " 'virginia academy of science': ('', -1000),\n",
       " 'ahmad ibrahim khalaf': ('', -1000),\n",
       " \"people's writer\": ('', -1000),\n",
       " 'hd 224635': ('', -1000),\n",
       " 'zăľm': ('', -1000),\n",
       " 'muntz baronets': ('', -1000),\n",
       " '9th airborne division (united states)': ('', -1000),\n",
       " 'max-80': ('amsdos', 0.7762573),\n",
       " 'fifteenth street and oklahoma avenue brick street': ('', -1000),\n",
       " 'hayat production': ('', -1000),\n",
       " 'count of paris': ('', -1000),\n",
       " 'flow (software)': ('', -1000),\n",
       " 'wallum froglet': ('', -1000),\n",
       " 'balassi institute': ('', -1000),\n",
       " 'interpretation of knowledge': ('', -1000),\n",
       " 'ceramic capacitor': ('', -1000),\n",
       " 'fork-256': ('', -1000),\n",
       " 'a-0 system': ('', -1000),\n",
       " 'cowell-stepney baronets': ('', -1000),\n",
       " 'hifk fotboll': ('', -1000),\n",
       " 'bill corbus': ('', -1000),\n",
       " 'austrochaperina guttata': ('', -1000),\n",
       " 'anton smetanin': ('', -1000),\n",
       " 'sri lanka software testing board': ('', -1000),\n",
       " 'countermeasure (computer)': ('', -1000),\n",
       " 'ama (title)': ('', -1000),\n",
       " 'dante poli': ('', -1000),\n",
       " 'sergei katunin': ('', -1000),\n",
       " 'protocol recordings': ('', -1000),\n",
       " 'oghab 2': ('', -1000),\n",
       " 'adrien l. j. leps': ('', -1000),\n",
       " 'hd 33636': ('', -1000),\n",
       " 'skweez media': ('', -1000),\n",
       " 'duck circovirus': ('', -1000),\n",
       " 'three-stage quantum cryptography protocol': ('', -1000),\n",
       " 'rai (title)': ('', -1000),\n",
       " 'laureano ruiz': ('', -1000),\n",
       " 'lake chelan airport': ('', -1000),\n",
       " 'telmatobius edaphonastes': ('', -1000),\n",
       " 'ms-chap': ('chapv2', 0.91095746),\n",
       " 'safe passage (charity)': ('', -1000),\n",
       " 'samata (ngo)': ('', -1000),\n",
       " 'aaq-26': ('', -1000),\n",
       " 'nasa crows landing airport': ('', -1000),\n",
       " 'scottish fire and rescue service': ('', -1000),\n",
       " 'george town aerodrome': ('', -1000),\n",
       " 'earl of ducie': ('', -1000),\n",
       " 'atc code h': ('', -1000),\n",
       " 'i hope i shall arrive soon': ('', -1000),\n",
       " 'windows 1.0': ('', -1000),\n",
       " 'paramo robber frog': ('', -1000),\n",
       " 'dybuster': ('calcularis', 0.84332633),\n",
       " 'american anglican council': ('', -1000),\n",
       " 'abbassa malik': ('', -1000),\n",
       " 'eduard dubinski': ('', -1000),\n",
       " 'antenna music': ('', -1000),\n",
       " 'hynek kmonă': ('', -1000),\n",
       " 'santiago hoyos': ('', -1000),\n",
       " 'libya shield force': ('', -1000),\n",
       " 'lene terp': ('', -1000),\n",
       " 'national center for missing & exploited children': ('', -1000),\n",
       " 'ilibrarian': ('', -1000),\n",
       " '2012 homeless world cup': ('', -1000),\n",
       " \"assane n'diaye\": ('', -1000),\n",
       " 'intermezzo (file system)': ('', -1000),\n",
       " 'the secret footballer': ('', -1000),\n",
       " 'single-hung': ('', -1000),\n",
       " 'cheboksary airport': ('', -1000),\n",
       " 'tesol international association': ('', -1000),\n",
       " 'abc (computer virus)': ('', -1000),\n",
       " 'underground symphony': ('', -1000),\n",
       " 'tvă': ('', -1000),\n",
       " 'koncert beogradska arena': ('', -1000),\n",
       " 'dunav zone league': ('', -1000),\n",
       " 'paris municipal airport': ('', -1000),\n",
       " 'international commission for the conservation of atlantic tunas': ('',\n",
       "  -1000),\n",
       " 'alan b. oppenheimer': ('', -1000),\n",
       " 'whitebelly tree frog': ('', -1000),\n",
       " 'smith-marriott baronets': ('', -1000),\n",
       " 'hp-71b': ('mobilepro', 0.75160855),\n",
       " 'aracati airport': ('', -1000),\n",
       " 'sir frederick currie, 1st baronet': ('', -1000),\n",
       " 'marksman anti-aircraft system': ('', -1000),\n",
       " 'li zhe (footballer)': ('', -1000),\n",
       " 'capistrano (software)': ('', -1000),\n",
       " 'psyshark': ('', -1000),\n",
       " 'abdullah al-saidi': ('', -1000),\n",
       " 'cherokee nation airport': ('', -1000),\n",
       " 'inter-american center of tax administrations': ('', -1000),\n",
       " 'sounds to sample': ('', -1000),\n",
       " 'miss world myanmar': ('', -1000),\n",
       " 'discover ireland': ('', -1000),\n",
       " 'westhollow society': ('', -1000),\n",
       " 'national strategy for victory in iraq': ('', -1000),\n",
       " 'ohio dental association': ('', -1000),\n",
       " 'moscow five': ('', -1000),\n",
       " 'fascial compartment': ('', -1000),\n",
       " 'said the gramophone': ('', -1000),\n",
       " 'simon harcourt, 1st earl harcourt': ('', -1000),\n",
       " 'server immersion cooling': ('', -1000),\n",
       " 'voltage controller': ('', -1000),\n",
       " 'istvăˇn ludăˇnszki': ('', -1000),\n",
       " 'tony dye': ('', -1000),\n",
       " 'cruzeiro do sul international airport': ('', -1000),\n",
       " 'george nakashima house, studio and workshop': ('', -1000),\n",
       " 'extended cold weather clothing system': ('', -1000),\n",
       " 'heyman baronets': ('', -1000),\n",
       " 'roderick sarell': ('', -1000),\n",
       " 'demsetz auction': ('', -1000),\n",
       " 'fanfare (decoy)': ('', -1000),\n",
       " 'albericus laurini': ('', -1000),\n",
       " 'national animal interest alliance': ('', -1000),\n",
       " 'jake dancy': ('', -1000),\n",
       " 'gather.com': ('wallstrip', 0.6233394),\n",
       " 'wannukandi airport': ('', -1000),\n",
       " 'computability in europe': ('', -1000),\n",
       " 'list of israeli ashkenazi jews': ('', -1000),\n",
       " 'seamiew records': ('', -1000),\n",
       " 'museum label': ('', -1000),\n",
       " 'keeprecipes': ('', -1000),\n",
       " 'dani massunguna': ('', -1000),\n",
       " 'raptor resource project': ('', -1000),\n",
       " 'international institute for inspiration': ('', -1000),\n",
       " 'noeprimer': ('', -1000),\n",
       " 'regi base foundation': ('', -1000),\n",
       " 'the world at one': ('', -1000),\n",
       " 'east berlin, connecticut': ('', -1000),\n",
       " 'shelton johnson (american football)': ('', -1000),\n",
       " 'red bull records': ('', -1000),\n",
       " \"bathsheba's spring and bower\": ('', -1000),\n",
       " 'free music production': ('', -1000),\n",
       " 'necw triple crown heavyweight championship': ('', -1000),\n",
       " 'roman pomazan': ('', -1000),\n",
       " 'knifefight': ('middledistancerunner', 0.70236576),\n",
       " 'jonathan nelson (american football)': ('', -1000),\n",
       " 'national vulnerability database': ('', -1000),\n",
       " 'linux-vserver': ('vserver', 0.82906413),\n",
       " 'jake records': ('', -1000),\n",
       " 'laser survey': ('', -1000),\n",
       " 'japanese velvet dogfish': ('', -1000),\n",
       " 'nottinghamshire fire and rescue service': ('', -1000),\n",
       " \"eve's weekly miss india\": ('', -1000),\n",
       " 'koenig memorandum': ('', -1000),\n",
       " 'civil pages': ('', -1000),\n",
       " 'braille kanji': ('', -1000),\n",
       " 'profit-map methodology': ('', -1000),\n",
       " 'opie authentication system': ('', -1000),\n",
       " 'gewayantana airport': ('', -1000),\n",
       " 'hanneke mensink': ('', -1000),\n",
       " 'ratnik (program)': ('', -1000),\n",
       " 'viscount of kenmure': ('', -1000),\n",
       " 'harlem airport': ('', -1000),\n",
       " 'spencer buford house': ('', -1000),\n",
       " 'geography of xanth': ('', -1000),\n",
       " 'sweet valley (music group)': ('', -1000),\n",
       " 'heirloom seal of the realm': ('', -1000),\n",
       " 'biological stain commission': ('', -1000),\n",
       " 'certificate of pharmaceutical product': ('', -1000),\n",
       " 'hans glaser': ('', -1000),\n",
       " 'charazani tree frog': ('', -1000),\n",
       " 'hiroshi ota': ('', -1000),\n",
       " 'ramah navajo indian reservation': ('', -1000),\n",
       " 'v.c. corner australian cemetery and memorial': ('', -1000),\n",
       " 'diante do trono (label)': ('', -1000),\n",
       " 'ctx (computer virus)': ('', -1000),\n",
       " 'coordination committee of maoist parties and organisations of south asia': ('',\n",
       "  -1000),\n",
       " 'dssim': ('momel', 0.75709504),\n",
       " 'mountain mile': ('', -1000),\n",
       " 'viva records (philippines)': ('', -1000),\n",
       " 'lg optimus pad': ('', -1000),\n",
       " 'salted music': ('', -1000),\n",
       " '5-uco': ('sigtot', 0.7818365),\n",
       " 'plone (software)': ('', -1000),\n",
       " 'medical library association': ('', -1000),\n",
       " 'hex (discworld)': ('', -1000),\n",
       " 'the losers (vertigo)': ('', -1000),\n",
       " 'vila vila municipality': ('', -1000),\n",
       " 'abc (yet another bittorrent client)': ('', -1000),\n",
       " 'short-legged horned toad': ('', -1000),\n",
       " 'anna kournikova (computer virus)': ('', -1000),\n",
       " 'riyadh the facilitator': ('', -1000),\n",
       " 'caribbean centre of excellence for teacher training': ('', -1000),\n",
       " 'national episcopal historians and archivists': ('', -1000),\n",
       " 'david pickering (writer)': ('', -1000),\n",
       " 'mequieroir.com': ('', -1000),\n",
       " 'running battle': ('', -1000),\n",
       " 'too late to turn back now (album)': ('', -1000),\n",
       " 'oz zion, mateh binyamin': ('', -1000),\n",
       " 'peter jackisch': ('', -1000),\n",
       " 'akelarre (cipher)': ('', -1000),\n",
       " 'abmaps': ('', -1000),\n",
       " 'ningbo rail transit': ('', -1000),\n",
       " 'victor leyva': ('', -1000),\n",
       " 'endless love (tv series)': ('', -1000),\n",
       " 'acc (programming language)': ('', -1000),\n",
       " 'mary m. ourisman': ('', -1000),\n",
       " 'vyacheslav lukhtanov': ('', -1000),\n",
       " 'gorman defreest larner': ('', -1000),\n",
       " 'hristijan dragarski': ('', -1000),\n",
       " 'w. metcalfe and son': ('', -1000),\n",
       " 'journey (ngo)': ('', -1000),\n",
       " 'list of muni metro stations': ('', -1000),\n",
       " 'oscar heron': ('', -1000),\n",
       " 'donn b. parker': ('', -1000),\n",
       " 'anodonthyla moramora': ('', -1000),\n",
       " 'selebi-phikwe airport': ('', -1000),\n",
       " 'tun (product standard)': ('', -1000),\n",
       " 'stand-alone power system': ('', -1000),\n",
       " 'athens bid for the 2010 summer youth olympics': ('', -1000),\n",
       " 'list of on the red dot episodes': ('', -1000),\n",
       " 'louis fleeming jenkin': ('', -1000),\n",
       " 'aceproject': ('', -1000),\n",
       " 'taebukpo ri airport': ('', -1000),\n",
       " 'firebase airborne': ('', -1000),\n",
       " 'compucolor ii': ('', -1000),\n",
       " 'sing totavee': ('', -1000),\n",
       " 'the fireman (band)': ('', -1000),\n",
       " 'alcohol and drugs history society': ('', -1000),\n",
       " 'microhyla erythropoda': ('', -1000),\n",
       " 'institutions in the southern victory series': ('', -1000),\n",
       " 'acer allegro': ('', -1000),\n",
       " 'uluguru blue-bellied frog': ('', -1000),\n",
       " 'desco da parto': ('', -1000),\n",
       " 'sargis karapetyan (footballer, born 1963)': ('', -1000),\n",
       " \"bullwinkle's testimonial dinner\": ('', -1000),\n",
       " 'apocalypse in other media': ('', -1000),\n",
       " 'international association of astronomical artists': ('', -1000),\n",
       " 'hong kong five-cent note': ('', -1000),\n",
       " 'golden pass (disney)': ('', -1000),\n",
       " 'bigdecisions': ('', -1000),\n",
       " 'canadian internet registration authority': ('', -1000),\n",
       " 'crown records (1930s label)': ('', -1000),\n",
       " 'best coding practices': ('', -1000),\n",
       " 'settings of world of darkness': ('', -1000),\n",
       " 'category of topological spaces': ('', -1000),\n",
       " 'orodara airport': ('', -1000),\n",
       " 'resource development international': ('', -1000),\n",
       " 'hero of artsakh': ('', -1000),\n",
       " 'u-dise': ('', -1000),\n",
       " 'earl of leinster': ('', -1000),\n",
       " \"hawaiian sugar planters' association\": ('', -1000),\n",
       " 'frank wills (security guard)': ('', -1000),\n",
       " 'quasi-zenith satellite system': ('', -1000),\n",
       " 'acer betouch': ('', -1000),\n",
       " \"o'neal airport\": ('', -1000),\n",
       " 'waterford charter roll': ('', -1000),\n",
       " 'acer betouch e110': ('', -1000),\n",
       " 'beta-lactam ring records': ('', -1000),\n",
       " 'paradox of toil': ('', -1000),\n",
       " 'all around the world productions': ('', -1000),\n",
       " 'versor (physics)': ('', -1000),\n",
       " 'g. mcmurtrie godley': ('', -1000),\n",
       " 'pugachev airport': ('', -1000),\n",
       " 'sparcstation 10': ('', -1000),\n",
       " 'mount pleasant regional airport (south carolina)': ('', -1000),\n",
       " 'darkstalkers: the night warriors': ('', -1000),\n",
       " 'crypto phone': ('', -1000),\n",
       " 'manitoba labour representation committee': ('', -1000),\n",
       " 'assem jaber': ('', -1000),\n",
       " 'the third twin (duo)': ('', -1000),\n",
       " 'calico (consortium)': ('', -1000),\n",
       " 'wardlaw baronets': ('', -1000),\n",
       " 'hunter-blair baronets': ('', -1000),\n",
       " 'international cancer genome consortium': ('', -1000),\n",
       " 'john awdely': ('', -1000),\n",
       " 'georgia wing civil air patrol': ('', -1000),\n",
       " 'firmex': ('creditmantri', 0.80683553),\n",
       " 'denise moore': ('', -1000),\n",
       " 'ali ceesay': ('', -1000),\n",
       " 'kl-51': ('', -1000),\n",
       " 'trillian (software)': ('', -1000),\n",
       " 'yunnan sudden death syndrome': ('', -1000),\n",
       " 'just jj world tour': ('', -1000),\n",
       " 'holy see representative': ('', -1000),\n",
       " 'eads harfang': ('', -1000),\n",
       " 'hwangju airport': ('', -1000),\n",
       " 'coates baronets': ('', -1000),\n",
       " 'parral villa baviera airport': ('', -1000),\n",
       " 'mantidactylus bellyi': ('', -1000),\n",
       " 'rejecting jane': ('', -1000),\n",
       " 'acer betouch e120': ('', -1000),\n",
       " 'canaima (operating system)': ('', -1000),\n",
       " 'romina bell': ('', -1000),\n",
       " 'national alliance of families': ('', -1000),\n",
       " 'numerical sight-singing': ('', -1000),\n",
       " 'charles wingfield': ('', -1000),\n",
       " 'villeneuve les vertus aerodrome': ('', -1000),\n",
       " 'union list of artist names': ('', -1000),\n",
       " 'feature (machine learning)': ('', -1000),\n",
       " 'defy thirst': ('', -1000),\n",
       " \"heaven's memo pad\": ('', -1000),\n",
       " 'cinema 16: british short films': ('', -1000),\n",
       " 'veterans of future wars': ('', -1000),\n",
       " 'dell inspiron 1764': ('', -1000),\n",
       " 'arie kouandjio': ('', -1000),\n",
       " 'danish society for nature conservation': ('', -1000),\n",
       " 'voivodeship road': ('', -1000),\n",
       " 'paris elia': ('', -1000),\n",
       " 'phylomedb': ('locuslink', 0.86812055),\n",
       " 'bokondini airport': ('', -1000),\n",
       " 'ternary search tree': ('', -1000),\n",
       " 'shelby county republican party (tennessee)': ('', -1000),\n",
       " 'ontario march of dimes': ('', -1000),\n",
       " 'living earth simulator project': ('', -1000),\n",
       " 'battle of hanna': ('', -1000),\n",
       " 'list of films: f': ('', -1000),\n",
       " 'boma airstrip': ('', -1000),\n",
       " 'purple ribbon records': ('', -1000),\n",
       " 'david a. wagner': ('', -1000),\n",
       " 'row heavyweight championship': ('', -1000),\n",
       " 'confederation of chilean students': ('', -1000),\n",
       " 'xiao sijin': ('', -1000),\n",
       " 'nuangels': ('skai', 0.74676156),\n",
       " 'james fitz-morris': ('', -1000),\n",
       " 'perfect match (1994 game show)': ('', -1000),\n",
       " 'acer betouch e130': ('', -1000),\n",
       " 'bureau of alcohol, tobacco, firearms and explosives': ('', -1000),\n",
       " 'isa 500 audit evidence': ('', -1000),\n",
       " 'vanity award': ('', -1000),\n",
       " 'miss kurdistan': ('', -1000),\n",
       " 'world flute society': ('', -1000),\n",
       " 'certified radio operator': ('', -1000),\n",
       " 'saifee villa': ('', -1000),\n",
       " 'saskatoon john g. diefenbaker international airport': ('', -1000),\n",
       " 'gerris (software)': ('', -1000),\n",
       " 'dj luck & mc neat': ('', -1000),\n",
       " 'dadabik': ('yadifa', 0.8124941),\n",
       " 'zacharias charalambous': ('', -1000),\n",
       " 'm-labs': ('milkymist', 0.78114796),\n",
       " 'hylarana gracilis': ('', -1000),\n",
       " 'center for the study of the presidency and congress': ('', -1000),\n",
       " 'acer betouch e140': ('', -1000),\n",
       " 'california chaparral institute': ('', -1000),\n",
       " 'fawzi al-mulki': ('', -1000),\n",
       " 'independent order of vikings': ('', -1000),\n",
       " 'enzo osella': ('', -1000),\n",
       " 'port (medical)': ('', -1000),\n",
       " '128 records': ('', -1000),\n",
       " 'tripoli airport (greece)': ('', -1000),\n",
       " 'scapular anastomosis': ('', -1000),\n",
       " 'gangrene (group)': ('', -1000),\n",
       " 'qarn alam airport': ('', -1000),\n",
       " 'frank riley (author)': ('', -1000),\n",
       " 'buumi': ('kaymor', 0.8000274),\n",
       " 'stash (software)': ('', -1000),\n",
       " 'rayat ash-shaghilah': ('', -1000),\n",
       " \"bulmer's tree frog\": ('', -1000),\n",
       " 'ed kahn': ('', -1000),\n",
       " 'xie hangsheng': ('', -1000),\n",
       " 'catherine colonna': ('', -1000),\n",
       " 'filaret association': ('', -1000),\n",
       " 'comparison of disk encryption software': ('', -1000),\n",
       " 'master of business and management': ('', -1000),\n",
       " \"qt's diary\": ('', -1000),\n",
       " 'witt and berg': ('', -1000),\n",
       " \"smith's wrinkled frog\": ('', -1000),\n",
       " 'mantidactylus massorum': ('', -1000),\n",
       " 'mariăˇn ĺ\\xa0trbăˇk': ('', -1000),\n",
       " 'great-west lifeco': ('', -1000),\n",
       " 'wisconsin valley library service': ('', -1000),\n",
       " 'massachusetts general hospital academy': ('', -1000),\n",
       " 'from black power to hip hop': ('', -1000),\n",
       " 'acer betouch e200': ('', -1000),\n",
       " 'national academy of popular music': ('', -1000),\n",
       " 'co-aid': ('belpada', 0.73306406),\n",
       " '1000memories': ('cotweet', 0.7724868),\n",
       " 'romanovsky and phillips': ('', -1000),\n",
       " 'defence intelligence (company)': ('', -1000),\n",
       " 'ixiamas airport': ('', -1000),\n",
       " 'henry howard (diplomat)': ('', -1000),\n",
       " 'sundy best': ('', -1000),\n",
       " 'zac kerin': ('', -1000),\n",
       " 'risc-v': ('load', 0.70087427),\n",
       " 'music recording sales certification': ('', -1000),\n",
       " 'trolleybuses in arnhem': ('', -1000),\n",
       " 'jewish people policy institute': ('', -1000),\n",
       " 'ak 47 mayanja': ('', -1000),\n",
       " 'dick mccabe (american football)': ('', -1000),\n",
       " 'yervand sukiasyan': ('', -1000),\n",
       " 'oryol yuzhny airport': ('', -1000),\n",
       " 'aubenas aerodrome': ('', -1000),\n",
       " 'intelligent home control': ('', -1000),\n",
       " 'ncr 315': ('', -1000),\n",
       " 'acer betouch e400': ('', -1000),\n",
       " 'gaetano alibrandi': ('', -1000),\n",
       " 'stockholm memorandum': ('', -1000),\n",
       " 'flatbed digital printer': ('', -1000),\n",
       " 'esplen baronets': ('', -1000),\n",
       " 'bis hallmark': ('', -1000),\n",
       " \"rueppel's big-eyed tree frog\": ('', -1000),\n",
       " 'yer-sub': ('', -1000),\n",
       " 'bulb keel': ('', -1000),\n",
       " 'imperial seal of the mongols': ('', -1000),\n",
       " 'acer cloudmobile s500': ('', -1000),\n",
       " 'firebase argonne': ('', -1000),\n",
       " 'punkrock.net': ('', -1000),\n",
       " 'johannes klein': ('', -1000),\n",
       " 'deral boykin': ('', -1000),\n",
       " 'advanced rocket research center': ('', -1000),\n",
       " 'park găľell': ('', -1000),\n",
       " 'baton broadcast system': ('', -1000),\n",
       " 'acer liquid a1': ('', -1000),\n",
       " 'longman baronets': ('', -1000),\n",
       " 'dagmar urbancovăˇ': ('', -1000),\n",
       " 'matt slauson': ('', -1000),\n",
       " 'united states railroad administration': ('', -1000),\n",
       " 'sou hrostao akoma ena klama': ('', -1000),\n",
       " 'trouble on the corner': ('', -1000),\n",
       " 'irish feudal barony': ('', -1000),\n",
       " 'smartguy': ('windian', 0.69988096),\n",
       " 'acer liquid e': ('', -1000),\n",
       " 'groups of traditional buildings': ('', -1000),\n",
       " 'beech hill preserve': ('', -1000),\n",
       " 'philadelphia center for architecture': ('', -1000),\n",
       " 'marcos alberto skavinski': ('', -1000),\n",
       " 'scharfăľhrer': ('', -1000),\n",
       " 'd. j. morrell': ('', -1000),\n",
       " 'list of television stations in france': ('', -1000),\n",
       " \"child's war\": ('', -1000),\n",
       " 'jim martin (american football)': ('', -1000),\n",
       " 'natas (computer virus)': ('', -1000),\n",
       " 'clayton j. lloyd international airport': ('', -1000),\n",
       " 'counterspy (software)': ('', -1000),\n",
       " 'ohlson baronets': ('', -1000),\n",
       " 'start-up': ('', -1000),\n",
       " 'hwa television championship': ('', -1000),\n",
       " 'projeqtor': ('', -1000),\n",
       " 'caper (organization)': ('', -1000),\n",
       " 'united states patent classification': ('', -1000),\n",
       " 'pierre gauderman': ('', -1000),\n",
       " 'compton airport (oregon)': ('', -1000),\n",
       " 'high iq society': ('', -1000),\n",
       " 'asia-pacific network information centre': ('', -1000),\n",
       " 'bruised tongue': ('', -1000),\n",
       " 'engelver herrera': ('', -1000),\n",
       " 'salvage for the saint': ('', -1000),\n",
       " 'league of saint george': ('', -1000),\n",
       " 'new england tree frog': ('', -1000),\n",
       " 'oxydactyla alpestris': ('', -1000),\n",
       " \"dollo's law of irreversibility\": ('', -1000),\n",
       " 'victoria mară': ('', -1000),\n",
       " 'triple crown of brazilian football': ('', -1000),\n",
       " 'belrain aerodrome': ('', -1000),\n",
       " 'oreophryne flava': ('', -1000),\n",
       " '2009 north american christmas blizzard': ('', -1000),\n",
       " 'ovalle airport': ('', -1000),\n",
       " 'wonderboy records': ('', -1000),\n",
       " 'check-mate system': ('', -1000),\n",
       " 'gaggal airport': ('', -1000),\n",
       " 'board of intermediate and secondary education, barisal': ('', -1000),\n",
       " 'parody religion': ('', -1000),\n",
       " 'north shore (oahu)': ('', -1000),\n",
       " 'internet explorer 5': ('', -1000),\n",
       " '4frontsecurity': ('', -1000),\n",
       " 'direcciăłn general del sistema penitenciario de guatemala': ('', -1000),\n",
       " 'volume 2: release': ('', -1000),\n",
       " 'international time capsule society': ('', -1000),\n",
       " 'acer liquid z5': ('', -1000),\n",
       " 'manila municipal airport': ('', -1000),\n",
       " 'yang xiao (scientist)': ('', -1000),\n",
       " 'three colors trilogy': ('', -1000),\n",
       " 'mathematics genealogy project': ('', -1000),\n",
       " 'world taxation system': ('', -1000),\n",
       " 'rwandapaparazzi': ('', -1000),\n",
       " 'tasklist': ('htop', 0.8509525),\n",
       " 'load records discography': ('', -1000),\n",
       " 'acer liquid z630': ('', -1000),\n",
       " 'acer n311': ('', -1000),\n",
       " 'afo records': ('', -1000),\n",
       " 'judge dredd (film)': ('', -1000),\n",
       " 'cipurse': ('jcop', 0.7651534),\n",
       " 'claude boucher (diplomat)': ('', -1000),\n",
       " 'thinkpad tablet 2': ('', -1000),\n",
       " 'nä': ('muudu', 0.68683255),\n",
       " 'cholet le pontreau airport': ('', -1000),\n",
       " 'hans colliander': ('', -1000),\n",
       " 'tennessee justice center': ('', -1000),\n",
       " 'acer n50': ('', -1000),\n",
       " 'acer neotouch': ('', -1000),\n",
       " 'duke of albret': ('', -1000),\n",
       " 'twisp municipal airport': ('', -1000),\n",
       " 'pah world hypothesis': ('', -1000),\n",
       " 'filgoal': ('dowidar', 0.79180306),\n",
       " 'g.i. joe: the atlantis factor': ('', -1000),\n",
       " 'michel follet': ('', -1000),\n",
       " 'a.j. eisenberg airport': ('', -1000),\n",
       " \"workers' militia pps-wrn\": ('', -1000),\n",
       " 'commercial mortgage-backed security': ('', -1000),\n",
       " 'edition records': ('', -1000),\n",
       " 'mikhail borisov': ('', -1000),\n",
       " 'acer neotouch p300': ('', -1000),\n",
       " 'senya fault': ('', -1000),\n",
       " 'ataq airport': ('', -1000),\n",
       " 'george gauer': ('', -1000),\n",
       " 'lev urusov': ('', -1000),\n",
       " 'special communications service of russia': ('', -1000),\n",
       " 'andrey blyndu': ('', -1000),\n",
       " \"chain o'lakes\": ('', -1000),\n",
       " 'board of intermediate and secondary education, comilla': ('', -1000),\n",
       " 'palca municipality': ('', -1000),\n",
       " 'star wars (film)': ('', -1000),\n",
       " 'acer neotouch p400': ('', -1000),\n",
       " 'big4books': ('', -1000),\n",
       " 'isle of man passport': ('', -1000),\n",
       " 'airnav systems radarbox': ('', -1000),\n",
       " 'qualis (capes)': ('', -1000),\n",
       " 'jack keenan (american football)': ('', -1000),\n",
       " 'al-qaeda in sinai peninsula': ('', -1000),\n",
       " 'megan brigman': ('', -1000),\n",
       " 'heart records and tapes of canada ltd.': ('', -1000),\n",
       " 'jăˇn maslo': ('', -1000),\n",
       " 'acer neotouch s200': ('', -1000),\n",
       " 'peter l. gluck': ('', -1000),\n",
       " 'back-story (production)': ('', -1000),\n",
       " 'national committee of americans of polish extraction': ('', -1000),\n",
       " 'warded lock': ('', -1000),\n",
       " 'vukovar resolution': ('', -1000),\n",
       " '1909 cherry mine disaster': ('', -1000),\n",
       " 'acer stream': ('', -1000),\n",
       " 'digital self-defense': ('', -1000),\n",
       " 'long river (guangxi)': ('', -1000),\n",
       " 'benny perrin': ('', -1000),\n",
       " 'giorgio giacomelli': ('', -1000),\n",
       " 'multi-purpose viewer': ('', -1000),\n",
       " 'harun thohir airport': ('', -1000),\n",
       " 'the bonfire of the vanities (film)': ('', -1000),\n",
       " 'hans palmquist': ('', -1000),\n",
       " 'milton levy': ('', -1000),\n",
       " 'ill flava records': ('', -1000),\n",
       " 'obscenity prosecution task force': ('', -1000),\n",
       " 'grey bamboo shark': ('', -1000),\n",
       " 'viron p. vaky': ('', -1000),\n",
       " 'ezourvedam': ('vishnupurana', 0.73064405),\n",
       " 'miracle cars scam': ('', -1000),\n",
       " 'atacalco sul airport': ('', -1000),\n",
       " 'mark leishman': ('', -1000),\n",
       " 'al-sadr online': ('', -1000),\n",
       " 'vision mobile browser': ('', -1000),\n",
       " 'charles & eddie': ('', -1000),\n",
       " 'albuquerque youth symphony': ('', -1000),\n",
       " 'artificial sun': ('', -1000),\n",
       " 'inverted river delta': ('', -1000),\n",
       " 'agua hedionda spa': ('', -1000),\n",
       " 'jmy records': ('', -1000),\n",
       " 'holocartoons': ('', -1000),\n",
       " 'tim vogler': ('', -1000),\n",
       " 'fieber (christina stăľrmer song)': ('', -1000),\n",
       " 'robson & jerome': ('', -1000),\n",
       " 'okushiri airport': ('', -1000),\n",
       " 'electronic distributed monitoring and evaluation solution': ('', -1000),\n",
       " 'bundoo': ('momversation', 0.7174192),\n",
       " 'sorce intranet': ('', -1000),\n",
       " 'zdravko iliev': ('', -1000),\n",
       " 'deputy făľhrer': ('', -1000),\n",
       " 'erich geyer': ('', -1000),\n",
       " 'dominique awono essama': ('', -1000),\n",
       " 'eleftherios mertakas': ('', -1000),\n",
       " 'karlsruhe stadtbahn': ('', -1000),\n",
       " 'hendrick vaal neto': ('', -1000),\n",
       " 'lincoln house (mumbai)': ('', -1000),\n",
       " 'mul.apin': ('lesath', 0.7041819),\n",
       " 'uk emissions trading scheme': ('', -1000),\n",
       " 'learning plan': ('', -1000),\n",
       " 'brian washington': ('', -1000),\n",
       " 'ltr standard and passport': ('', -1000),\n",
       " 'down on sunset': ('', -1000),\n",
       " 'sono luminus': ('', -1000),\n",
       " 'pancasila economics': ('', -1000),\n",
       " 'titans radio network': ('', -1000),\n",
       " 'george washington bridge plaza': ('', -1000),\n",
       " 'canby municipal airport': ('', -1000),\n",
       " 'voyi': ('', -1000),\n",
       " \"bangui m'poko international airport\": ('', -1000),\n",
       " 'mister philippines 2012': ('', -1000),\n",
       " 'mantidactylus leucocephalus': ('', -1000),\n",
       " 'corrigan baronets': ('', -1000),\n",
       " 'acid (computer virus)': ('', -1000),\n",
       " 'motorola type ii smartzone omnilink': ('', -1000),\n",
       " \"the women's conference\": ('', -1000),\n",
       " 'aurora municipal airport (illinois)': ('', -1000),\n",
       " 'nomad (band)': ('', -1000),\n",
       " 'vladislav ovsyannikov': ('', -1000),\n",
       " 'greg jeffries': ('', -1000),\n",
       " 'european genetics foundation': ('', -1000),\n",
       " 'james waldegrave, 1st earl waldegrave': ('', -1000),\n",
       " 'samuel b. thomsen': ('', -1000),\n",
       " 'simon hayes (police commissioner)': ('', -1000),\n",
       " 'nigel haywood': ('', -1000),\n",
       " 'david muirhead': ('', -1000),\n",
       " \"the world's most beautiful transsexual contest\": ('', -1000),\n",
       " 'deutscher băľhnenverein': ('', -1000),\n",
       " 'f. p. martin house': ('', -1000),\n",
       " 'nwa georgia heavyweight championship': ('', -1000),\n",
       " 'robin and linda williams': ('', -1000),\n",
       " 'fusenet': ('icmeg', 0.7880921),\n",
       " 'gamerfitnation': ('blogcast', 0.6616436),\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
