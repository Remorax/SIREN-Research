{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, pickle, os, sys, pickledb, time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from copy import deepcopy\n",
    "\n",
    "train_file = \"/data/Vivek/original/HypeNET/dataset/custom_train_0.0_0.05.tsv\"\n",
    "test_file =  \"/data/Vivek/original/HypeNET/dataset/custom_test_0.0_0.05.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat/\"\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "use_embeddings = \"../files/embeddings.pt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_VECTOR = np.random.rand(300)\n",
    "# NULL_PATH = ((tuple(NULL_VECTOR), 0, 0, 0),)\n",
    "NULL_PATH = ((0, 0, 0, 0), )\n",
    "# relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/db_files/\"\n",
    "op_file = \"../junk/glove_vanilla.pkl\"\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format(\"/data/Vivek/glove_tmp\")\n",
    "\n",
    "# wiki2vec = KeyedVectors.load_word2vec_format(\"/home/vlead/enwiki_20180420_win10_300d.txt\")\n",
    "\n",
    "# og_dict = deepcopy(wiki2vec.wv.vocab)\n",
    "# for k in og_dict:\n",
    "#     if \"/\" in k:\n",
    "#         wiki2vec.wv.vocab[k.split(\"/\")[1].lower()] = wiki2vec.wv.vocab[k]\n",
    "#         del wiki2vec.wv.vocab[k]\n",
    "# del og_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.88\n",
    "failed, success = [], []\n",
    "# embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db.get(str(entity_id))\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db.get(str(entity_id))\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, faile\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db.get(key)\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def return_sim(model, word):\n",
    "    try:\n",
    "        return tuple(model[\"_\".join(word.split())])\n",
    "    except:\n",
    "        return tuple(NULL_VECTOR)\n",
    "# def load_embeddings_from_disk():\n",
    "#     try:\n",
    "#         vectors = bcolz.open(embeddings_folder)[:]\n",
    "#         word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "        \n",
    "#         vocab = set([a for a in success + dataset_keys if a])\n",
    "#         word2idx_lite = {\"_unk_\": 0}\n",
    "#         embeddings_lite = bcolz.carray(np.random.uniform(-1, 1, (1, 300)), rootdir=embeddings_folder, mode='w')\n",
    "        \n",
    "#         for word in word2idx:\n",
    "#             if word not in vocab:\n",
    "#                 continue\n",
    "#             embeddings_lite.append(embeddings[word2idx[word]])\n",
    "#             word2idx_lite[word] = idx\n",
    "#             idx += 1\n",
    "            \n",
    "#         embeddings = vectors\n",
    "#     except:\n",
    "#         embeddings_lite, word2idx_lite = create_embeddings()\n",
    "#     return embeddings_lite, word2idx_lite\n",
    "\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        embeddings = bcolz.open(embeddings_folder)[:]\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "    except:\n",
    "        embeddings, word2idx = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "\n",
    "\n",
    "def create_embeddings():\n",
    "#     vocab = set([a for a in success + dataset_keys if a])\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.uniform(-1, 1, (1, 300)), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = [a[::-1] for a in l[::-1].split(\" \", 300)[::-1]]\n",
    "            word, vector = line[0], [float(s) for s in line[1:]]\n",
    "            vectors.append(np.resize(np.array(vector), (1, 300)).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "\n",
    "    return vectors, word2idx\n",
    "\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = tuple(embeddings[emb_indexer.get(\"_\".join(embedding.split()),0)]), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = tup\n",
    "    paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "    x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "    y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "    path_count_dict = { id_to_path(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "#     paths_xy = list(extract_paths(relations_db,x,y).items())\n",
    "#     paths_yx = list(extract_paths(relations_db,y,x).items())\n",
    "#     path_count_dict = { id_to_path(id2path_db, path) : freq for (path, freq) in paths_xy }\n",
    "#     path_count_dict.update({ id_to_path(id2path_db, path).replace(\"X/\", '@@@').replace('Y/', 'X/').replace('@@@', 'Y/') : freq for (path, freq) in paths_yx })\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    print (\"Parsing dataset for \", prefix)\n",
    "    \n",
    "    dataset_ids = [(entity_to_id(word2id_db, tup[0]), entity_to_id(word2id_db, tup[1])) for tup in dataset]\n",
    "    \n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset_ids]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    empty = [list(dataset_ids)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "#     embed_indices = [(return_sim(x), return_sim(y)) for (x,y) in dataset]\n",
    "    embed_indices = []\n",
    "    embed_indices = [(emb_indexer.get(\"_\".join(x.split()),0), emb_indexer.get(\"_\".join(y.split()),0)) for (x,y) in dataset]\n",
    "    for (x,y) in dataset:\n",
    "        try:\n",
    "            embed_indices.append((tuple(embeddings[emb_indexer.get(\"_\".join(x.split()),0)]), tuple(embeddings[emb_indexer.get(\"_\".join(y.split()),0)])))\n",
    "        except:\n",
    "            print (x,y)\n",
    "    return embed_indices, paths\n",
    "\n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys()) + list(test_instances.keys()) + list(test_knocked.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values()) + list(test_instances.values()) + list(test_knocked.values())\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "mappingDict = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "f.close()\n",
    "\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'nils_melzer'\n",
    "emb_indexer.get(\"_\".join(x.split()),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../junk/glove_input.pkl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "e = nn.Embedding(3, 3)\n",
    "ls = [[0, 1, 2], [3,4,5], [6,7,8]]\n",
    "ls = np.array([np.array(el) for el in ls])\n",
    "e.load_state_dict({'weight': torch.from_numpy(ls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trust no one (internet security)', 'approach'),\n",
       " ('human computer interaction (security)', 'study'),\n",
       " ('igor muttik', 'expert'),\n",
       " ('organization', '(isc)²'),\n",
       " ('hacker', '0x80'),\n",
       " ('virus', '1260 (computer virus)'),\n",
       " ('emulator', '1964 (emulator)'),\n",
       " ('undefined behavior', 'result'),\n",
       " ('skipjack (cipher)', 'algorithm'),\n",
       " ('arp spoofing', 'technique'),\n",
       " ('policy', 'system'),\n",
       " ('xscreensaver', 'collection'),\n",
       " ('beyondtrust', 'company'),\n",
       " ('asset', 'resource'),\n",
       " ('interbase', 'system'),\n",
       " ('sparc', 'instruction'),\n",
       " ('list of government mass surveillance projects', 'list'),\n",
       " ('ibm system z10', 'line'),\n",
       " ('dialer', '1legcall'),\n",
       " ('umts', '3 skypephone'),\n",
       " ('umts', '3 skypephone s2'),\n",
       " ('domainkeys identified mail', 'method'),\n",
       " ('certificate-based encryption', 'system'),\n",
       " ('server', '389 directory server'),\n",
       " ('security policy', 'definition'),\n",
       " ('software', '3cx phone system'),\n",
       " ('malware', '3wplayer'),\n",
       " ('ccie certification', 'certification'),\n",
       " ('virus', '4k (computer virus)'),\n",
       " ('real time streaming protocol', 'protocol'),\n",
       " ('ron rivest', 'cryptographer'),\n",
       " ('salvatore d. morgera', 'engineer'),\n",
       " ('property', 'completeness (cryptography)'),\n",
       " ('polymorphic engine', 'program'),\n",
       " ('haystack (software)', 'program'),\n",
       " ('anthony zboralski', 'hacker'),\n",
       " ('computer task group', 'company'),\n",
       " ('email fraud', 'deception'),\n",
       " ('rustock botnet', 'botnet'),\n",
       " ('oblivious data structure', 'structure'),\n",
       " ('major cities of europe it users group', 'association'),\n",
       " ('network access protection', 'technology'),\n",
       " ('deception', '9th airborne division (united states)'),\n",
       " ('compiler', 'a-0 system'),\n",
       " ('countermeasure (computer)', 'action'),\n",
       " ('upering', 'worm'),\n",
       " ('internet explorer mobile', 'browser'),\n",
       " ('three-stage quantum cryptography protocol', 'method'),\n",
       " ('ms-chap', 'version'),\n",
       " ('detection', 'aaq-26'),\n",
       " ('linux console', 'system'),\n",
       " ('hacker', 'abbassa malik'),\n",
       " ('virus', 'abc (computer virus)'),\n",
       " ('authorization certificate', 'document'),\n",
       " ('digital audio access protocol', 'protocol'),\n",
       " ('national vulnerability database', 'repository'),\n",
       " ('linux-vserver', 'implementation'),\n",
       " ('php', 'language'),\n",
       " ('opie authentication system', 'initialism'),\n",
       " ('icechat', 'client'),\n",
       " ('ctx (computer virus)', 'virus'),\n",
       " ('plone (software)', 'system'),\n",
       " ('software', 'abc (yet another bittorrent client)'),\n",
       " ('anna kournikova (computer virus)', 'worm'),\n",
       " ('akelarre (cipher)', 'cipher'),\n",
       " ('qualified security assessor', 'designation'),\n",
       " ('server', 'abmaps'),\n",
       " ('compiler', 'acc (programming language)'),\n",
       " ('multi media interface', 'system'),\n",
       " ('software', 'aceproject'),\n",
       " ('mobile device forensics', 'branch'),\n",
       " ('dangling pointer', 'pointers'),\n",
       " ('smartphone', 'acer allegro'),\n",
       " ('canadian internet registration authority', 'organization'),\n",
       " ('qemu', 'hypervisor'),\n",
       " ('smartphone', 'acer betouch'),\n",
       " ('smartphone', 'acer betouch e110'),\n",
       " ('crypto phone', 'telephones'),\n",
       " ('trillian (software)', 'application'),\n",
       " ('cryptosystem', 'convergent encryption'),\n",
       " ('smartphone', 'acer betouch e120'),\n",
       " ('cdnetworks', 'network'),\n",
       " ('gisbert hasenjaeger', 'logician'),\n",
       " ('smartphone', 'acer betouch e130'),\n",
       " ('smartphone', 'acer betouch e140'),\n",
       " ('adi shamir', 'cryptographer'),\n",
       " ('smartphone', 'acer betouch e200'),\n",
       " ('total intelligence solutions', 'management'),\n",
       " ('adaptive server enterprise', 'product'),\n",
       " ('risc-v', 'instruction'),\n",
       " ('johnny long', 'expert'),\n",
       " ('network function virtualization', 'concept'),\n",
       " ('smartphone', 'acer betouch e400'),\n",
       " ('geforce', 'brand'),\n",
       " ('smartphone', 'acer cloudmobile s500'),\n",
       " ('smartphone', 'acer liquid a1'),\n",
       " ('internet', 'system'),\n",
       " ('smartphone', 'acer liquid e'),\n",
       " ('telegraphy', 'transmission'),\n",
       " ('natas (computer virus)', 'virus'),\n",
       " ('personal antivirus', 'software'),\n",
       " ('avenda systems', 'start-up'),\n",
       " ('asia-pacific network information centre', 'registry'),\n",
       " ('computercop', 'software'),\n",
       " ('4frontsecurity', 'provider'),\n",
       " ('package manager', 'collection'),\n",
       " ('smartphone', 'acer liquid z5'),\n",
       " ('smartphone', 'acer liquid z630'),\n",
       " ('pda', 'acer n311'),\n",
       " ('hotline', 'communications'),\n",
       " ('pda', 'acer n50'),\n",
       " ('smartphone', 'acer neotouch'),\n",
       " ('smartphone', 'acer neotouch p300'),\n",
       " ('smartphone', 'acer neotouch p400'),\n",
       " ('security awareness', 'knowledge'),\n",
       " ('identity theft', 'use'),\n",
       " ('smartphone', 'acer neotouch s200'),\n",
       " ('industrial espionage', 'form'),\n",
       " ('application firewall', 'form'),\n",
       " ('video banking', 'term'),\n",
       " ('turing robotic industries', 'company'),\n",
       " ('smartphone', 'acer stream'),\n",
       " ('cipher', 'achterbahn'),\n",
       " ('vision mobile browser', 'platform'),\n",
       " ('linked list', 'collection'),\n",
       " ('failed state', 'body'),\n",
       " ('database activity monitoring', 'technology'),\n",
       " ('virus', 'acid (computer virus)'),\n",
       " ('software', 'acid cryptofiler'),\n",
       " ('extended smtp', 'definition'),\n",
       " ('mumu (computer worm)', 'worm'),\n",
       " ('virus', 'acme (computer virus)'),\n",
       " ('remote desktop services', 'components'),\n",
       " ('google drive', 'storage'),\n",
       " ('code audit', 'analysis'),\n",
       " ('computer', 'acorn network computer'),\n",
       " ('sim application toolkit', 'standard'),\n",
       " ('network virtualization', 'process'),\n",
       " ('computer and network surveillance', 'monitoring'),\n",
       " ('seed', 'cipher'),\n",
       " ('rickrolling', 'prank'),\n",
       " ('nanoc', 'compiler'),\n",
       " ('message', 'across the universe (message)'),\n",
       " ('message', 'action alert'),\n",
       " ('software', 'actionstep'),\n",
       " ('firewall', 'activearmor'),\n",
       " ('darpa', 'actuv'),\n",
       " ('mynotex', 'software'),\n",
       " ('virus', 'ada (computer virus)'),\n",
       " ('software', 'demultiplexer (media file)'),\n",
       " ('hacker', 'adam botbyl'),\n",
       " ('fraud', 'address fraud'),\n",
       " ('nils melzer', 'expert'),\n",
       " ('software', 'adms'),\n",
       " ('weightless (wireless communications)', 'set'),\n",
       " ('workstation', 'adobe audition'),\n",
       " ('software', 'adobe connect'),\n",
       " ('workgroup (computer networking)', 'term'),\n",
       " ('basic access authentication', 'method'),\n",
       " ('software', 'adobe creative cloud'),\n",
       " ('eucalyptus (software)', 'software'),\n",
       " ('national collegiate cyber defense competition', 'event'),\n",
       " ('server', 'adobe jrun'),\n",
       " ('data loss', 'condition'),\n",
       " ('software', 'adobe livecycle reader extensions'),\n",
       " ('emulator', 'adripsx'),\n",
       " ('computer', 'advanced maryland automatic network disk archiver'),\n",
       " ('common computing security standards forum', 'organization'),\n",
       " ('internationalization and localization', 'means'),\n",
       " ('process control network', 'network'),\n",
       " ('fraud', 'advance-fee scam'),\n",
       " ('software', 'adventure game toolkit'),\n",
       " ('outsourcing', 'aegis (company)'),\n",
       " ('emulator', 'aemulor'),\n",
       " ('emulator', 'aeolus (organ simulator)'),\n",
       " ('cassidy wolf', 'model'),\n",
       " ('michigan terminal system', 'systems'),\n",
       " ('software', 'aeolus (software)'),\n",
       " ('document', 'aes-2id'),\n",
       " ('cipher', 'affine cipher'),\n",
       " ('ayasdi', 'company'),\n",
       " ('rootkit', 'afx windows rootkit 2003'),\n",
       " ('festi botnet', 'botnet'),\n",
       " ('virus', 'agena (computer virus)'),\n",
       " ('khalil sehnaoui', 'consultant'),\n",
       " ('platform as a service', 'category'),\n",
       " ('dos', 'agi-plan'),\n",
       " ('virustotal', 'website'),\n",
       " ('backdoor.win32.ircbot', 'worm'),\n",
       " ('software', 'ahnenblatt'),\n",
       " ('data-centric security', 'approach'),\n",
       " ('software', 'ahpl'),\n",
       " ('threefish', 'cipher'),\n",
       " ('asp.net mvc', 'framework'),\n",
       " ('virus', 'ai (computer virus)'),\n",
       " ('c2.lop', 'malware'),\n",
       " ('dolan (surname)', 'surname'),\n",
       " ('virus', 'aids (computer virus)'),\n",
       " ('panda security', 'company'),\n",
       " ('nhttpd', 'server'),\n",
       " ('software', 'aigaion'),\n",
       " ('malware', 'air gap malware'),\n",
       " ('netware', 'network'),\n",
       " ('server', 'airmarshal'),\n",
       " ('software', 'airnav systems radarbox'),\n",
       " ('facial recognition system', 'application'),\n",
       " ('international association for cryptologic research', 'organization'),\n",
       " ('o2 wireless box', 'router'),\n",
       " ('improper input validation', 'vulnerability'),\n",
       " ('nsakey', 'name'),\n",
       " ('gsm', 'airtel bangladesh'),\n",
       " ('certificate authority security council', 'group'),\n",
       " ('netscape', 'company'),\n",
       " ('limewire', 'file'),\n",
       " ('forgery', 'process'),\n",
       " ('software', 'airy (software)'),\n",
       " ('cipher', 'akelarre (cipher)'),\n",
       " ('factor', 'akna'),\n",
       " ('comparison of instant messaging protocols', 'comparison'),\n",
       " ('electron (software framework)', 'framework'),\n",
       " ('powwow365', 'web'),\n",
       " ('jupiter broadcasting', 'network'),\n",
       " ('software', 'akushaper'),\n",
       " ('information science', 'field'),\n",
       " ('software', \"al misbah qur'an learning software\"),\n",
       " ('virus', 'alabama (computer virus)'),\n",
       " ('security', 'alarmforce'),\n",
       " ('spybot worm', 'family'),\n",
       " ('hacker', 'albert gonzalez'),\n",
       " ('kerberos (protocol)', 'protocol'),\n",
       " ('defender', 'alberto soto'),\n",
       " ('smartphone', 'alcatel one touch 980'),\n",
       " ('smartphone', 'alcatel one touch 990'),\n",
       " ('digital signature', 'scheme'),\n",
       " ('virus', 'alcon (computer virus)'),\n",
       " ('server', 'alcs transaction monitor'),\n",
       " ('computer engineering', 'discipline'),\n",
       " ('software', 'aldus photostyler'),\n",
       " ('phone fraud', 'use'),\n",
       " ('radius', 'alexandra radius'),\n",
       " ('jessica mydek hoax letter', 'letter'),\n",
       " ('open source day', 'fans'),\n",
       " ('compiler', 'algol 68rs'),\n",
       " ('master of science in information assurance', 'degree'),\n",
       " ('methodology', 'algorithm engineering'),\n",
       " ('slide attack', 'form'),\n",
       " ('software-defined protection', 'architecture'),\n",
       " ('enhanced interior gateway routing protocol', 'protocol'),\n",
       " ('theoretical computer science', 'division'),\n",
       " ('960th cyberspace operations group', 'united'),\n",
       " ('software', 'all my movies'),\n",
       " ('badtrans', 'worm'),\n",
       " ('plaintext', 'information'),\n",
       " ('solange ghernaouti', 'professor'),\n",
       " ('organization', 'alliance of digital humanities organizations'),\n",
       " ('information', 'alpha data'),\n",
       " ('adobe browserlab', 'service'),\n",
       " ('typex', 'machines'),\n",
       " ('iso development environment', 'implementation'),\n",
       " ('shellcode', 'alphanumeric shellcode'),\n",
       " ('software', 'altera quartus'),\n",
       " ('firewall', 'amazingports'),\n",
       " ('formal methods', 'kind'),\n",
       " ('tactical communications', 'communications'),\n",
       " ('database', 'amazon simpledb'),\n",
       " ('festi', 'rootkit'),\n",
       " ('western digital', 'company'),\n",
       " ('ios 8', 'release'),\n",
       " ('workstation', 'ambivu 3d workstation'),\n",
       " ('red october (malware)', 'program'),\n",
       " ('networking hardware', 'receiver'),\n",
       " ('virus', 'ambulance (computer virus)'),\n",
       " ('neo (the matrix)', 'character'),\n",
       " ('organization', 'american cryptogram association'),\n",
       " ('stephen trimberger', 'scientist'),\n",
       " ('organization', 'american society for cybernetics'),\n",
       " ('iviz security', 'company'),\n",
       " ('server', 'amphetarate'),\n",
       " ('crimeware', 'malware'),\n",
       " ('vulnerability', 'analog hole'),\n",
       " ('call-recording hardware', 'hardware'),\n",
       " ('vpn blocking', 'technique'),\n",
       " ('biswanath mukherjee', 'professor'),\n",
       " ('software', 'andreamosaic'),\n",
       " ('one-key mac', 'code'),\n",
       " ('universal plug and play', 'set'),\n",
       " ('hacker', 'andrew huang'),\n",
       " ('smartphone', 'android auto'),\n",
       " ('system', 'anomaly-based intrusion detection system'),\n",
       " ('security', 'asset-backed security'),\n",
       " ('server', 'anonymous remailer'),\n",
       " ('advanced vector extensions', 'extensions'),\n",
       " ('dynamic random-access memory', 'memory'),\n",
       " ('software', 'ant software limited'),\n",
       " ('nasir memon', 'scientist'),\n",
       " ('hacker', 'anthony zboralski'),\n",
       " ('courage foundation', 'trust'),\n",
       " ('virus', 'anti (computer virus)'),\n",
       " ('salsa20', 'cipher'),\n",
       " ('software protection dongle', 'protection'),\n",
       " ('great lakes integrated sciences and assessments', 'collaboration'),\n",
       " ('devnull', 'worm'),\n",
       " ('hoax', 'antichrist (virus hoax)'),\n",
       " ('system', 'anti-hijack system'),\n",
       " ('software', 'anti-keylogger'),\n",
       " ('ibm websphere application server community edition', 'server'),\n",
       " ('hetzner', 'internet'),\n",
       " ('software', 'anti-tamper software'),\n",
       " ('kodi (software)', 'application'),\n",
       " ('traffic shaping', 'technique'),\n",
       " ('software', 'antivirus gold'),\n",
       " ('software', 'antivirus software'),\n",
       " ('software', 'antutu'),\n",
       " ('supersingular isogeny key exchange', 'algorithm'),\n",
       " ('network centric product support', 'architecture'),\n",
       " ('cipher', 'anubis (cipher)'),\n",
       " ('xml', 'apache axis'),\n",
       " ('system shock', 'game'),\n",
       " ('software', 'apache cloudstack'),\n",
       " ('browser security', 'application'),\n",
       " ('solid-state drive', 'device'),\n",
       " ('server', 'apache continuum'),\n",
       " ('local area network', 'network'),\n",
       " ('private.me', 'platform'),\n",
       " ('fully switched network', 'network'),\n",
       " ('server', 'apache geronimo'),\n",
       " ('cipher method', 'format'),\n",
       " ('ron erickson (investor)', 'executive'),\n",
       " ('software', 'apache subversion'),\n",
       " ('server', 'apache tomcat'),\n",
       " ('software', 'apfel (software)'),\n",
       " ('server', 'appaserver'),\n",
       " ('emulator', 'appleiigo'),\n",
       " ('efficient xml interchange', 'format'),\n",
       " ('frogbit (cipher)', 'algorithm'),\n",
       " ('emulator', 'applewin'),\n",
       " ('email', 'appriver'),\n",
       " ('net operation', 'operation'),\n",
       " ('rc2', 'cipher'),\n",
       " ('server', 'approx'),\n",
       " ('security', 'appthority'),\n",
       " ('software', 'apros (software)'),\n",
       " ('carlisle adams', 'cryptographer'),\n",
       " ('smartphone', 'aqua fish'),\n",
       " ('kaberi gayen', 'academic'),\n",
       " ('downgrade attack', 'form'),\n",
       " ('christopher lyon', 'researcher'),\n",
       " ('captcha', 'test'),\n",
       " ('seafile', 'file'),\n",
       " ('software', 'aquad'),\n",
       " ('warrior pride', 'gchq'),\n",
       " ('lamont boiler', 'type'),\n",
       " ('ibm secure blue', 'hardware'),\n",
       " ('koobface', 'worm'),\n",
       " ('smartphone', 'aquos phone sh-06d'),\n",
       " ('in-session phishing', 'form'),\n",
       " ('software', 'archibus'),\n",
       " ('server', 'arcims'),\n",
       " ('volatility (memory forensics)', 'framework'),\n",
       " ('software', 'arcplan'),\n",
       " ('supercomputer', 'arctur-1'),\n",
       " ('system integrity protection', 'feature'),\n",
       " ('digibox', 'device'),\n",
       " ('software', 'areca backup'),\n",
       " ('axigen', 'server'),\n",
       " ('online armor personal firewall', 'firewall'),\n",
       " ('secure terminal equipment', 'government'),\n",
       " ('security', 'arey (company)'),\n",
       " ('software', 'argus monitor'),\n",
       " ('client-side', 'application'),\n",
       " ('multicast', 'communication'),\n",
       " ('cipher', 'aria (cipher)'),\n",
       " ('foreground detection', 'tasks'),\n",
       " ('plausible deniability', 'ability'),\n",
       " ('international data encryption algorithm', 'cipher'),\n",
       " ('software', 'disk encryption software'),\n",
       " ('radio rivendell', 'station'),\n",
       " ('smartphone', 'arirang (smartphone)'),\n",
       " ('microsoft sql server', 'system'),\n",
       " ('information security forum', 'association'),\n",
       " ('outpost firewall pro', 'package'),\n",
       " ('software', 'arlequin'),\n",
       " ('simon (cipher)', 'family'),\n",
       " ('protection', 'armour (anatomy)'),\n",
       " ('.tk', 'domain'),\n",
       " ('moose file system', 'system'),\n",
       " ('cipher', 'arnold cipher'),\n",
       " ('pcmos', 'technology'),\n",
       " ('software', 'art academy (video game)'),\n",
       " ('dmoz', 'directory'),\n",
       " ('database', 'artcyclopedia'),\n",
       " ('pa server monitor', 'server'),\n",
       " ('hacker', 'arthur hacker'),\n",
       " ('file descriptor', 'indicator'),\n",
       " ('u.s. army information technology agency', 'organizations'),\n",
       " ('html', 'article element (html5)'),\n",
       " ('software', 'article video robot'),\n",
       " ('codec', 'asao (codec)'),\n",
       " ('supercomputer', 'asc purple'),\n",
       " ('supercomputer', 'asci blue mountain'),\n",
       " ('supercomputer', 'asci blue pacific'),\n",
       " ('core storage', 'system'),\n",
       " ('supercomputer', 'asci q'),\n",
       " ('nist sp 800-90a', 'publication'),\n",
       " ('distributed firewall', 'system'),\n",
       " ('ezproxy', 'server'),\n",
       " ('onion model', 'template'),\n",
       " ('public key certificate', 'document'),\n",
       " ('server-based signatures', 'signatures'),\n",
       " ('supercomputer', 'asci white'),\n",
       " ('threat', 'intent'),\n",
       " ('firmware', 'askoziapbx'),\n",
       " ('xmx', 'cipher'),\n",
       " ('cover (telecommunications)', 'technique'),\n",
       " ('cineasset', 'suite'),\n",
       " ('botnet', 'asprox botnet'),\n",
       " ('virtru', 'encryption'),\n",
       " ('protection', 'assuranceforeningen gard'),\n",
       " ('software', 'astra (software)'),\n",
       " ('software', 'asure software'),\n",
       " ('mujahedeen secrets', 'program'),\n",
       " ('web container', 'component'),\n",
       " ('detection', 'autodetection'),\n",
       " ('smartphone', 'asus padfone'),\n",
       " ('denial-of-service attack', 'attempt'),\n",
       " ('data', 'asynchronous communication mechanism'),\n",
       " ('firefox', 'browser'),\n",
       " ('ibm', 'at'),\n",
       " ('davix', 'cd'),\n",
       " ('khazad', 'cipher'),\n",
       " ('cipher', 'atbash'),\n",
       " ('list of web service specifications', 'variety'),\n",
       " ('chipless rfid', 'tags'),\n",
       " ('emulator', 'aterm'),\n",
       " ('rfb protocol', 'protocol'),\n",
       " ('intel', 'atmel at89 series'),\n",
       " ('team solomid', 'organization'),\n",
       " ('hacker', 'ăťmir vigfăşsson'),\n",
       " ('software', 'atomistix toolkit'),\n",
       " ('software', 'atomistix virtual nanolab'),\n",
       " ('delegation (computer security)', 'process'),\n",
       " ('security bug', 'bug'),\n",
       " ('routing', 'process'),\n",
       " ('software', 'audiomulch'),\n",
       " ('virtual security appliance', 'appliance'),\n",
       " ('cisco certified entry networking technician', 'stage'),\n",
       " ('vietnam', 'vietnam information security association'),\n",
       " ('cryptogram', 'puzzle'),\n",
       " ('server', 'audix'),\n",
       " ('cryptocurrency', 'auroracoin'),\n",
       " ('software', 'autoq3d community'),\n",
       " ('mnet (peer-to-peer network)', 'software'),\n",
       " ('vector', 'autosketch'),\n",
       " ('software', 'autoturn'),\n",
       " ('system', 'auxiliary feedwater'),\n",
       " ('software', 'avactis'),\n",
       " ('software', 'avid symphony'),\n",
       " ('software', 'avionics software'),\n",
       " ('lucideus', 'company'),\n",
       " ('software', 'avs video editor'),\n",
       " ('server', 'axigen'),\n",
       " ('compiler', 'aztec c'),\n",
       " ('impossible differential cryptanalysis', 'form'),\n",
       " ('barcode', 'aztec code'),\n",
       " ('ada (computer virus)', 'virus'),\n",
       " ('british computer society', 'body'),\n",
       " ('digital visual interface', 'interface'),\n",
       " ('server', 'b news'),\n",
       " ('software', 'backup exec'),\n",
       " ('sigint (conference)', 'conference'),\n",
       " ('category', 'backup sync share'),\n",
       " ('software', 'backup4all'),\n",
       " ('threat', 'badbios'),\n",
       " ('server', 'bamboo (software)'),\n",
       " ('message authentication code', 'piece'),\n",
       " ('virtual control program interface', 'specification'),\n",
       " ('control', 'bandspread'),\n",
       " ('information system', 'study'),\n",
       " ('internet vigilantism', 'phenomenon'),\n",
       " ('cyberberkut', 'group'),\n",
       " ('database', 'barcode of life data systems'),\n",
       " ('hacker', 'barnaby jack'),\n",
       " ('compiler', 'bartok (compiler)'),\n",
       " ('rendezvous protocol', 'protocol'),\n",
       " ('border gateway protocol', 'protocol'),\n",
       " ('postgresql', 'system'),\n",
       " ('server', 'base4'),\n",
       " ('patched (malware)', 'trojan'),\n",
       " ('cipher', 'baseking'),\n",
       " ('emulator', 'basilisk ii'),\n",
       " ('cipher', 'bassomatic'),\n",
       " ('cipher department of the high command of the wehrmacht', 'agency'),\n",
       " ('hardware virtualization', 'virtualization'),\n",
       " ('cryptographically secure pseudorandom number generator', 'generator'),\n",
       " ('surveillance', 'bayraktar tactical uas'),\n",
       " ('nintendo network', 'service'),\n",
       " ('software', 'bazooka blitzkrieg'),\n",
       " ('compiler', 'bds c'),\n",
       " ('software', 'beat detection'),\n",
       " ('graphical identification and authentication', 'component'),\n",
       " ('detection', 'beatmapping'),\n",
       " ('cipher', 'beaufort cipher'),\n",
       " ('substitution', 'bechamp reaction'),\n",
       " ('emulator', 'beebdroid'),\n",
       " ('strange brew (computer virus)', 'virus'),\n",
       " ('emulator', 'beebem'),\n",
       " ('misty1', 'cipher'),\n",
       " ('smartphone', 'benq p30'),\n",
       " ('gsm', 'benq-siemens sl91'),\n",
       " ('hillar aarelaid', 'ceo'),\n",
       " ('one instruction set computer', 'machine'),\n",
       " ('system', 'beos'),\n",
       " ('surveillance', 'bernard spindel'),\n",
       " ('pgpcoder', 'trojan'),\n",
       " ('identification friend or foe', 'system'),\n",
       " ('hackbb', 'tor'),\n",
       " ('hacker', 'bernd fix'),\n",
       " ('hacker', 'bernie s'),\n",
       " ('divx', 'attempt'),\n",
       " ('cooperative storage cloud', 'model'),\n",
       " ('software', 'betinvest'),\n",
       " ('storage made easy', 'broker'),\n",
       " ('emulator', 'bhole'),\n",
       " ('rsa securid', 'mechanism'),\n",
       " ('hypervisor', 'bhyve'),\n",
       " ('bitbucket', 'service'),\n",
       " ('organization', 'cyber civil rights initiative'),\n",
       " ('global positioning system', 'system'),\n",
       " ('global conference on cyberspace', 'conferences'),\n",
       " ('turnkey linux virtual appliance library', 'project'),\n",
       " ('software', 'bibus'),\n",
       " ('cipher', 'bifid cipher'),\n",
       " ('software', 'big memory'),\n",
       " ('screenos', 'system'),\n",
       " ('hacker', 'bill landreth'),\n",
       " ('security', 'billguard'),\n",
       " ('garr', 'network'),\n",
       " (\"merkle's puzzles\", 'construction'),\n",
       " ('hacker', 'billy hoffman'),\n",
       " ('wot services', 'company'),\n",
       " ('hp data protector', 'backup'),\n",
       " ('software', 'bio7'),\n",
       " ('content-control software', 'software'),\n",
       " ('measurement and signature intelligence', 'branch'),\n",
       " ('firmware', 'bios'),\n",
       " ('usb', 'bioslax'),\n",
       " ('ipv6 transition mechanism', 'technology'),\n",
       " ('magento', 'platform'),\n",
       " ('owasp zap', 'scanner'),\n",
       " ('oracle weblogic server', 'server'),\n",
       " ('asset', 'bitcoin'),\n",
       " ('epoc', 'efficient probabilistic public-key encryption scheme'),\n",
       " ('bittorrent', 'bitcomet'),\n",
       " ('software', 'bitrix24'),\n",
       " ('guardtime', 'company'),\n",
       " ('arm architecture', 'family'),\n",
       " ('atm controller', 'system'),\n",
       " ('microsoft forefront unified access gateway', 'solution'),\n",
       " ('hacker', 'black hat'),\n",
       " ('web of trust', 'concept'),\n",
       " ('software', 'blackadder (named data networking)'),\n",
       " ('smartphone', 'blackberry bold 9700'),\n",
       " ('smartphone', 'blackberry classic'),\n",
       " ('smeg virus construction kit', 'engine'),\n",
       " ('smartphone', 'blackberry curve 8520'),\n",
       " ('smartphone', 'blackberry leap'),\n",
       " ('jdbgmgr.exe virus hoax', 'virus'),\n",
       " ('smartphone', 'blackberry passport'),\n",
       " ('information privacy', 'relationship'),\n",
       " ('smartphone', \"blackberry porsche design p'9981\"),\n",
       " ('kn-cipher', 'cipher'),\n",
       " ('smartphone', \"blackberry porsche design p'9982\"),\n",
       " ('toxbot', 'worm'),\n",
       " ('smartphone', 'blackberry priv'),\n",
       " ('blacker (security)', 'department'),\n",
       " ('ai (computer virus)', 'virus'),\n",
       " ('smartphone', 'blackberry q10'),\n",
       " ('smartphone', 'blackberry q5'),\n",
       " ('smartphone', 'blackberry storm'),\n",
       " ('smartphone', 'blackberry storm 2'),\n",
       " ('author domain signing practices', 'extension'),\n",
       " ('smartphone', 'blackberry z3'),\n",
       " ('kryptos', 'sculpture'),\n",
       " ('syscloud', 'company'),\n",
       " ('smartphone', 'blackberry z30'),\n",
       " ('transportation security administration', 'agency'),\n",
       " ('cryptocurrency', 'blackcoin'),\n",
       " ('smartphone', 'blackphone'),\n",
       " ('process (computing)', 'instance'),\n",
       " ('cipher', 'blowfish (cipher)'),\n",
       " ('dash7', 'sensor'),\n",
       " ('tcp gender changer', 'method'),\n",
       " ('elearnsecurity', 'company'),\n",
       " ('supercomputer', 'blue mountain (supercomputer)'),\n",
       " ('viral phenomenon', 'objects'),\n",
       " ('supercomputer', 'blue waters'),\n",
       " ('supercomputer', 'bluefire supercomputer'),\n",
       " ('privatos', 'system'),\n",
       " ('software', 'monitoring and surveillance agents'),\n",
       " ('software', 'bluefish (software)'),\n",
       " ('hc-256', 'cipher'),\n",
       " ('network domain', 'term'),\n",
       " ('emulator', 'bluemsx'),\n",
       " ('defense academic information technology consortium', 'organization'),\n",
       " ('software', 'bluenog'),\n",
       " ('online certificate status protocol', 'protocol'),\n",
       " ('software', 'bluetooth stack'),\n",
       " ('bs2000', 'computer'),\n",
       " ('software', 'bmc control-m'),\n",
       " ('validsoft', 'company'),\n",
       " ('swipsy', 'toolkit'),\n",
       " ('server', 'bmc remedy action request system'),\n",
       " ('resource access control facility', 'product'),\n",
       " ('the hacker wars', 'film'),\n",
       " ('server', 'boa (web server)'),\n",
       " ('characters of casualty', 'drama'),\n",
       " ('software', 'board portal'),\n",
       " ('virus', 'bomber (computer virus)'),\n",
       " ('triple des', 'name'),\n",
       " ('server', 'boneyards'),\n",
       " ('software', 'child care management software'),\n",
       " ('cipher', 'book cipher'),\n",
       " ('digital will', 'document'),\n",
       " ('ring', 'boolean ring'),\n",
       " ('server', 'borland enterprise server'),\n",
       " ('message', 'bounce message'),\n",
       " ('start network', 'company'),\n",
       " ('signature', 'bounded weak echo region'),\n",
       " ('anti-tamper software', 'software'),\n",
       " ('biometric database law', 'law'),\n",
       " ('emulator', 'boycottadvance'),\n",
       " ('information security', 'practice'),\n",
       " ('acme (computer virus)', 'virus'),\n",
       " ('donbot botnet', 'botnet'),\n",
       " ('hans hermes', 'mathematician'),\n",
       " ('souradyuti paul', 'cryptologist'),\n",
       " ('cryptoverif', 'tool'),\n",
       " ('steganography', 'bpcs-steganography'),\n",
       " ('smartphone', 'bq aquaris e4.5'),\n",
       " ('software', 'brainstorming software'),\n",
       " ('black opal stakes', 'race'),\n",
       " ('cascade (computer virus)', 'virus'),\n",
       " ('software', 'brake to vacate'),\n",
       " ('software', 'brandlive'),\n",
       " ('security', 'breach (security exploit)'),\n",
       " ('swen', 'worm'),\n",
       " ('botnet', 'bredolab botnet'),\n",
       " ('bruce schneier', 'cryptographer'),\n",
       " ('soatest', 'testing'),\n",
       " ('server', 'brekeke sip server'),\n",
       " ('communications machine', 'device'),\n",
       " ('mikrotik', 'manufacturer'),\n",
       " ('huawei', 'networking'),\n",
       " ('hacker', 'brian aker'),\n",
       " ('emulator', 'brmsx'),\n",
       " ('hacker', 'bruce fancher'),\n",
       " ('spysubtract', 'application'),\n",
       " ('software', 'bruce lee lives'),\n",
       " ('heuristic (computer science)', 'technique'),\n",
       " ('bsi', 'bs 25999'),\n",
       " ('trifid cipher', 'cipher'),\n",
       " ('bsi', 'bs 7925-1'),\n",
       " ('itil', 'set'),\n",
       " ('scanner', 'buck-security'),\n",
       " ('software', 'build-a-bird'),\n",
       " ('software', 'buildbox'),\n",
       " ('initialization vector', 'input'),\n",
       " ('server', 'building site'),\n",
       " ('software', 'buildout'),\n",
       " ('nyaa torrents', 'website'),\n",
       " ('software', 'busybox'),\n",
       " ('multiprogram research facility', 'facility'),\n",
       " ('null session', 'connection'),\n",
       " ('network voice protocol', 'protocol'),\n",
       " ('server', 'butler sql'),\n",
       " ('transfer secret', 'domain'),\n",
       " ('netware core protocol', 'protocol'),\n",
       " ('software', 'bvckup 2'),\n",
       " ('polymorphic code', 'code'),\n",
       " ('m8 (cipher)', 'cipher'),\n",
       " ('software', 'bybox'),\n",
       " ('virus', 'byte bandit'),\n",
       " ('linux distribution', 'system'),\n",
       " ('server', 'bytemark'),\n",
       " ('wireless transport layer security', 'protocol'),\n",
       " ('malware', 'c2.lop'),\n",
       " ('alfred (mobile app)', 'app'),\n",
       " ('software', 'ca spectrum'),\n",
       " ('network crack program hacker group', 'group'),\n",
       " ('intrusion tolerance', 'approach'),\n",
       " ('network cloaking', 'attempt'),\n",
       " ('software', 'cabri geometry'),\n",
       " ('software', 'cadac group'),\n",
       " ('software', 'cadam'),\n",
       " ('mongodb', 'database'),\n",
       " ('voice phishing', 'practice'),\n",
       " ('workstation', 'cakewalk sonar'),\n",
       " ('server', 'calendar and contacts server'),\n",
       " ('organization', 'california council on science and technology'),\n",
       " ('cross-site scripting', 'vulnerability'),\n",
       " ('simple service discovery protocol', 'protocol'),\n",
       " ('software', 'calliduscloud'),\n",
       " ('open source information system', 'name'),\n",
       " ('renam', 'research'),\n",
       " ('square (cipher)', 'cipher'),\n",
       " ('cipher', 'camellia (cipher)'),\n",
       " ('computer', 'device'),\n",
       " ('multi router traffic grapher', 'software'),\n",
       " ('mahdi (malware)', 'malware'),\n",
       " ('hacker', 'cameron lacroix'),\n",
       " ('microframework', 'camping (microframework)'),\n",
       " ('compiler', 'program'),\n",
       " ('database', 'campus (database)'),\n",
       " ('vulnerability (computing)', 'weakness'),\n",
       " ('password psychology', 'study'),\n",
       " ('pokki', 'platform'),\n",
       " ('privacy engineering', 'discipline'),\n",
       " ('digital subscriber line', 'family'),\n",
       " ('airtable', 'service'),\n",
       " ('information security audit', 'audit'),\n",
       " ('organization', 'canadian internet registration authority'),\n",
       " ('software', 'cantera (software)'),\n",
       " ('advocacy', 'activity'),\n",
       " ('software', 'cantor (music software)'),\n",
       " ('control', 'capicom'),\n",
       " ('control', 'carbon copy (software)'),\n",
       " ('system', 'card catalog (cryptology)'),\n",
       " ('botnet', 'carna botnet'),\n",
       " ('data integrity', 'aspect'),\n",
       " ('ccrypt', 'utility'),\n",
       " ('email client', 'program'),\n",
       " ('supercomputer', 'carter (supercomputer)'),\n",
       " ('policy', 'carter doctrine'),\n",
       " ('collusion syndicate', 'security'),\n",
       " ('namecoin', 'cryptocurrency'),\n",
       " ('blue box', 'device'),\n",
       " ('xoftspy portable anti-spyware', 'application'),\n",
       " ('virus', 'cascade (computer virus)'),\n",
       " ('py (cipher)', 'cipher'),\n",
       " ('virus', 'casino (computer virus)'),\n",
       " ('smartphone', \"casio g'zone commando\"),\n",
       " ('server', 'casparcg'),\n",
       " ('cipher', 'cast-128'),\n",
       " ('cipher', 'cast-256'),\n",
       " ('software', 'cats (software)'),\n",
       " ('visual cryptography', 'technique'),\n",
       " ('server', 'caudium (web server)'),\n",
       " ('patch (computing)', 'piece'),\n",
       " ('protection', 'cave conservation'),\n",
       " ('bittorrent', 'protocol'),\n",
       " ('software', 'cbecon'),\n",
       " ('zoc (software)', 'emulator'),\n",
       " ('two-way security', 'term'),\n",
       " ('blaster (computer worm)', 'worm'),\n",
       " ('microsoft', 'active server pages'),\n",
       " ('emulator', 'cbterm'),\n",
       " ('cyber storm iii', 'exercise'),\n",
       " ('nuttx', 'system'),\n",
       " ('software', 'c-cast'),\n",
       " ('domain hack', 'name'),\n",
       " ('anycast', 'network'),\n",
       " ('software', 'ccg profiles'),\n",
       " ('emulator', 'ccs64'),\n",
       " ('protecting children from internet pornographers act of 2011', 'united'),\n",
       " ('adversary (cryptography)', 'entity'),\n",
       " ('xml', 'cctrl'),\n",
       " ('software', 'cd ripper'),\n",
       " ('draft communications data bill', 'legislation'),\n",
       " ('microsoft message queuing', 'implementation'),\n",
       " ('user-managed access', 'management'),\n",
       " ('dalnet', 'network'),\n",
       " ('supercomputer', 'cdc 6600'),\n",
       " ('exploit kit', 'system'),\n",
       " ('facebook messenger', 'service'),\n",
       " ('supercomputer', 'cdc star-100'),\n",
       " ('protection', 'cd-cops'),\n",
       " ('software', 'cdex'),\n",
       " ('misuse case', 'tool'),\n",
       " ('b1 (archive format)', 'format'),\n",
       " ('linux', 'cdfs'),\n",
       " ('spamchek', 'name'),\n",
       " ('open-source software security', 'measure'),\n",
       " ('software', 'cdrwin'),\n",
       " ('cryptosystem', 'ceilidh'),\n",
       " ('xml', 'cellml'),\n",
       " ('private vlan', 'technique'),\n",
       " ('internet printing protocol', 'protocol'),\n",
       " ('mass assignment vulnerability', 'vulnerability'),\n",
       " ('software', 'cellprofiler'),\n",
       " ('software', 'centrify'),\n",
       " ('illumio', 'data'),\n",
       " ('forward anonymity', 'property'),\n",
       " ('server', 'cerberus ftp server'),\n",
       " ('chris hoofnagle', 'professor'),\n",
       " ('internet', 'certificate management over cms'),\n",
       " ('spoofing attack', 'situation'),\n",
       " (\"the hacker's handbook\", 'book'),\n",
       " ('document', 'certificate of identity'),\n",
       " ('document', 'certificate policy'),\n",
       " ('linux from scratch', 'type'),\n",
       " ('message', 'certificate signing request'),\n",
       " ('compiler', 'cfront'),\n",
       " ('software', 'cgram software'),\n",
       " ('patriotic hacking', 'term'),\n",
       " ('hacker', 'chad davis'),\n",
       " ('rar (file format)', 'format'),\n",
       " ('spamming', 'use'),\n",
       " ('countermeasure', 'chaff (countermeasure)'),\n",
       " ('firewall', 'check point vpn-1'),\n",
       " ('distinguished warfare medal', 'united'),\n",
       " ('keng lim', 'server'),\n",
       " ('hdmi', 'interface'),\n",
       " ('database', 'chemical database'),\n",
       " ('voice over ip', 'methodology'),\n",
       " ('drizzle (database server)', 'system'),\n",
       " ('server', 'cherokee (web server)'),\n",
       " ('bootstrapping node', 'node'),\n",
       " ('smartphone', 'cherry mobile omega spectrum'),\n",
       " ('emulator', 'cherryos'),\n",
       " ('android one', 'line'),\n",
       " ('hijackthis', 'tool'),\n",
       " ('zu chongzhi', 'mathematician'),\n",
       " ('software', 'chess informant expert'),\n",
       " ('4chan', 'website'),\n",
       " ('niels ferguson', 'cryptographer'),\n",
       " ('server', 'chess live'),\n",
       " ('cobit', 'framework'),\n",
       " ('cipher', 'chiasmus (cipher)'),\n",
       " ('oracle iplanet web proxy server', 'software'),\n",
       " ('microsoft office', 'suite'),\n",
       " ('encryption ban proposal in the united kingdom', 'pledge'),\n",
       " ('software', 'chip support package'),\n",
       " ('hierarchy', 'chomsky hierarchy'),\n",
       " ('amazingports', 'firewall'),\n",
       " ('tinfoil hat linux', 'distribution'),\n",
       " ('hacker', 'christopher abad'),\n",
       " ('sharyl attkisson', 'author'),\n",
       " ('security and maintenance', 'component'),\n",
       " ('laptop', 'chromebook'),\n",
       " ('laptop', 'chromebook pixel'),\n",
       " ('ieee biometrics council', 'councils'),\n",
       " ('telecommunication', \"chunghwa int'l communication network\"),\n",
       " ('jacques stern', 'cryptographer'),\n",
       " ('asset', 'ci exploitation teams'),\n",
       " ('software', 'ciberbit'),\n",
       " ('supercomputer', 'cielo (supercomputer)'),\n",
       " ('dynamic dns', 'method'),\n",
       " ('software', 'cietmap'),\n",
       " ('hyperjacking', 'attack'),\n",
       " ('cipher', 'ciks-1'),\n",
       " ('software', 'cimtrak'),\n",
       " ('oracle database', 'system'),\n",
       " ('software', 'cinderella (software)'),\n",
       " ('multiotp', 'class'),\n",
       " ('section', 'cyber terror response center'),\n",
       " ('codec', 'cineform'),\n",
       " ('prody parrot', 'program'),\n",
       " ('set-top box', 'device'),\n",
       " ('codec', 'cinepak'),\n",
       " ('software', 'blended threat'),\n",
       " ('national cybersecurity center of excellence', 'organization'),\n",
       " ('property', 'ciphertext indistinguishability'),\n",
       " ('cyber intelligence sharing and protection act', 'law'),\n",
       " ('cipher', 'cipherunicorn-a'),\n",
       " ('password authentication protocol', 'protocol'),\n",
       " ('cipher', 'cipherunicorn-e'),\n",
       " ('software', 'cisco ios'),\n",
       " ('firewall', 'cisco pix'),\n",
       " ('webroot antivirus with spy sweeper', 'utility'),\n",
       " ('internet content adaptation protocol', 'protocol'),\n",
       " ('norton confidential', 'program'),\n",
       " ('open virtualization format', 'standard'),\n",
       " ('wi-fi', 'technology'),\n",
       " ('system', 'cisco security agent'),\n",
       " ('security', 'cisco security monitoring, analysis, and response system'),\n",
       " ('the duel after the masquerade', 'painting'),\n",
       " ('bluebugging', 'form'),\n",
       " ('organization', 'citizen cyberscience centre'),\n",
       " ('encrypted media extensions', 'specification'),\n",
       " ('matthew d. green', 'cryptographer'),\n",
       " ('system', 'gamma-ray burst coordinates network'),\n",
       " ('laptop', 'computer'),\n",
       " ('xhamster', 'video'),\n",
       " ('software', 'clara.io'),\n",
       " ('server', 'claris officemail'),\n",
       " ('ipsw', 'format'),\n",
       " ('orientdb', 'system'),\n",
       " ('hosts (file)', 'file'),\n",
       " ('p2000 (network)', 'network'),\n",
       " ('audit trail', 'record'),\n",
       " ('the hacker files', 'dc'),\n",
       " ('message authentication', 'property'),\n",
       " ('quality engineering', 'management'),\n",
       " ('software', 'cleanmail antispam'),\n",
       " ('server', 'cl-http'),\n",
       " ('protection', 'assisted natural regeneration'),\n",
       " ('ccid (protocol)', 'protocol'),\n",
       " ('cryptobuddy', 'application'),\n",
       " ('fraud', 'click fraud'),\n",
       " ('qmail', 'agent'),\n",
       " ('software', 'click.to'),\n",
       " ('scytale', 'tool'),\n",
       " ('hacking at random', 'conference'),\n",
       " ('hacker', 'clinton haines'),\n",
       " ('software', 'clipgrab'),\n",
       " ('compiler', 'clipper (programming language)'),\n",
       " ('open-source software', 'software'),\n",
       " ('information', 'clli code'),\n",
       " ('ccna', 'certification'),\n",
       " ('html', 'language'),\n",
       " ('system', 'cloud (operating system)'),\n",
       " ('windows media video', 'type'),\n",
       " ('system', 'cloud cms'),\n",
       " ('database', 'cloud database'),\n",
       " ('organization', 'cloud security alliance'),\n",
       " ('transport layer security', 'protocols'),\n",
       " ('language-based security', 'set'),\n",
       " ('7z', 'format'),\n",
       " ('software testing', 'investigation'),\n",
       " ('controlled cryptographic item', 'term'),\n",
       " ('appliance', 'cloud storage gateway'),\n",
       " ('software', 'cloudbric'),\n",
       " ('voter-verified paper audit trail', 'method'),\n",
       " ('software', 'cloudcompare'),\n",
       " ('norwegian cyber defence force', 'branch'),\n",
       " ('information systems international conference', 'chapter'),\n",
       " ('zachman framework', 'ontology'),\n",
       " ('anti-spyware coalition', 'group'),\n",
       " ('motion (surveillance software)', 'application'),\n",
       " ('acoustic cryptanalysis', 'attack'),\n",
       " ('mobile identity management', 'development'),\n",
       " ('internet bot', 'application'),\n",
       " ('cloudera', 'cloudera impala'),\n",
       " ('lawrence a. gordon', 'professor'),\n",
       " ('confide', 'band'),\n",
       " ('system', 'cloudlinux os'),\n",
       " ('backup exec', 'software'),\n",
       " ('pin tumbler lock', 'mechanism'),\n",
       " ('openvms', 'computer'),\n",
       " ('multipath tcp', 'effort'),\n",
       " ('software', 'cmdbuild'),\n",
       " ('system', 'cms'),\n",
       " ('eff des cracker', 'machine'),\n",
       " ('trusted system', 'system'),\n",
       " ('non-commutative cryptography', 'area'),\n",
       " ('computer', 'cnc router'),\n",
       " ('javapos', 'standard'),\n",
       " ('supercomputer', 'coates (supercomputer)'),\n",
       " ('spy sweeper', 'product'),\n",
       " ('solitude (supergirl)', 'episode'),\n",
       " ('committee for state security (ukraine)', 'committee'),\n",
       " ('software', 'cobian backup'),\n",
       " ('server', 'cocktail waitress'),\n",
       " ('zsnes', 'emulator'),\n",
       " ('cipher', 'coconut98'),\n",
       " ('software', 'code reviewing software'),\n",
       " ('codepeer', 'tool'),\n",
       " ('amazon s3', 'service'),\n",
       " ('goatse security', 'group'),\n",
       " ('protection', 'code wheel'),\n",
       " ('document', 'codebook'),\n",
       " ('city identification card', 'form'),\n",
       " ('bosm (festival)', 'festival'),\n",
       " ('codec', 'codec2'),\n",
       " ('compiler', 'codesynthesis xsd'),\n",
       " ('fiddler (software)', 'http'),\n",
       " ('cryptosystem', \"cohen's cryptosystem\"),\n",
       " ('cryptocurrency', 'coinye'),\n",
       " ('software', 'coldmud'),\n",
       " ('software', 'collabtive'),\n",
       " ('security', 'collusion syndicate'),\n",
       " ('devconf.cz', 'conference'),\n",
       " ('swiftbroadband', 'network'),\n",
       " ('supercomputer', 'columbia (supercomputer)'),\n",
       " ('software', 'com port redirector'),\n",
       " ('radio', 'combat-net radio'),\n",
       " ('security', 'command consulting group'),\n",
       " ('ciphercloud', 'company'),\n",
       " ('retina-x studios', 'company'),\n",
       " ('gpg4win', 'email'),\n",
       " ('kernel.org', 'point'),\n",
       " ('laptop', 'commodore lcd'),\n",
       " ('darpa', 'common affordable lightweight fighter'),\n",
       " ('software', 'common music notation'),\n",
       " ('iso 9564', 'standard'),\n",
       " ('ios jailbreaking', 'process'),\n",
       " ('librecmc', 'distribution'),\n",
       " ('hasty pudding cipher', 'cipher'),\n",
       " ('software', 'commsuite 95'),\n",
       " ('server', 'communigate pro'),\n",
       " ('software', 'comparison of embroidery software'),\n",
       " ('software', 'child exploitation tracking system'),\n",
       " ('software', 'comparison of video codecs'),\n",
       " ('server', 'completeftp'),\n",
       " ('pragyan', 'iso'),\n",
       " ('test-driven development', 'process'),\n",
       " ('halon (software)', 'distribution'),\n",
       " ('secure cryptoprocessor', 'computer'),\n",
       " ('software', 'completetax'),\n",
       " ('anti (computer virus)', 'virus'),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in word2idx if \" \" in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took me 16.031342029571533 seconds to extract USE embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crateva greveana flowers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_word_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wansecure firewall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Values and keys obtained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim=-1000, closest_word=\"\"):\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    closest_word = \"\"\n",
    "    with shelve.open(use_embeddings, 'c') as db:\n",
    "        for i, w in enumerate(word_embeddings):\n",
    "            db[words[i]] = w\n",
    "        closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[closest_word_idx]\n",
    "        del word_embeddings\n",
    "    del db\n",
    "    return closest_word, max_sim\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "    if os.path.isfile(use_embeddings):\n",
    "        with shelve.open(use_embeddings, 'r') as db:\n",
    "            embeds = np.array(list(db.values()))\n",
    "            words = np.array(list(db.keys()))\n",
    "            print (\"Values and keys obtained\", time.time()-a)\n",
    "            sim_mat = awesome_cossim_topn(coo_matrix(embeds, dtype=np.float64), coo_matrix(word_to_compare.T, dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250)\n",
    "            print (\"Sim mat calculated\", time.time()-a)\n",
    "            closest_word_idx = np.argmax(sim_mat)\n",
    "            print (\"idx gotten\", time.time()-a)\n",
    "            closest_word = words[closest_word_idx]\n",
    "    else:\n",
    "        words = list(word2id_db.keys())\n",
    "        print (\"Obtained list of words\")\n",
    "        len_part = 100000\n",
    "        max_sim = -1000\n",
    "        n_parts = ceil(len(words)/len_part)\n",
    "        closest_word = \"\"\n",
    "        for i in range(n_parts):\n",
    "            words_part = words[i*len_part:(i+1)*len_part]\n",
    "            closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "words = list(word2id_db.getall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "\n",
    "org_names = names['buyer'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "\n",
    "\n",
    "print('All 3-grams in \"Department\":')\n",
    "print(extract_ngrams('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016379356384277344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"the\" in words_set\n",
    "time.time() - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "\n",
    "def extract_ngrams(string, n=3):\n",
    "    string = fix_text(string).encode(\"ascii\", errors=\"ignore\").decode().lower() # fix text\n",
    "    string = string.replace('&', 'and').replace(',', ' ').replace('-', ' ').title()\n",
    "    string = re.sub('[' + re.escape(''.join(chars_to_remove)) + ']', '', string)\n",
    "    string = ' ' + re.sub(' +',' ',string).strip() + ' '\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    ngrams = [''.join(ngram) for ngram in ngrams]\n",
    "    return ngrams\n",
    "\n",
    "word_to_match = \"margherita pizza\"\n",
    "words = list(word2id_db.keys())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(words + [word_to_match])\n",
    "\n",
    "# d = awesome_cossim_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    print (sparserows)\n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "words[np.argmax(d)]\n",
    "print (\"time: \", start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_db_new = shelve.open(prefix + \"_relations_map.db\", \"c\")\n",
    "for k, v in relations_db.items():\n",
    "    relations_db_new[\"###\".join(k.split(\"_\"))] = v\n",
    "relations_db_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"\\t\".join(l.split(\"\\t\")[1:-1]) for l in open(\"../junk/security_dataset.tsv\",\"r\").read().split(\"\\n\")[1:]]\n",
    "open(\"../files/dataset/dataset.tsv\",\"w\").write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open(use_embeddings, 'r') as db:    \n",
    "    allitems = list(db.items())\n",
    "    emb = [el[1] for el in allitems]\n",
    "    wds = [el[0] for el in allitems]\n",
    "    file = open(\"../files/embeddings_list.pkl\", \"wb\")\n",
    "    pickle.dump(allitems, file)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "            \n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22912765"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab peter wyche (diplomat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab acoma-zuni section\n",
      "Original word not in vocab madan-harini\n",
      "Original word not in vocab trust no one (internet security)\n",
      "Original word not in vocab international tibet independence movement\n",
      "Original word not in vocab isobase\n",
      "Original word not in vocab human computer interaction (security)\n",
      "Original word not in vocab poetas de karaoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab ipa pulmonic consonant chart with audio\n",
      "Original word not in vocab lego clutch powers: bad hair day\n",
      "Original word not in vocab aed (non-profit)\n",
      "Original word not in vocab quilmes airport\n",
      "Original word not in vocab yendegaia airport\n",
      "Original word not in vocab the pack a.d.\n",
      "Original word not in vocab harvie-watt baronets\n",
      "Original word not in vocab sharp actius rd3d notebook\n",
      "Original word not in vocab big beach boutique ii - the movie\n",
      "Original word not in vocab privacy by design\n",
      "Original word not in vocab motorola devour\n",
      "Original word not in vocab piracy act\n",
      "Original word not in vocab starter ring gear\n",
      "Original word not in vocab antonio sánchez (puerto rican host)\n",
      "Original word not in vocab electronic logbook\n",
      "Original word not in vocab greg burke (journalist)\n",
      "Original word not in vocab deaths in november 2013\n",
      "Original word not in vocab hp mini 311\n",
      "Original word not in vocab confederation of indigenous nationalities of the ecuadorian amazon\n",
      "Original word not in vocab url subscription architecture\n",
      "Original word not in vocab snowballers entertainment\n",
      "Original word not in vocab basic strategic arts program\n",
      "Original word not in vocab mars (ticket reservation system)\n",
      "Original word not in vocab matija kristić\n",
      "Original word not in vocab edward graham lee\n",
      "Original word not in vocab rebellion of the three guards\n",
      "Original word not in vocab the recipe for gertrude\n",
      "Original word not in vocab quiet pc\n",
      "Original word not in vocab russian amateur radio union\n",
      "1 done\n",
      "Original word not in vocab core strategy document\n",
      "Original word not in vocab mutukula airport\n",
      "2 done\n",
      "Original word not in vocab magnus l. kpakol\n",
      "1 done\n",
      "Original word not in vocab strangers (malibu comics)\n",
      "2 done\n",
      "3 done\n",
      "Original word not in vocab cary baronets\n",
      "3 done\n",
      "Original word not in vocab andrew wood (diplomat)\n",
      "4 done\n",
      "Original word not in vocab lead petty officer\n",
      "4 done\n",
      "Original word not in vocab corps of military police (india)\n",
      "5 done\n",
      "Original word not in vocab siniša radanović\n",
      "5 done\n",
      "Original word not in vocab oatmeal cookie\n",
      "6 done\n",
      "Original word not in vocab academic research alliance\n",
      "6 done\n",
      "1 done\n",
      "Original word not in vocab joseph e. duncan iii\n",
      "7 done\n",
      "7 done\n",
      "Original word not in vocab lideta army airport\n",
      "2 done\n",
      "8 done\n",
      "8 done\n",
      "3 done\n",
      "9 done\n",
      "9 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "10 done\n",
      "10 done\n",
      "7 done\n",
      "11 done\n",
      "11 done\n",
      "12 done\n",
      "12 done\n",
      "8 done\n",
      "13 done\n",
      "13 done\n",
      "1 done\n",
      "9 done\n",
      "14 done\n",
      "2 done\n",
      "14 done\n",
      "1 done\n",
      "15 done\n",
      "3 done\n",
      "15 done\n",
      "10 done\n",
      "16 done\n",
      "2 done\n",
      "11 done\n",
      "4 done\n",
      "16 done\n",
      "3 done\n",
      "12 done\n",
      "17 done\n",
      "5 done\n",
      "4 done\n",
      "17 done\n",
      "13 done\n",
      "6 done\n",
      "18 done\n",
      "5 done\n",
      "18 done\n",
      "14 done\n",
      "6 done\n",
      "19 done\n",
      "7 done\n",
      "19 done\n",
      "15 done\n",
      "7 done\n",
      "20 done\n",
      "20 done\n",
      "16 done\n",
      "8 done\n",
      "21 done\n",
      "8 done\n",
      "21 done\n",
      "17 done\n",
      "9 done\n",
      "22 done\n",
      "22 done\n",
      "9 done\n",
      "18 done\n",
      "23 done\n",
      "23 done\n",
      "19 done\n",
      "10 done\n",
      "24 done\n",
      "10 done\n",
      "24 done\n",
      "20 done\n",
      "11 done\n",
      "25 done\n",
      "11 done\n",
      "25 done\n",
      "21 done\n",
      "12 done\n",
      "12 done\n",
      "26 done\n",
      "13 done\n",
      "26 done\n",
      "13 done\n",
      "22 done\n",
      "27 done\n",
      "14 done\n",
      "27 done\n",
      "14 done\n",
      "28 done\n",
      "15 done\n",
      "23 done\n",
      "28 done\n",
      "16 done\n",
      "15 done\n",
      "29 done\n",
      "24 done\n",
      "17 done\n",
      "16 done\n",
      "29 done\n",
      "30 done\n",
      "25 done\n",
      "18 done\n",
      "30 done\n",
      "17 done\n",
      "26 done\n",
      "19 done\n",
      "18 done\n",
      "27 done\n",
      "31 done\n",
      "20 done\n",
      "31 done\n",
      "19 done\n",
      "28 done\n",
      "32 done\n",
      "21 done\n",
      "20 done\n",
      "32 done\n",
      "33 done\n",
      "29 done\n",
      "22 done\n",
      "33 done\n",
      "21 done\n",
      "34 done\n",
      "30 done\n",
      "34 done\n",
      "23 done\n",
      "22 done\n",
      "35 done\n",
      "24 done\n",
      "35 done\n",
      "36 done\n",
      "23 done\n",
      "25 done\n",
      "31 done\n",
      "36 done\n",
      "37 done\n",
      "37 done\n",
      "24 done\n",
      "26 done\n",
      "32 done\n",
      "38 done\n",
      "38 done\n",
      "25 done\n",
      "27 done\n",
      "33 done\n",
      "39 done\n",
      "39 done\n",
      "28 done\n",
      "26 done\n",
      "34 done\n",
      "40 done\n",
      "40 done\n",
      "29 done\n",
      "27 done\n",
      "41 done\n",
      "35 done\n",
      "30 done\n",
      "41 done\n",
      "28 done\n",
      "42 done\n",
      "36 done\n",
      "42 done\n",
      "29 done\n",
      "43 done\n",
      "37 done\n",
      "31 done\n",
      "43 done\n",
      "44 done\n",
      "30 done\n",
      "38 done\n",
      "44 done\n",
      "45 done\n",
      "32 done\n",
      "39 done\n",
      "45 done\n",
      "33 done\n",
      "46 done\n",
      "31 done\n",
      "40 done\n",
      "34 done\n",
      "46 done\n",
      "47 done\n",
      "32 done\n",
      "47 done\n",
      "35 done\n",
      "33 done\n",
      "41 done\n",
      "48 done\n",
      "48 done\n",
      "34 done\n",
      "36 done\n",
      "42 done\n",
      "49 done\n",
      "49 done\n",
      "37 done\n",
      "43 done\n",
      "35 done\n",
      "50 done\n",
      "44 done\n",
      "50 done\n",
      "38 done\n",
      "36 done\n",
      "51 done\n",
      "51 done\n",
      "45 done\n",
      "39 done\n",
      "37 done\n",
      "52 done\n",
      "52 done\n",
      "46 done\n",
      "53 done\n",
      "38 done\n",
      "40 done\n",
      "53 done\n",
      "47 done\n",
      "54 done\n",
      "39 done\n",
      "41 done\n",
      "54 done\n",
      "48 done\n",
      "55 done\n",
      "42 done\n",
      "55 done\n",
      "40 done\n",
      "49 done\n",
      "56 done\n",
      "43 done\n",
      "50 done\n",
      "41 done\n",
      "57 done\n",
      "56 done\n",
      "44 done\n",
      "51 done\n",
      "57 done\n",
      "58 done\n",
      "42 done\n",
      "45 done\n",
      "58 done\n",
      "52 done\n",
      "59 done\n",
      "43 done\n",
      "46 done\n",
      "59 done\n",
      "53 done\n",
      "44 done\n",
      "60 done\n",
      "47 done\n",
      "60 done\n",
      "54 done\n",
      "45 done\n",
      "61 done\n",
      "48 done\n",
      "55 done\n",
      "61 done\n",
      "62 done\n",
      "46 done\n",
      "49 done\n",
      "62 done\n",
      "56 done\n",
      "63 done\n",
      "50 done\n",
      "47 done\n",
      "57 done\n",
      "63 done\n",
      "64 done\n",
      "51 done\n",
      "48 done\n",
      "58 done\n",
      "64 done\n",
      "52 done\n",
      "49 done\n",
      "65 done\n",
      "59 done\n",
      "50 done\n",
      "65 done\n",
      "53 done\n",
      "66 done\n",
      "60 done\n",
      "51 done\n",
      "54 done\n",
      "67 done\n",
      "66 done\n",
      "52 done\n",
      "61 done\n",
      "55 done\n",
      "68 done\n",
      "67 done\n",
      "53 done\n",
      "62 done\n",
      "56 done\n",
      "68 done\n",
      "69 done\n",
      "54 done\n",
      "63 done\n",
      "57 done\n",
      "69 done\n",
      "70 done\n",
      "55 done\n",
      "64 done\n",
      "58 done\n",
      "70 done\n",
      "71 done\n",
      "56 done\n",
      "65 done\n",
      "72 done\n",
      "71 done\n",
      "59 done\n",
      "57 done\n",
      "66 done\n",
      "72 done\n",
      "73 done\n",
      "60 done\n",
      "58 done\n",
      "67 done\n",
      "73 done\n",
      "61 done\n",
      "74 done\n",
      "59 done\n",
      "68 done\n",
      "62 done\n",
      "74 done\n",
      "75 done\n",
      "60 done\n",
      "69 done\n",
      "63 done\n",
      "75 done\n",
      "76 done\n",
      "64 done\n",
      "61 done\n",
      "70 done\n",
      "76 done\n",
      "77 done\n",
      "62 done\n",
      "71 done\n",
      "65 done\n",
      "77 done\n",
      "72 done\n",
      "78 done\n",
      "63 done\n",
      "78 done\n",
      "66 done\n",
      "73 done\n",
      "79 done\n",
      "64 done\n",
      "79 done\n",
      "67 done\n",
      "74 done\n",
      "80 done\n",
      "65 done\n",
      "68 done\n",
      "80 done\n",
      "75 done\n",
      "69 done\n",
      "81 done\n",
      "81 done\n",
      "66 done\n",
      "70 done\n",
      "76 done\n",
      "67 done\n",
      "82 done\n",
      "71 done\n",
      "82 done\n",
      "68 done\n",
      "77 done\n",
      "83 done\n",
      "72 done\n",
      "83 done\n",
      "69 done\n",
      "84 done\n",
      "78 done\n",
      "73 done\n",
      "84 done\n",
      "70 done\n",
      "79 done\n",
      "74 done\n",
      "71 done\n",
      "85 done\n",
      "85 done\n",
      "80 done\n",
      "72 done\n",
      "75 done\n",
      "86 done\n",
      "86 done\n",
      "81 done\n",
      "76 done\n",
      "73 done\n",
      "87 done\n",
      "87 done\n",
      "74 done\n",
      "77 done\n",
      "88 done\n",
      "82 done\n",
      "88 done\n",
      "89 done\n",
      "78 done\n",
      "75 done\n",
      "83 done\n",
      "89 done\n",
      "90 done\n",
      "76 done\n",
      "79 done\n",
      "90 done\n",
      "84 done\n",
      "80 done\n",
      "77 done\n",
      "91 done\n",
      "91 done\n",
      "85 done\n",
      "92 done\n",
      "81 done\n",
      "78 done\n",
      "92 done\n",
      "93 done\n",
      "86 done\n",
      "79 done\n",
      "93 done\n",
      "82 done\n",
      "87 done\n",
      "83 done\n",
      "80 done\n",
      "94 done\n",
      "94 done\n",
      "88 done\n",
      "95 done\n",
      "84 done\n",
      "95 done\n",
      "81 done\n",
      "89 done\n",
      "96 done\n",
      "96 done\n",
      "90 done\n",
      "82 done\n",
      "85 done\n",
      "97 done\n",
      "97 done\n",
      "91 done\n",
      "83 done\n",
      "86 done\n",
      "98 done\n",
      "98 done\n",
      "92 done\n",
      "84 done\n",
      "87 done\n",
      "93 done\n",
      "88 done\n",
      "99 done\n",
      "99 done\n",
      "89 done\n",
      "85 done\n",
      "100 done\n",
      "100 done\n",
      "94 done\n",
      "90 done\n",
      "86 done\n",
      "101 done\n",
      "101 done\n",
      "95 done\n",
      "91 done\n",
      "87 done\n",
      "92 done\n",
      "102 done\n",
      "102 done\n",
      "96 done\n",
      "88 done\n",
      "93 done\n",
      "103 done\n",
      "89 done\n",
      "97 done\n",
      "103 done\n",
      "94 done\n",
      "90 done\n",
      "104 done\n",
      "98 done\n",
      "104 done\n",
      "95 done\n",
      "105 done\n",
      "99 done\n",
      "91 done\n",
      "106 done\n",
      "96 done\n",
      "100 done\n",
      "105 done\n",
      "92 done\n",
      "97 done\n",
      "101 done\n",
      "106 done\n",
      "107 done\n",
      "93 done\n",
      "102 done\n",
      "108 done\n",
      "98 done\n",
      "107 done\n",
      "94 done\n",
      "103 done\n",
      "109 done\n",
      "108 done\n",
      "95 done\n",
      "99 done\n",
      "104 done\n",
      "100 done\n",
      "109 done\n",
      "96 done\n",
      "110 done\n",
      "105 done\n",
      "97 done\n",
      "101 done\n",
      "111 done\n",
      "106 done\n",
      "110 done\n",
      "102 done\n",
      "112 done\n",
      "98 done\n",
      "113 done\n",
      "111 done\n",
      "107 done\n",
      "103 done\n",
      "99 done\n",
      "114 done\n",
      "108 done\n",
      "112 done\n",
      "100 done\n",
      "115 done\n",
      "109 done\n",
      "104 done\n",
      "113 done\n",
      "101 done\n",
      "116 done\n",
      "105 done\n",
      "114 done\n",
      "102 done\n",
      "110 done\n",
      "106 done\n",
      "115 done\n",
      "117 done\n",
      "111 done\n",
      "103 done\n",
      "107 done\n",
      "116 done\n",
      "118 done\n",
      "104 done\n",
      "108 done\n",
      "112 done\n",
      "113 done\n",
      "105 done\n",
      "109 done\n",
      "119 done\n",
      "117 done\n",
      "114 done\n",
      "106 done\n",
      "120 done\n",
      "118 done\n",
      "115 done\n",
      "110 done\n",
      "107 done\n",
      "121 done\n",
      "111 done\n",
      "119 done\n",
      "116 done\n",
      "108 done\n",
      "122 done\n",
      "120 done\n",
      "112 done\n",
      "109 done\n",
      "123 done\n",
      "113 done\n",
      "121 done\n",
      "117 done\n",
      "124 done\n",
      "110 done\n",
      "114 done\n",
      "122 done\n",
      "118 done\n",
      "111 done\n",
      "125 done\n",
      "115 done\n",
      "123 done\n",
      "112 done\n",
      "119 done\n",
      "126 done\n",
      "116 done\n",
      "124 done\n",
      "113 done\n",
      "120 done\n",
      "127 done\n",
      "125 done\n",
      "114 done\n",
      "121 done\n",
      "128 done\n",
      "117 done\n",
      "122 done\n",
      "115 done\n",
      "118 done\n",
      "129 done\n",
      "126 done\n",
      "123 done\n",
      "130 done\n",
      "127 done\n",
      "116 done\n",
      "119 done\n",
      "124 done\n",
      "131 done\n",
      "128 done\n",
      "120 done\n",
      "129 done\n",
      "117 done\n",
      "125 done\n",
      "132 done\n",
      "121 done\n",
      "118 done\n",
      "130 done\n",
      "133 done\n",
      "122 done\n",
      "126 done\n",
      "131 done\n",
      "134 done\n",
      "119 done\n",
      "123 done\n",
      "127 done\n",
      "135 done\n",
      "132 done\n",
      "124 done\n",
      "120 done\n",
      "128 done\n",
      "136 done\n",
      "133 done\n",
      "129 done\n",
      "121 done\n",
      "125 done\n",
      "134 done\n",
      "130 done\n",
      "122 done\n",
      "126 done\n",
      "137 done\n",
      "135 done\n",
      "123 done\n",
      "131 done\n",
      "127 done\n",
      "136 done\n",
      "138 done\n",
      "124 done\n",
      "132 done\n",
      "139 done\n",
      "128 done\n",
      "125 done\n",
      "140 done\n",
      "133 done\n",
      "129 done\n",
      "137 done\n",
      "138 done\n",
      "130 done\n",
      "134 done\n",
      "126 done\n",
      "141 done\n",
      "139 done\n",
      "135 done\n",
      "127 done\n",
      "142 done\n",
      "131 done\n",
      "140 done\n",
      "136 done\n",
      "143 done\n",
      "128 done\n",
      "141 done\n",
      "132 done\n",
      "142 done\n",
      "144 done\n",
      "129 done\n",
      "133 done\n",
      "137 done\n",
      "130 done\n",
      "145 done\n",
      "143 done\n",
      "134 done\n",
      "138 done\n",
      "146 done\n",
      "135 done\n",
      "144 done\n",
      "131 done\n",
      "147 done\n",
      "139 done\n",
      "148 done\n",
      "136 done\n",
      "145 done\n",
      "149 done\n",
      "132 done\n",
      "140 done\n",
      "150 done\n",
      "146 done\n",
      "133 done\n",
      "147 done\n",
      "151 done\n",
      "141 done\n",
      "137 done\n",
      "134 done\n",
      "148 done\n",
      "142 done\n",
      "152 done\n",
      "135 done\n",
      "138 done\n",
      "153 done\n",
      "149 done\n",
      "136 done\n",
      "154 done\n",
      "143 done\n",
      "139 done\n",
      "150 done\n",
      "155 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "151 done\n",
      "145 done\n",
      "152 done\n",
      "137 done\n",
      "157 done\n",
      "141 done\n",
      "153 done\n",
      "146 done\n",
      "158 done\n",
      "138 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 done\n",
      "154 done\n",
      "147 done\n",
      "159 done\n",
      "139 done\n",
      "143 done\n",
      "155 done\n",
      "148 done\n",
      "160 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "161 done\n",
      "149 done\n",
      "145 done\n",
      "157 done\n",
      "141 done\n",
      "150 done\n",
      "146 done\n",
      "142 done\n",
      "158 done\n",
      "162 done\n",
      "151 done\n",
      "147 done\n",
      "159 done\n",
      "143 done\n",
      "152 done\n",
      "148 done\n",
      "163 done\n",
      "160 done\n",
      "144 done\n",
      "149 done\n",
      "153 done\n",
      "161 done\n",
      "145 done\n",
      "150 done\n",
      "146 done\n",
      "154 done\n",
      "164 done\n",
      "151 done\n",
      "147 done\n",
      "155 done\n",
      "148 done\n",
      "152 done\n",
      "156 done\n",
      "165 done\n",
      "162 done\n",
      "149 done\n",
      "157 done\n",
      "166 done\n",
      "163 done\n",
      "153 done\n",
      "150 done\n",
      "158 done\n",
      "154 done\n",
      "151 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "164 done\n",
      "152 done\n",
      "168 done\n",
      "156 done\n",
      "160 done\n",
      "165 done\n",
      "153 done\n",
      "169 done\n",
      "161 done\n",
      "157 done\n",
      "166 done\n",
      "170 done\n",
      "154 done\n",
      "158 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "171 done\n",
      "168 done\n",
      "156 done\n",
      "162 done\n",
      "160 done\n",
      "172 done\n",
      "169 done\n",
      "157 done\n",
      "161 done\n",
      "163 done\n",
      "173 done\n",
      "170 done\n",
      "158 done\n",
      "171 done\n",
      "159 done\n",
      "174 done\n",
      "172 done\n",
      "162 done\n",
      "160 done\n",
      "175 done\n",
      "164 done\n",
      "161 done\n",
      "173 done\n",
      "163 done\n",
      "176 done\n",
      "165 done\n",
      "174 done\n",
      "166 done\n",
      "177 done\n",
      "175 done\n",
      "178 done\n",
      "164 done\n",
      "167 done\n",
      "162 done\n",
      "176 done\n",
      "168 done\n",
      "165 done\n",
      "179 done\n",
      "163 done\n",
      "169 done\n",
      "166 done\n",
      "177 done\n",
      "170 done\n",
      "180 done\n",
      "178 done\n",
      "167 done\n",
      "181 done\n",
      "164 done\n",
      "171 done\n",
      "179 done\n",
      "168 done\n",
      "172 done\n",
      "165 done\n",
      "182 done\n",
      "169 done\n",
      "166 done\n",
      "180 done\n",
      "170 done\n",
      "173 done\n",
      "183 done\n",
      "181 done\n",
      "171 done\n",
      "174 done\n",
      "167 done\n",
      "184 done\n",
      "172 done\n",
      "182 done\n",
      "175 done\n",
      "168 done\n",
      "185 done\n",
      "173 done\n",
      "176 done\n",
      "183 done\n",
      "169 done\n",
      "186 done\n",
      "174 done\n",
      "170 done\n",
      "184 done\n",
      "177 done\n",
      "187 done\n",
      "171 done\n",
      "175 done\n",
      "185 done\n",
      "178 done\n",
      "188 done\n",
      "172 done\n",
      "176 done\n",
      "186 done\n",
      "179 done\n",
      "189 done\n",
      "177 done\n",
      "187 done\n",
      "173 done\n",
      "180 done\n",
      "190 done\n",
      "174 done\n",
      "178 done\n",
      "188 done\n",
      "181 done\n",
      "189 done\n",
      "179 done\n",
      "191 done\n",
      "175 done\n",
      "182 done\n",
      "192 done\n",
      "180 done\n",
      "190 done\n",
      "176 done\n",
      "193 done\n",
      "183 done\n",
      "181 done\n",
      "194 done\n",
      "177 done\n",
      "184 done\n",
      "195 done\n",
      "191 done\n",
      "182 done\n",
      "178 done\n",
      "185 done\n",
      "192 done\n",
      "196 done\n",
      "183 done\n",
      "179 done\n",
      "186 done\n",
      "193 done\n",
      "197 done\n",
      "184 done\n",
      "180 done\n",
      "187 done\n",
      "198 done\n",
      "194 done\n",
      "185 done\n",
      "181 done\n",
      "199 done\n",
      "188 done\n",
      "186 done\n",
      "195 done\n",
      "182 done\n",
      "187 done\n",
      "189 done\n",
      "200 done\n",
      "196 done\n",
      "183 done\n",
      "188 done\n",
      "190 done\n",
      "197 done\n",
      "201 done\n",
      "184 done\n",
      "189 done\n",
      "198 done\n",
      "202 done\n",
      "190 done\n",
      "185 done\n",
      "191 done\n",
      "199 done\n",
      "191 done\n",
      "203 done\n",
      "186 done\n",
      "192 done\n",
      "200 done\n",
      "192 done\n",
      "204 done\n",
      "193 done\n",
      "187 done\n",
      "201 done\n",
      "193 done\n",
      "194 done\n",
      "188 done\n",
      "205 done\n",
      "194 done\n",
      "202 done\n",
      "189 done\n",
      "206 done\n",
      "195 done\n",
      "195 done\n",
      "207 done\n",
      "190 done\n",
      "203 done\n",
      "196 done\n",
      "204 done\n",
      "196 done\n",
      "208 done\n",
      "197 done\n",
      "197 done\n",
      "191 done\n",
      "209 done\n",
      "198 done\n",
      "205 done\n",
      "198 done\n",
      "210 done\n",
      "192 done\n",
      "199 done\n",
      "206 done\n",
      "199 done\n",
      "200 done\n",
      "193 done\n",
      "211 done\n",
      "207 done\n",
      "200 done\n",
      "194 done\n",
      "208 done\n",
      "201 done\n",
      "201 done\n",
      "195 done\n",
      "212 done\n",
      "202 done\n",
      "209 done\n",
      "202 done\n",
      "213 done\n",
      "196 done\n",
      "203 done\n",
      "210 done\n",
      "214 done\n",
      "197 done\n",
      "203 done\n",
      "204 done\n",
      "215 done\n",
      "211 done\n",
      "198 done\n",
      "204 done\n",
      "205 done\n",
      "216 done\n",
      "206 done\n",
      "199 done\n",
      "212 done\n",
      "217 done\n",
      "207 done\n",
      "205 done\n",
      "218 done\n",
      "200 done\n",
      "213 done\n",
      "206 done\n",
      "208 done\n",
      "219 done\n",
      "214 done\n",
      "209 done\n",
      "207 done\n",
      "201 done\n",
      "215 done\n",
      "220 done\n",
      "210 done\n",
      "208 done\n",
      "221 done\n",
      "202 done\n",
      "216 done\n",
      "209 done\n",
      "211 done\n",
      "203 done\n",
      "222 done\n",
      "217 done\n",
      "204 done\n",
      "218 done\n",
      "210 done\n",
      "212 done\n",
      "223 done\n",
      "205 done\n",
      "213 done\n",
      "211 done\n",
      "219 done\n",
      "206 done\n",
      "214 done\n",
      "224 done\n",
      "220 done\n",
      "212 done\n",
      "215 done\n",
      "225 done\n",
      "207 done\n",
      "221 done\n",
      "213 done\n",
      "216 done\n",
      "208 done\n",
      "214 done\n",
      "226 done\n",
      "222 done\n",
      "217 done\n",
      "209 done\n",
      "215 done\n",
      "227 done\n",
      "218 done\n",
      "223 done\n",
      "210 done\n",
      "216 done\n",
      "228 done\n",
      "219 done\n",
      "211 done\n",
      "217 done\n",
      "229 done\n",
      "224 done\n",
      "220 done\n",
      "225 done\n",
      "230 done\n",
      "218 done\n",
      "221 done\n",
      "212 done\n",
      "231 done\n",
      "219 done\n",
      "226 done\n",
      "222 done\n",
      "213 done\n",
      "232 done\n",
      "227 done\n",
      "220 done\n",
      "214 done\n",
      "223 done\n",
      "233 done\n",
      "221 done\n",
      "215 done\n",
      "228 done\n",
      "234 done\n",
      "222 done\n",
      "216 done\n",
      "224 done\n",
      "229 done\n",
      "235 done\n",
      "217 done\n",
      "225 done\n",
      "223 done\n",
      "236 done\n",
      "218 done\n",
      "230 done\n",
      "226 done\n",
      "237 done\n",
      "231 done\n",
      "219 done\n",
      "227 done\n",
      "224 done\n",
      "232 done\n",
      "238 done\n",
      "220 done\n",
      "225 done\n",
      "228 done\n",
      "233 done\n",
      "229 done\n",
      "239 done\n",
      "221 done\n",
      "226 done\n",
      "234 done\n",
      "230 done\n",
      "222 done\n",
      "240 done\n",
      "231 done\n",
      "227 done\n",
      "235 done\n",
      "241 done\n",
      "223 done\n",
      "232 done\n",
      "236 done\n",
      "228 done\n",
      "237 done\n",
      "242 done\n",
      "233 done\n",
      "224 done\n",
      "229 done\n",
      "243 done\n",
      "238 done\n",
      "234 done\n",
      "225 done\n",
      "230 done\n",
      "235 done\n",
      "244 done\n",
      "239 done\n",
      "226 done\n",
      "231 done\n",
      "236 done\n",
      "227 done\n",
      "240 done\n",
      "232 done\n",
      "237 done\n",
      "245 done\n",
      "241 done\n",
      "238 done\n",
      "228 done\n",
      "233 done\n",
      "246 done\n",
      "242 done\n",
      "234 done\n",
      "229 done\n",
      "247 done\n",
      "239 done\n",
      "235 done\n",
      "243 done\n",
      "240 done\n",
      "248 done\n",
      "230 done\n",
      "236 done\n",
      "249 done\n",
      "241 done\n",
      "244 done\n",
      "231 done\n",
      "237 done\n",
      "250 done\n",
      "232 done\n",
      "242 done\n",
      "238 done\n",
      "245 done\n",
      "251 done\n",
      "233 done\n",
      "243 done\n",
      "246 done\n",
      "239 done\n",
      "252 done\n",
      "234 done\n",
      "247 done\n",
      "244 done\n",
      "240 done\n",
      "235 done\n",
      "253 done\n",
      "248 done\n",
      "241 done\n",
      "236 done\n",
      "249 done\n",
      "254 done\n",
      "245 done\n",
      "237 done\n",
      "250 done\n",
      "242 done\n",
      "255 done\n",
      "246 done\n",
      "238 done\n",
      "243 done\n",
      "256 done\n",
      "247 done\n",
      "251 done\n",
      "239 done\n",
      "248 done\n",
      "252 done\n",
      "244 done\n",
      "257 done\n",
      "240 done\n",
      "249 done\n",
      "253 done\n",
      "258 done\n",
      "241 done\n",
      "250 done\n",
      "245 done\n",
      "259 done\n",
      "254 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "242 done\n",
      "255 done\n",
      "261 done\n",
      "252 done\n",
      "243 done\n",
      "247 done\n",
      "256 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "244 done\n",
      "257 done\n",
      "258 done\n",
      "249 done\n",
      "263 done\n",
      "254 done\n",
      "264 done\n",
      "250 done\n",
      "259 done\n",
      "245 done\n",
      "255 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "256 done\n",
      "265 done\n",
      "261 done\n",
      "252 done\n",
      "247 done\n",
      "257 done\n",
      "266 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "258 done\n",
      "267 done\n",
      "249 done\n",
      "263 done\n",
      "259 done\n",
      "268 done\n",
      "254 done\n",
      "250 done\n",
      "264 done\n",
      "260 done\n",
      "269 done\n",
      "255 done\n",
      "251 done\n",
      "270 done\n",
      "261 done\n",
      "256 done\n",
      "265 done\n",
      "252 done\n",
      "271 done\n",
      "262 done\n",
      "257 done\n",
      "266 done\n",
      "253 done\n",
      "272 done\n",
      "258 done\n",
      "263 done\n",
      "267 done\n",
      "264 done\n",
      "273 done\n",
      "259 done\n",
      "254 done\n",
      "268 done\n",
      "274 done\n",
      "260 done\n",
      "255 done\n",
      "269 done\n",
      "265 done\n",
      "261 done\n",
      "256 done\n",
      "275 done\n",
      "270 done\n",
      "266 done\n",
      "262 done\n",
      "257 done\n",
      "267 done\n",
      "276 done\n",
      "271 done\n",
      "258 done\n",
      "268 done\n",
      "263 done\n",
      "277 done\n",
      "272 done\n",
      "269 done\n",
      "259 done\n",
      "264 done\n",
      "278 done\n",
      "273 done\n",
      "270 done\n",
      "260 done\n",
      "279 done\n",
      "274 done\n",
      "261 done\n",
      "265 done\n",
      "271 done\n",
      "280 done\n",
      "272 done\n",
      "275 done\n",
      "262 done\n",
      "281 done\n",
      "266 done\n",
      "273 done\n",
      "276 done\n",
      "263 done\n",
      "282 done\n",
      "267 done\n",
      "277 done\n",
      "274 done\n",
      "264 done\n",
      "268 done\n",
      "283 done\n",
      "278 done\n",
      "275 done\n",
      "269 done\n",
      "284 done\n",
      "279 done\n",
      "276 done\n",
      "265 done\n",
      "280 done\n",
      "277 done\n",
      "270 done\n",
      "285 done\n",
      "266 done\n",
      "281 done\n",
      "278 done\n",
      "271 done\n",
      "267 done\n",
      "286 done\n",
      "282 done\n",
      "272 done\n",
      "279 done\n",
      "268 done\n",
      "287 done\n",
      "283 done\n",
      "280 done\n",
      "269 done\n",
      "273 done\n",
      "288 done\n",
      "270 done\n",
      "284 done\n",
      "281 done\n",
      "274 done\n",
      "282 done\n",
      "271 done\n",
      "289 done\n",
      "285 done\n",
      "275 done\n",
      "272 done\n",
      "283 done\n",
      "286 done\n",
      "276 done\n",
      "290 done\n",
      "284 done\n",
      "277 done\n",
      "273 done\n",
      "291 done\n",
      "287 done\n",
      "285 done\n",
      "278 done\n",
      "274 done\n",
      "288 done\n",
      "292 done\n",
      "286 done\n",
      "279 done\n",
      "275 done\n",
      "293 done\n",
      "289 done\n",
      "280 done\n",
      "287 done\n",
      "276 done\n",
      "290 done\n",
      "281 done\n",
      "288 done\n",
      "277 done\n",
      "294 done\n",
      "291 done\n",
      "282 done\n",
      "278 done\n",
      "295 done\n",
      "289 done\n",
      "292 done\n",
      "283 done\n",
      "279 done\n",
      "296 done\n",
      "284 done\n",
      "293 done\n",
      "280 done\n",
      "290 done\n",
      "297 done\n",
      "285 done\n",
      "281 done\n",
      "291 done\n",
      "298 done\n",
      "294 done\n",
      "282 done\n",
      "286 done\n",
      "295 done\n",
      "292 done\n",
      "283 done\n",
      "296 done\n",
      "299 done\n",
      "287 done\n",
      "284 done\n",
      "293 done\n",
      "297 done\n",
      "288 done\n",
      "300 done\n",
      "285 done\n",
      "301 done\n",
      "298 done\n",
      "294 done\n",
      "289 done\n",
      "286 done\n",
      "302 done\n",
      "295 done\n",
      "299 done\n",
      "303 done\n",
      "290 done\n",
      "287 done\n",
      "296 done\n",
      "300 done\n",
      "291 done\n",
      "304 done\n",
      "301 done\n",
      "288 done\n",
      "297 done\n",
      "292 done\n",
      "305 done\n",
      "302 done\n",
      "289 done\n",
      "298 done\n",
      "306 done\n",
      "293 done\n",
      "303 done\n",
      "290 done\n",
      "307 done\n",
      "299 done\n",
      "291 done\n",
      "304 done\n",
      "294 done\n",
      "300 done\n",
      "308 done\n",
      "295 done\n",
      "305 done\n",
      "309 done\n",
      "292 done\n",
      "301 done\n",
      "310 done\n",
      "296 done\n",
      "302 done\n",
      "306 done\n",
      "293 done\n",
      "311 done\n",
      "297 done\n",
      "307 done\n",
      "303 done\n",
      "298 done\n",
      "308 done\n",
      "312 done\n",
      "294 done\n",
      "304 done\n",
      "309 done\n",
      "295 done\n",
      "313 done\n",
      "299 done\n",
      "305 done\n",
      "310 done\n",
      "296 done\n",
      "300 done\n",
      "314 done\n",
      "306 done\n",
      "311 done\n",
      "297 done\n",
      "301 done\n",
      "307 done\n",
      "315 done\n",
      "298 done\n",
      "312 done\n",
      "302 done\n",
      "308 done\n",
      "316 done\n",
      "313 done\n",
      "303 done\n",
      "309 done\n",
      "299 done\n",
      "317 done\n",
      "314 done\n",
      "310 done\n",
      "300 done\n",
      "318 done\n",
      "304 done\n",
      "301 done\n",
      "311 done\n",
      "315 done\n",
      "305 done\n",
      "302 done\n",
      "316 done\n",
      "312 done\n",
      "319 done\n",
      "306 done\n",
      "303 done\n",
      "317 done\n",
      "313 done\n",
      "307 done\n",
      "320 done\n",
      "308 done\n",
      "318 done\n",
      "304 done\n",
      "309 done\n",
      "314 done\n",
      "305 done\n",
      "310 done\n",
      "319 done\n",
      "311 done\n",
      "315 done\n",
      "321 done\n",
      "306 done\n",
      "316 done\n",
      "322 done\n",
      "312 done\n",
      "307 done\n",
      "320 done\n",
      "313 done\n",
      "317 done\n",
      "323 done\n",
      "308 done\n",
      "318 done\n",
      "321 done\n",
      "309 done\n",
      "324 done\n",
      "314 done\n",
      "322 done\n",
      "310 done\n",
      "315 done\n",
      "325 done\n",
      "311 done\n",
      "319 done\n",
      "323 done\n",
      "326 done\n",
      "316 done\n",
      "324 done\n",
      "327 done\n",
      "312 done\n",
      "317 done\n",
      "320 done\n",
      "325 done\n",
      "313 done\n",
      "318 done\n",
      "328 done\n",
      "326 done\n",
      "321 done\n",
      "314 done\n",
      "327 done\n",
      "329 done\n",
      "319 done\n",
      "322 done\n",
      "315 done\n",
      "328 done\n",
      "330 done\n",
      "323 done\n",
      "316 done\n",
      "320 done\n",
      "329 done\n",
      "324 done\n",
      "331 done\n",
      "317 done\n",
      "321 done\n",
      "330 done\n",
      "325 done\n",
      "332 done\n",
      "318 done\n",
      "322 done\n",
      "326 done\n",
      "333 done\n",
      "331 done\n",
      "323 done\n",
      "327 done\n",
      "334 done\n",
      "332 done\n",
      "319 done\n",
      "324 done\n",
      "333 done\n",
      "328 done\n",
      "335 done\n",
      "334 done\n",
      "325 done\n",
      "320 done\n",
      "336 done\n",
      "329 done\n",
      "326 done\n",
      "335 done\n",
      "337 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 done\n",
      "321 done\n",
      "336 done\n",
      "327 done\n",
      "338 done\n",
      "322 done\n",
      "337 done\n",
      "328 done\n",
      "331 done\n",
      "323 done\n",
      "339 done\n",
      "338 done\n",
      "332 done\n",
      "324 done\n",
      "329 done\n",
      "340 done\n",
      "339 done\n",
      "325 done\n",
      "333 done\n",
      "340 done\n",
      "330 done\n",
      "326 done\n",
      "341 done\n",
      "334 done\n",
      "327 done\n",
      "341 done\n",
      "331 done\n",
      "335 done\n",
      "342 done\n",
      "332 done\n",
      "336 done\n",
      "328 done\n",
      "343 done\n",
      "342 done\n",
      "337 done\n",
      "333 done\n",
      "344 done\n",
      "343 done\n",
      "329 done\n",
      "345 done\n",
      "338 done\n",
      "344 done\n",
      "334 done\n",
      "346 done\n",
      "330 done\n",
      "339 done\n",
      "345 done\n",
      "335 done\n",
      "347 done\n",
      "340 done\n",
      "336 done\n",
      "331 done\n",
      "348 done\n",
      "346 done\n",
      "337 done\n",
      "347 done\n",
      "332 done\n",
      "341 done\n",
      "349 done\n",
      "348 done\n",
      "338 done\n",
      "333 done\n",
      "350 done\n",
      "342 done\n",
      "349 done\n",
      "334 done\n",
      "339 done\n",
      "351 done\n",
      "343 done\n",
      "350 done\n",
      "340 done\n",
      "335 done\n",
      "344 done\n",
      "351 done\n",
      "336 done\n",
      "352 done\n",
      "345 done\n",
      "341 done\n",
      "353 done\n",
      "337 done\n",
      "346 done\n",
      "352 done\n",
      "354 done\n",
      "342 done\n",
      "347 done\n",
      "338 done\n",
      "353 done\n",
      "355 done\n",
      "348 done\n",
      "343 done\n",
      "339 done\n",
      "356 done\n",
      "354 done\n",
      "344 done\n",
      "349 done\n",
      "340 done\n",
      "355 done\n",
      "345 done\n",
      "357 done\n",
      "350 done\n",
      "356 done\n",
      "346 done\n",
      "341 done\n",
      "358 done\n",
      "351 done\n",
      "347 done\n",
      "357 done\n",
      "359 done\n",
      "348 done\n",
      "360 done\n",
      "358 done\n",
      "342 done\n",
      "361 done\n",
      "352 done\n",
      "359 done\n",
      "349 done\n",
      "343 done\n",
      "360 done\n",
      "344 done\n",
      "353 done\n",
      "362 done\n",
      "350 done\n",
      "361 done\n",
      "345 done\n",
      "351 done\n",
      "363 done\n",
      "354 done\n",
      "362 done\n",
      "346 done\n",
      "364 done\n",
      "355 done\n",
      "363 done\n",
      "347 done\n",
      "365 done\n",
      "356 done\n",
      "364 done\n",
      "352 done\n",
      "348 done\n",
      "366 done\n",
      "365 done\n",
      "357 done\n",
      "353 done\n",
      "349 done\n",
      "366 done\n",
      "358 done\n",
      "367 done\n",
      "354 done\n",
      "367 done\n",
      "350 done\n",
      "359 done\n",
      "355 done\n",
      "368 done\n",
      "351 done\n",
      "360 done\n",
      "356 done\n",
      "368 done\n",
      "369 done\n",
      "361 done\n",
      "369 done\n",
      "357 done\n",
      "370 done\n",
      "352 done\n",
      "362 done\n",
      "370 done\n",
      "371 done\n",
      "358 done\n",
      "371 done\n",
      "353 done\n",
      "363 done\n",
      "372 done\n",
      "359 done\n",
      "372 done\n",
      "354 done\n",
      "364 done\n",
      "360 done\n",
      "355 done\n",
      "373 done\n",
      "373 done\n",
      "365 done\n",
      "356 done\n",
      "361 done\n",
      "374 done\n",
      "374 done\n",
      "357 done\n",
      "362 done\n",
      "366 done\n",
      "375 done\n",
      "375 done\n",
      "358 done\n",
      "363 done\n",
      "367 done\n",
      "359 done\n",
      "364 done\n",
      "376 done\n",
      "368 done\n",
      "360 done\n",
      "376 done\n",
      "365 done\n",
      "369 done\n",
      "361 done\n",
      "377 done\n",
      "370 done\n",
      "366 done\n",
      "377 done\n",
      "362 done\n",
      "378 done\n",
      "371 done\n",
      "379 done\n",
      "378 done\n",
      "367 done\n",
      "363 done\n",
      "372 done\n",
      "379 done\n",
      "380 done\n",
      "364 done\n",
      "368 done\n",
      "380 done\n",
      "373 done\n",
      "381 done\n",
      "365 done\n",
      "369 done\n",
      "381 done\n",
      "374 done\n",
      "382 done\n",
      "370 done\n",
      "382 done\n",
      "366 done\n",
      "371 done\n",
      "383 done\n",
      "375 done\n",
      "383 done\n",
      "367 done\n",
      "372 done\n",
      "384 done\n",
      "384 done\n",
      "368 done\n",
      "373 done\n",
      "385 done\n",
      "369 done\n",
      "376 done\n",
      "385 done\n",
      "374 done\n",
      "386 done\n",
      "370 done\n",
      "386 done\n",
      "377 done\n",
      "371 done\n",
      "375 done\n",
      "387 done\n",
      "387 done\n",
      "378 done\n",
      "372 done\n",
      "379 done\n",
      "388 done\n",
      "388 done\n",
      "373 done\n",
      "380 done\n",
      "376 done\n",
      "389 done\n",
      "389 done\n",
      "374 done\n",
      "381 done\n",
      "390 done\n",
      "390 done\n",
      "377 done\n",
      "375 done\n",
      "391 done\n",
      "382 done\n",
      "391 done\n",
      "378 done\n",
      "383 done\n",
      "379 done\n",
      "392 done\n",
      "392 done\n",
      "384 done\n"
     ]
    }
   ],
   "source": [
    "# words_sample = [\"pizza hut\", \"burger king\", \"south africa\", \"nasa\"]\n",
    "# del og_dict\n",
    "def calculate_sim(words, word1, max_sim, closest_word):\n",
    "    t = time.time()\n",
    "    i = 0\n",
    "    for word2 in words:\n",
    "        try:\n",
    "            sim = wiki2vec.similarity(\"_\".join(word1.lower().split()), \"_\".join(word2.split()))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                closest_word = word2\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    print (\"Original word: \", word1, \"Closest Word: \", closest_word)\n",
    "    print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-a))\n",
    "    sys.stdout.flush()\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_w2v(word1):\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    if word1 not in wiki2vec.wv.vocab:\n",
    "        print (\"Original word not in vocab\", word1)\n",
    "        return (closest_word, max_sim)\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, word1, max_sim, closest_word)\n",
    "    return word1, closest_word          \n",
    "\n",
    "a = time.time()\n",
    "\n",
    "# closest_word = closest_word_w2v(\"margherita pizza\")\n",
    "\n",
    "# closest_word_w2v(\"nelson mandela\")\n",
    "\n",
    "resolved = dict()\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    for res in executor.map(closest_word_w2v, failed):\n",
    "        resolved[res[0]] = res[1]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, pickle\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "\n",
    "f = open(\"../junk/failed_words\", \"rb\") \n",
    "failed, words = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim, closest_word):\n",
    "\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    \n",
    "    for i,w in enumerate(word_embeddings):\n",
    "        sim = np.dot(word_to_compare, w)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[i]\n",
    "\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(argument):\n",
    "    \n",
    "    word, embed = compare\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, embed, max_sim, closest_word)\n",
    "    with counter.get_lock():\n",
    "        counter.value += 1\n",
    "    print (\"Original word: \", word, \"Closest Word: \", closest_word, \"Sim: \", max_sim)\n",
    "    print (\"Percentage done: \", float(counter.value*100/len(failed)))\n",
    "    return word1, closest_word, max_sim\n",
    "\n",
    "\n",
    "resolved = dict()\n",
    "print (\"Working on it...\")\n",
    "counter = Value('i', 0)\n",
    "a = time.time()\n",
    "failed_embeddings = extractUSEEmbeddings(failed)\n",
    "print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "with concurrent.futures.ProcessPoolExecutor(=) as executor:\n",
    "    for res in executor.map(closest_word_w2v, zip(failed, failed_embeddings)):\n",
    "        resolved[res[0]] = (res[1], res[2])\n",
    "\n",
    "f = open(\"resolved\", \"wb\")\n",
    "pickle.dump(resolved, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'approach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-484d7c044890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresolved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"approach\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'approach'"
     ]
    }
   ],
   "source": [
    "resolved[\"approach\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005288124084472656\n",
      "0.00016236305236816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"abrkadabra\" in w2v.wv\n",
    "print (time.time()-a)\n",
    "a = time.time()\n",
    "try:\n",
    "    w2v.similarity(\"margherita_pizza\", \"abrkadabra\")\n",
    "except:    \n",
    "    print (time.time()-a)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2id_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0884,  0.2092, -0.1895, -0.1527, -0.0978,  0.0378, -0.1611,\n",
       "        0.0245,  0.0549, -0.2892,  0.0931, -0.3243, -0.2276, -0.0727,\n",
       "        0.0521, -0.2883, -0.0754, -0.0059, -0.0705, -0.3562, -0.1019,\n",
       "        0.0847,  0.111 ,  0.0049, -0.3304, -0.2235,  0.1369, -0.1037,\n",
       "       -0.0751, -0.3887,  0.1092, -0.1504,  0.0167,  0.0217,  0.0204,\n",
       "        0.064 , -0.2647,  0.3114, -0.0973,  0.1509, -0.2116, -0.0882,\n",
       "        0.1436, -0.2557,  0.23  ,  0.1662,  0.04  , -0.1121,  0.0426,\n",
       "       -0.179 , -0.0356, -0.1443, -0.2153, -0.1841, -0.2113, -0.1561,\n",
       "        0.258 , -0.0593, -0.1704, -0.0394, -0.0992, -0.1615,  0.0623,\n",
       "       -0.1708, -0.1204,  0.2041,  0.173 , -0.3095, -0.0589, -0.0366,\n",
       "        0.0084, -0.2201, -0.3896, -0.2086,  0.323 , -0.0779, -0.1028,\n",
       "        0.0626,  0.2596,  0.0631,  0.18  ,  0.1857,  0.3112,  0.0103,\n",
       "        0.2184, -0.102 ,  0.0504,  0.0907,  0.2355,  0.2216,  0.0125,\n",
       "        0.0075,  0.0846, -0.1534,  0.4137,  0.0309, -0.2167, -0.0785,\n",
       "       -0.0552,  0.1009,  0.2382,  0.0789, -0.0333, -0.2412, -0.1341,\n",
       "        0.0201,  0.2794,  0.0011,  0.0298, -0.1577, -0.1338, -0.2247,\n",
       "        0.0086, -0.1434,  0.1252,  0.1057, -0.0273, -0.1806,  0.07  ,\n",
       "       -0.0414, -0.2173, -0.1507, -0.1246, -0.006 , -0.3306,  0.2111,\n",
       "       -0.1423,  0.0107, -0.1104, -0.1613,  0.4693,  0.0907, -0.1883,\n",
       "        0.0796, -0.0664, -0.0051, -0.2936, -0.146 ,  0.3968,  0.126 ,\n",
       "       -0.003 ,  0.0855,  0.2327,  0.0266, -0.1693, -0.1261,  0.1542,\n",
       "       -0.062 ,  0.0307,  0.0056, -0.274 ,  0.1674, -0.0414,  0.2391,\n",
       "       -0.2105, -0.1144,  0.0929, -0.12  ,  0.1924, -0.0865, -0.0856,\n",
       "        0.1252,  0.1665, -0.2329, -0.0251, -0.0148,  0.0345, -0.188 ,\n",
       "       -0.0538,  0.0334,  0.1629, -0.1095, -0.1309, -0.0365, -0.2053,\n",
       "        0.0962,  0.5739, -0.1491,  0.1356,  0.1067,  0.2074,  0.1196,\n",
       "        0.0801,  0.2219, -0.1299,  0.0929, -0.2259, -0.1049, -0.3055,\n",
       "       -0.1043, -0.0748,  0.1801, -0.35  ,  0.0795,  0.2982, -0.049 ,\n",
       "        0.1819, -0.3701,  0.1377, -0.0465,  0.1129, -0.2537,  0.1466,\n",
       "       -0.032 , -0.005 , -0.0516,  0.0883, -0.1519, -0.1497, -0.0941,\n",
       "       -0.1741,  0.1991, -0.1138, -0.0219, -0.122 , -0.0123, -0.0136,\n",
       "        0.2865, -0.3002, -0.0241,  0.0726,  0.0654, -0.235 , -0.1227,\n",
       "       -0.2656, -0.2147, -0.1193,  0.011 , -0.3328,  0.3237,  0.0375,\n",
       "        0.1942,  0.0471, -0.0605,  0.0011,  0.0636, -0.03  , -0.1148,\n",
       "       -0.0559, -0.1026, -0.3166,  0.3044,  0.1773,  0.2265,  0.1125,\n",
       "       -0.114 ,  0.1113,  0.0789, -0.2937, -0.2036, -0.2807,  0.0188,\n",
       "        0.0573,  0.0679,  0.1155,  0.1796, -0.1041,  0.1025, -0.0483,\n",
       "        0.0043,  0.1314,  0.0846,  0.1881,  0.1721,  0.463 , -0.1782,\n",
       "       -0.1547,  0.2518,  0.0634,  0.2856,  0.6526, -0.3789,  0.1362,\n",
       "        0.1545,  0.0719, -0.1429, -0.1224,  0.0836,  0.0851, -0.0349,\n",
       "       -0.2552, -0.2929, -0.183 ,  0.2112, -0.0847, -0.148 , -0.1365,\n",
       "       -0.1245,  0.3218,  0.0563,  0.245 ,  0.0132, -0.1204,  0.0391,\n",
       "       -0.1132, -0.0707,  0.2167, -0.0322, -0.1085,  0.0054],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2vec[\"january\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"../resolved_wiki2vec.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " f = open(\"../junk/failed_words\", \"wb+\") \n",
    "pickle.dump([failed, words], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = resolved.keys()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "embed = extractUSEEmbeddings([\"orange\", \"apple\", \"elon musk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "glove_file = datapath('/data/Vivek/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"/data/Vivek/glove_tmp\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(success)\n",
    "words = ['_unk_']\n",
    "idx = 1\n",
    "word2idx = {\"_unk_\": 0}\n",
    "vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "with open(embeddings_file, 'r') as f:\n",
    "    for l in f:\n",
    "        line = [a[::-1] for a in l[::-1].split(\" \", 300)[::-1]]\n",
    "        word, vector = line[0], [float(s) for s in line[1:]]\n",
    "        if len(vector) != 300:\n",
    "            print (len(vector))\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        words.append(word)\n",
    "        vectors.append(np.array(vector).astype(np.float))\n",
    "        word2idx[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = bcolz.carray(np.random.rand(1, 300), rootdir=embeddings_folder, mode='w')\n",
    "v.append(np.resize(np.array(vector), (1, 300)).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89821\n"
     ]
    }
   ],
   "source": [
    "true_file = open(\"../files/dataset/dataset_t.tsv\", \"r\").read().split(\"\\n\")\n",
    "false_file = open(\"../files/dataset/dataset_f.tsv\", \"r\").read().split(\"\\n\")\n",
    "print (len(false_file))\n",
    "false_file = false_file[:int(len(false_file) * float(dbpedia_neg/100))]\n",
    "\n",
    "custom_train = true_file[:int(0.9*len(true_file))] + false_file[:int(0.9*len(false_file))]\n",
    "custom_test = true_file[int(0.9*len(true_file)):] + false_file[int(0.9*len(false_file)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18969, 2109, 13473)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_train), len(custom_test), len(false_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = \"../junk/Glove.dat/\"\n",
    "emb, w2i = load_embeddings_from_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = \"../junk/Glove_lite.dat/\"\n",
    "emb2, w2i2 = load_embeddings_from_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
