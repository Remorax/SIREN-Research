{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsddb3 import btopen\n",
    "import bcolz, pickle\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "prefix = \"../junk/Files/temp_threshold_3_4/temp\"\n",
    "dataset_file = \"../junk/temp_dataset.tsv\"\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat\"\n",
    "embeddings_file = \"/Users/vivek/SIREN-Research/Archive-LSTM/glove.6B/glove.6B.300d.txt\"\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "\n",
    "# relations = [\"hypernym\", \"hyponym\", \"synonym\", \"none\"]\n",
    "relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id2Entity(db, entity_id):\n",
    "    entity_id = str(entity_id).encode(\"utf-8\")\n",
    "    return db[entity_id].decode(\"utf-8\")\n",
    "\n",
    "def entity2ID(db, entity):\n",
    "    entity = entity.encode(\"utf-8\")\n",
    "    if entity in db:\n",
    "        return int(db[entity])\n",
    "    return -1\n",
    "\n",
    "def extractPaths(db, x, y):\n",
    "    key = (str(x) + '_' + str(y)).encode(\"utf-8\")\n",
    "    try:\n",
    "        relation = db[key].decode(\"utf-8\")\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = {w: vectors[word2idx[w]] for w in words}\n",
    "    except:\n",
    "        embeddings = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "        \n",
    "\n",
    "def create_embeddings():\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word, vector = line[0], line[1:]\n",
    "            words.append(word)\n",
    "            vectors.append(np.array(vector).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    vectors = vectors.reshape((-1, 300))\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "    \n",
    "    embeddings = {w: vectors[word2idx[w]] for w in words}\n",
    "    return embeddings, word2idx\n",
    "\n",
    "word2ID_db = btopen(prefix + \"_word_to_id.db\", \"r\")\n",
    "ID2word_db = btopen(prefix + \"_id_to_word.db\", \"r\")\n",
    "path2ID_db = btopen(prefix + \"_path_to_id.db\", \"r\")\n",
    "ID2path_db = btopen(prefix + \"_id_to_path.db\", \"r\")\n",
    "relations_db = btopen(prefix + \"_word_occurence_map.db\", \"r\")\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(dataset_file).read().split(\"\\n\")}\n",
    "\n",
    "mappingDict = {key: idx for (idx,key) in enumerate(relations)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3), (5904, 2, 4, 4)): 1, ((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3)): 3}, {}, {}, {}, {}, {}]\n",
      "Pairs without paths: 5 , all dataset: 6\n"
     ]
    }
   ],
   "source": [
    "arrowHeads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def extractDirection(edge):\n",
    "    \n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrowHeads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrowHeads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "    \n",
    "def parsePath(path):\n",
    "    parsedPath = []\n",
    "    for edge in path.split(\"_\"):\n",
    "        direction, edge = extractDirection(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            embedding, pos, dependency = edge.split(\"/\")\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsedPath.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsedPath)\n",
    "\n",
    "def extractAllPaths(x,y):\n",
    "    \n",
    "    paths = list(extractPaths(relations_db,x,y).items()) + list(extractPaths(relations_db,y,x).items())\n",
    "    x_word, y_word = id2Entity(ID2word_db, x), id2Entity(ID2word_db, y)\n",
    "    pathCountDict = { id2Entity(ID2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    pathCountDict = { parsePath(path) : pathCountDict[path] for path in pathCountDict }\n",
    "\n",
    "    return { path : pathCountDict[path] for path in pathCountDict if path}\n",
    "    \n",
    "def parseDataset(dataset):\n",
    "    keys = [(entity2ID(word2ID_db, x), entity2ID(word2ID_db, y)) for (x, y) in dataset]\n",
    "    paths = [extractAllPaths(x,y) for (x,y) in keys]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    print('Pairs without paths:', len(empty), ', all dataset:', len(dataset))\n",
    "    embed_indices = [(embeddings.get(x,0), embeddings.get(y,0)) for (x,y) in keys]\n",
    "    return embed_indices, paths\n",
    "    \n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "\n",
    "x = parseDataset(dataset.keys())\n",
    "y = [mappingDict[relation] for relation in dataset.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
