{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsddb3 import btopen\n",
    "import bcolz, pickle\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "prefix = \"../junk/Files/temp_threshold_3_4/temp\"\n",
    "train_file = \"../junk/train.tsv\"\n",
    "test_file = \"../junk/test.tsv\"\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat\"\n",
    "embeddings_file = \"/Users/vivek/SIREN-Research/Archive-LSTM/glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_PATH = \n",
    "relations = [\"hypernym\", \"hyponym\", \"synonym\", \"none\"]\n",
    "# relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-fb0ee07b308d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity_id = str(entity_id).encode(\"utf-8\")\n",
    "    return db[entity_id].decode(\"utf-8\")\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    entity = entity.encode(\"utf-8\")\n",
    "    if entity in db:\n",
    "        return int(db[entity])\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '_' + str(y)).encode(\"utf-8\")\n",
    "    try:\n",
    "        relation = db[key].decode(\"utf-8\")\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = {w: vectors[word2idx[w]] for w in words}\n",
    "    except:\n",
    "        embeddings = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "        \n",
    "\n",
    "def create_embeddings():\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word, vector = line[0], line[1:]\n",
    "            words.append(word)\n",
    "            vectors.append(np.array(vector).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    vectors = vectors.reshape((-1, EMBEDDING_DIM))\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "    \n",
    "    embeddings = {w: vectors[word2idx[w]] for w in words}\n",
    "    return embeddings, word2idx\n",
    "\n",
    "word2id_db = btopen(prefix + \"_word_to_id.db\", \"r\")\n",
    "id2word_db = btopen(prefix + \"_id_to_word.db\", \"r\")\n",
    "path2id_db = btopen(prefix + \"_path_to_id.db\", \"r\")\n",
    "id2path_db = btopen(prefix + \"_id_to_path.db\", \"r\")\n",
    "relations_db = btopen(prefix + \"_word_occurence_map.db\", \"r\")\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs without paths: 5 , all dataset: 6\n"
     ]
    }
   ],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def extract_direction(edge):\n",
    "    \n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "    \n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"_\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            embedding, pos, dependency = edge.split(\"/\")\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsedPath)\n",
    "\n",
    "def extract_all_paths(x,y):\n",
    "    \n",
    "    paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "    x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "    y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "    path_count_dict = { id_to_entity(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    path_count_dict = { parse_path(path) : path_count_dict[path] for path in path_count_dict }\n",
    "\n",
    "    return { path : path_count_dict[path] for path in path_count_dict if path}\n",
    "    \n",
    "def parse_dataset(dataset):\n",
    "    keys = [(entity_to_id(word2id_db, x), entity_to_id(word2id_db, y)) for (x, y) in dataset]\n",
    "    paths = [extract_all_paths(x,y) for (x,y) in keys]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    print('Pairs without paths:', len(empty), ', all dataset:', len(dataset))\n",
    "    embed_indices = [(embeddings.get(x,0), embeddings.get(y,0)) for (x,y) in keys]\n",
    "    return embed_indices, paths\n",
    "    \n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values())\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [i for (i,relation) in enumerate(dataset_vals)]\n",
    "\n",
    "embed_indices_train, embed_indices_test = embed_indices[:len(train_dataset)], embed_indices[len(train_dataset):len(train_dataset)+len(test_dataset)]\n",
    "x_train, x_test = x[:len(train_dataset)], x[len(train_dataset):len(train_dataset)+len(test_dataset)]\n",
    "y_train, y_test = y[:len(train_dataset)], y[len(train_dataset):len(train_dataset)+len(test_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.hidden_dim = HIDDEN_DIM + 2 * EMBEDDING_DIM\n",
    "        self.input_dim = POS_DIM + DEP_DIM + EMBEDDING_DIM + DIR_DIM\n",
    "        self.W = nn.Linear(NUM_RELATIONS, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(len(embeddings), EMBEDDING_DIM)\n",
    "        self.word_embeddings.load_state_dict({'weight': embeddings})\n",
    "        self.word_embeddings.require_grad = False\n",
    "        \n",
    "        self.pos_embeddings = nn.Embedding(len(pos_indexer), POS_DIM)\n",
    "        self.dep_embeddings = nn.Embedding(len(dep_indexer), DEP_DIM)\n",
    "        self.dir_embeddings = nn.Embedding(len(dir_indexer), DIR_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, NUM_LAYERS)\n",
    "    \n",
    "    def embed_path(self, elem):\n",
    "        path, count = elem\n",
    "        if path in self.cache:\n",
    "            return cache[path] * count\n",
    "        \n",
    "        word_embed = self.dropout_layer(self.word_embeddings(elem[0]))\n",
    "        pos_embed = self.dropout_layer(self.pos_embeddings(elem[1]))\n",
    "        dep_embed = self.dropout_layer(self.dep_embeddings(elem[2]))\n",
    "        dir_embed = self.dropout_layer(self.dir_embeddings(elem[2]))\n",
    "        \n",
    "        embeds = np.concatenate((word_embed, pos_embed, dep_embed, dir_embed))\n",
    "        output, _ = self.lstm(embeds)\n",
    "        cache[path] = output\n",
    "\n",
    "        return output * count\n",
    "    \n",
    "    def forward(self, data, emb_indexer):\n",
    "        if not data:\n",
    "            data[NULL_PATH] = 1\n",
    "        \n",
    "        num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        path_embeddings = [np.sum([self.embed_path(path) for path in paths.items()]) for paths in data]\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        h = [np.concatenate((self.word_embeddings(elem[0]), h[i], self.word_embeddings(elem[1]))) for i,emb in enumerate(emb_indexer)]\n",
    "        return self.softmax(self.W(h))\n",
    "\n",
    "HIDDEN_DIM = 60\n",
    "NUM_LAYERS = 2\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "batch_size = min(batch_size, dataset_size)\n",
    "num_batches = int(ceil(dataset_size/batch_size))\n",
    "\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "lstm = LSTM()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss, epoch_idx = 0, np.random.permutation(dataset_size)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch = epoch_idx[batch_start:batch_end]\n",
    "        \n",
    "        data, labels, embeddings_idx = x_train[batch], y_train[batch], embed_indices_train[batch]\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = lstm(data, embeddings_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= dataset_size\n",
    "    print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch + 1, num_epochs, total_loss))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictedLabels = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = lstm(data)\n",
    "        print (outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictedLabels.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3), (5904, 2, 4, 4)): 1,\n",
       "  ((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3)): 3},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda a, b: a + b, iter(x[0].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.5       , 1.        ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dynet as dy\n",
    "dy.esum([dy.inputTensor([1,2,3]),dy.inputTensor([3,4,3])]).value()\n",
    "np.divide([1,2,3], [3,4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
