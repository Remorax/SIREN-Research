{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickledb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "train_file = \"../files/dataset/pizza_train.tsv\"\n",
    "test_file =  \"../files/dataset/pizza_test.tsv\"\n",
    "# instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/pizza_knockedout.tsv'\n",
    "\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/Pizza/temp/pizza_threshold_7_10/\"\n",
    "\n",
    "USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "model = hub.load(USE_link)\n",
    "\n",
    "f = open(\"../junk/resolved_use_unbracketed.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def to_list(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, tuple):\n",
    "            yield list(to_list(item))\n",
    "        elif isinstance(item, list):\n",
    "            yield [list(to_list(elem)) for elem in item]\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer[embedding], pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = [entity_to_id(word2id_db, elem) for elem in tup]\n",
    "    paths_x, paths_y = list(extract_paths(relations_db,x,y).items()), list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict_x = { id_to_path(id2path_db, path).replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { id_to_path(id2path_db, path).replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def get_instance_key(tup):\n",
    "    return tuple([\" \".join([tok.text for tok in nlp(elem)]) for elem in tup])\n",
    "\n",
    "def parse_instance(tup):\n",
    "    \n",
    "    paths_x = list(instances_db.get(get_instance_key(tup), {}).items())\n",
    "    paths_y = list(instances_db.get(get_instance_key(tup[::-1]), {}).items())\n",
    "    path_count_dict_x = { path.replace(\"X/\", tup[0]+\"/\").replace(\"Y/\", tup[1]+\"/\") : freq for (path, freq) in paths_x }\n",
    "    path_count_dict_y = { path.replace(\"Y/\", tup[0]+\"/\").replace(\"X/\", tup[1]+\"/\") : freq for (path, freq) in paths_y }\n",
    "    path_count_dict = {**path_count_dict_x, **path_count_dict_y}\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_instance_dataset(dataset):\n",
    "    parsed_dicts = [parse_instance(tup) for tup in dataset.keys()]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    counts = [list(path_dict.values()) for path_dict in paths]\n",
    "    paths = [list(path_dict.keys()) for path_dict in paths]\n",
    "    targets = [rel_indexer[relation] for relation in dataset.values()]\n",
    "    return list(to_list(paths)), counts, targets\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, failed\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "    closest_entity = resolved.get(entity, \"\")\n",
    "    if closest_entity and closest_entity[0] and float(closest_entity[1]) > threshold:\n",
    "        success.append(entity)\n",
    "        return int(db[closest_entity[0]])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "word2id_db = pickle.load(open(prefix + \"w2i.db\", \"rb\"))\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Instance DB\n",
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path, maxlen):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if maxlen >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks, maxlen):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            filtered = [stringifyFilterPath(path, maxlen) for path in appendingElem]\n",
    "            paths.extend(filtered)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "doc = open(\"../files/dataset/security4.txt\").read()\n",
    "all_nounchunks = list(nlp(doc).noun_chunks).copy()\n",
    "\n",
    "sentences = [list(nlp(nlp(sent.text)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \")).sents)[0]\n",
    "             for sent in nlp(doc).sents]\n",
    "# [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "all_deps = []\n",
    "instances_db = {}\n",
    "for sentence in sentences:\n",
    "    noun_chunks = [n for n in all_nounchunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "    noun_chunks = list(nlp(sentence.text).noun_chunks)\n",
    "    dependencies = getDependencyPaths(sentence, nlp, noun_chunks, 10)\n",
    "    for dep in dependencies:\n",
    "        if not dep:\n",
    "            continue\n",
    "        key = tuple([preprocess_word(nlp(word)) for word in dep[:2]])\n",
    "        path = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in dep[-1].split(\"/\")])\n",
    "        if key not in instances_db:\n",
    "            instances_db[key] = [path]\n",
    "        else:\n",
    "            instances_db[key].append(path)\n",
    "instances_db = {key: Counter(instances_db[key]) for key in instances_db}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "0 35498\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "122 35376\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "148 35350\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "579 34919\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "735 34763\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "896 34602\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1048 34450\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1292 34206\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1515 33983\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1698 33800\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "1912 33586\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2139 33359\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2299 33199\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2648 32850\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "2917 32581\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3186 32312\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3460 32038\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "3871 31627\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4142 31356\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4454 31044\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "4730 30768\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "5091 30407\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "5462 30036\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "6663 28835\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7047 28451\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7331 28167\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7564 27934\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "7882 27616\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8168 27330\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8464 27034\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "8854 26644\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9129 26369\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9384 26114\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "9633 25865\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10103 25395\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10506 24992\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10581 24917\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "10642 24856\n",
      "Train len: 10739, Test len: 1197, Instance len: 275, Knocked len: 5538\n",
      "12648 22850\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# thresholds = [0.5, 0.59, 0.6, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
    "\n",
    "# for threshold in thresholds:\n",
    "threshold = 0.86\n",
    "    \n",
    "failed, success = [], []\n",
    "\n",
    "emb_indexer, pos_indexer, dep_indexer, dir_indexer = [defaultdict(count(0).__next__) for i in range(4)]\n",
    "unk_emb, unk_pos, unk_dep, unk_dir = emb_indexer[\"<UNK>\"], pos_indexer[\"<UNK>\"], dep_indexer[\"<UNK>\"], dir_indexer[\"<UNK>\"]\n",
    "rel_indexer = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "paths_train, counts_train, targets_train = parse_dataset(train_dataset)\n",
    "paths_test, counts_test, targets_test  = parse_dataset(test_dataset)\n",
    "paths_instances, counts_instances, targets_instances  = parse_instance_dataset(test_instances)\n",
    "paths_knocked, counts_knocked, targets_knocked  = parse_dataset(test_knocked)\n",
    "\n",
    "nodes_train = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in train_dataset]\n",
    "nodes_test = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_dataset]\n",
    "nodes_instances = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_instances]\n",
    "nodes_knocked = [[emb_indexer[tup[0]], emb_indexer[tup[1]]] for tup in test_knocked]\n",
    "\n",
    "print (\"Train len: {}, Test len: {}, Instance len: {}, Knocked len: {}\".format(len(paths_train), len(paths_test),  len(paths_instances), len(paths_knocked)))\n",
    "print (len(failed), len(success))\n",
    "emb_indexer_inv = {emb_indexer[key]: key for key in emb_indexer}\n",
    "embeds = extractUSEEmbeddings(list(emb_indexer.keys())[1:])\n",
    "emb_vals = np.array(np.zeros((1, embeds.shape[1])).tolist() + embeds.tolist())\n",
    "\n",
    "\n",
    "output_file = \"../Input/data_instances_sample.pkl\"\n",
    "f = open(output_file, \"wb+\")\n",
    "pickle.dump([nodes_train, paths_train, counts_train, targets_train, \n",
    "             nodes_test, paths_test, counts_test, targets_test,\n",
    "             nodes_instances, paths_instances, counts_instances, targets_instances,\n",
    "             nodes_knocked, paths_knocked, counts_knocked, targets_knocked,\n",
    "             emb_indexer, emb_indexer_inv, emb_vals, \n",
    "             pos_indexer, dep_indexer, dir_indexer, rel_indexer], f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Instances from a document\n",
    "\n",
    "import glob,  en_core_web_lg\n",
    "import spacy, neuralcoref, itertools\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "def preprocess(noun_chunks):\n",
    "    all_parsed_chunks = []\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    for np in noun_chunks:\n",
    "        start_index = [i for i,token in enumerate(np) if token.pos_ not in filt_tokens][0]\n",
    "        np_filt = np[start_index:].text\n",
    "        if \"(\" not in np_filt and \")\" in np_filt:\n",
    "            np_filt = np_filt.replace(\")\", \"\")\n",
    "        elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "            np_filt = np_filt.replace(\"(\", \"\")\n",
    "        all_parsed_chunks.append(np_filt)\n",
    "    return list(set(all_parsed_chunks))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model, for coreference resolution\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "nlp.tokenizer.add_special_case('Inc.', [{ORTH: 'Inc', LEMMA: 'Incorporated'}])\n",
    "\n",
    "for i,file in enumerate(sorted(glob.glob(\"../files/dataset/security*\"))):\n",
    "    paras = [t.text for t in list(nlp(open(file).read()).sents)]\n",
    "    paras = [nlp(para)._.coref_resolved.replace(\"\\n\", \" \").replace(\"  \", \" \") for para in paras]\n",
    "    instances = [preprocess(nlp(para).noun_chunks) for para in paras]\n",
    "    instances_pairs = []\n",
    "    for instances_sent in instances:\n",
    "        instances_pairs.extend(list(set(list(itertools.combinations(instances_sent, 2)))))\n",
    "\n",
    "    instances_pairs = [\"\\t\".join(list(pair) + [\"none\"]) for pair in instances_pairs if pair]\n",
    "\n",
    "    open(\"../files/dataset/instances\" + str(i) + \".tsv\", \"w+\").write(\"\\n\".join(instances_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "db type is dbm.gnu, but the module is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-50d6e68da899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../junk/db_files/pizza_term_to_id.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise error[0](\"db type is {0}, but the module is not \"\n\u001b[0;32m---> 91\u001b[0;31m                        \"available\".format(result))\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: db type is dbm.gnu, but the module is not available"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "shelve.open(\"../junk/db_files/pizza_term_to_id.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'': 1539640,\n",
       " b'0': 91,\n",
       " b'0 00': 93,\n",
       " b'0 00000214': 94,\n",
       " b'0 00008 people': 95,\n",
       " b'0 0001 0 0003 millimeters': 96,\n",
       " b'0 00014 pbq hr': 97,\n",
       " b'0 0002': 98,\n",
       " b'0 0004': 99,\n",
       " b'00 00 gmt': 100,\n",
       " b'00 00 utc': 101,\n",
       " b'00 01': 102,\n",
       " b'0 0012 cadmium': 103,\n",
       " b'0 0015': 104,\n",
       " b'0 001 g': 105,\n",
       " b'0 001 r s': 106,\n",
       " b'0 002': 109,\n",
       " b'0 002 0 01': 110,\n",
       " b'0 002 2 1': 111,\n",
       " b'0 002 inches': 112,\n",
       " b'0 002 ppb': 113,\n",
       " b'0 003': 114,\n",
       " b'0 004 ppb': 115,\n",
       " b'0 0050 0 0059': 116,\n",
       " b'0 0054': 117,\n",
       " b'0 005 body weight': 118,\n",
       " b'0 005 lead': 119,\n",
       " b'0 00625': 120,\n",
       " b'0 008 cfs': 121,\n",
       " b'0 008 g gj pm': 122,\n",
       " b'0 009 g': 123,\n",
       " b'000 kg': 124,\n",
       " b'000 to 0 rods': 125,\n",
       " b'0 01': 126,\n",
       " b'0 010': 127,\n",
       " b'0 01 0 02 cm sufficient distance': 128,\n",
       " b'0 010 inches': 129,\n",
       " b'0 01 1 ppm': 130,\n",
       " b'0 012': 131,\n",
       " b'0 013 g gj pm': 132,\n",
       " b'0 0179 w mk': 133,\n",
       " b'0 01 jains': 134,\n",
       " b'0 01 mg': 135,\n",
       " b'0 01 mg m': 136,\n",
       " b'0 01 pacific islander': 137,\n",
       " b'0 01 ppb': 138,\n",
       " b'0 01 ppm': 139,\n",
       " b'0 01 right angle': 140,\n",
       " b'0 01 seconds': 141,\n",
       " b'0 01 sikhs': 142,\n",
       " b'0 01 to 0 25 uranium oxides': 143,\n",
       " b'0 02': 144,\n",
       " b'0 02 0 025': 145,\n",
       " b'0 02 0 03': 146,\n",
       " b'0 02 0 03 fluorine': 147,\n",
       " b'0 02 0 04': 148,\n",
       " b'0 02 0 04 kg': 149,\n",
       " b'0 021': 150,\n",
       " b'0 022': 151,\n",
       " b'0 022 w m k': 152,\n",
       " b'0 024 g': 153,\n",
       " b'0 025': 154,\n",
       " b'0 025 tin': 155,\n",
       " b'0 026 w mk': 156,\n",
       " b'0 029 tin': 157,\n",
       " b'0 02 micrometres': 158,\n",
       " b'0 02 to 0 05 c': 159,\n",
       " b'0 02 uranium': 160,\n",
       " b'003': 161,\n",
       " b'0 03': 162,\n",
       " b'0 030 0 034': 163,\n",
       " b'0 039': 164,\n",
       " b'0 039 w m k': 165,\n",
       " b'003a': 166,\n",
       " b'0 03 buddhists': 167,\n",
       " b'0 03 inch width': 168,\n",
       " b'0 03 inhabitants': 169,\n",
       " b'0 03 iron': 170,\n",
       " b'0 03 others': 171,\n",
       " b'0 03 pacific islander': 172,\n",
       " b'0 04': 173,\n",
       " b'0 04 0 1': 174,\n",
       " b'0 04 0 1 sv h': 175,\n",
       " b'0 04 2 7 pairs 10 acres': 176,\n",
       " b'0 049': 177,\n",
       " b'004b': 178,\n",
       " b'0 04 buddhists': 179,\n",
       " b'0 04 mm year': 180,\n",
       " b'0 04 pacific islander': 181,\n",
       " b'0 05': 183,\n",
       " b'0 05 0 09 chlorine': 184,\n",
       " b'0 051 iron': 185,\n",
       " b'0 05 euro': 186,\n",
       " b'0 05 inches': 187,\n",
       " b'0 05 mg kg day': 188,\n",
       " b'0 05 milligrams': 189,\n",
       " b'0 05 ounces': 190,\n",
       " b'0 05 pacific islander': 191,\n",
       " b'0 05 phosphorus': 192,\n",
       " b'0 05 sikhs': 193,\n",
       " b'0 06': 194,\n",
       " b'0 06 calcium': 195,\n",
       " b'0 06 inch length': 196,\n",
       " b'0 06 second': 197,\n",
       " b'0 07': 198,\n",
       " b'0 074 nmi': 199,\n",
       " b'0 078 g': 200,\n",
       " b'0 07 buddhist': 201,\n",
       " b'0 07 fishing': 202,\n",
       " b'0 07 pacific islander': 203,\n",
       " b'007 series': 204,\n",
       " b'0 08': 205,\n",
       " b'0 0810 aluminum': 206,\n",
       " b'0 083 percent': 207,\n",
       " b'0 088': 208,\n",
       " b'0 08 cobalt content': 209,\n",
       " b'0 08 inches': 210,\n",
       " b'0 08 uranium': 211,\n",
       " b'0 09': 212,\n",
       " b'0 09 0 3 sulfur': 213,\n",
       " b'0 092 g saturated fat': 214,\n",
       " b'0 093': 215,\n",
       " b'0 093 m': 216,\n",
       " b'0 09 to 0 13 degrees c': 217,\n",
       " b'00 buckshot': 218,\n",
       " b'0 0 children': 219,\n",
       " b'00 low gluten flour': 220,\n",
       " b'00 n': 221,\n",
       " b'0 0 pacific islander': 222,\n",
       " b'0 1': 227,\n",
       " b'1': 16114,\n",
       " b'0 10': 230,\n",
       " b'0 1000 m asl': 231,\n",
       " b'0 100 meters': 232,\n",
       " b'0 1 0 15': 233,\n",
       " b'0 1 0 15 g': 234,\n",
       " b'01 07 2011': 236,\n",
       " b'0 109 sv h': 237,\n",
       " b'0 10 ash': 238,\n",
       " b'0 10 coins': 239,\n",
       " b'0 10 grain': 240,\n",
       " b'0 10 kwh': 241,\n",
       " b'0 10 native american': 242,\n",
       " b'0 10 ounce': 243,\n",
       " b'0 10 v analogue interface': 244,\n",
       " b'0 11': 245,\n",
       " b'0 1 1 0': 246,\n",
       " b'0 1 100 g ml': 247,\n",
       " b'0 11 0 14ct': 248,\n",
       " b'0 11 degree': 249,\n",
       " b'0 11 fat': 250,\n",
       " b'0 11 ma': 251,\n",
       " b'0 11 uranium': 252,\n",
       " b'0 12': 254,\n",
       " b'01 23 local time': 255,\n",
       " b'0 1250 ad': 256,\n",
       " b'0 1250 ce': 257,\n",
       " b'0 12 g': 258,\n",
       " b'0 12 rapes': 259,\n",
       " b'0 13': 260,\n",
       " b'0 130 inch nominal depth': 261,\n",
       " b'0 13 g': 262,\n",
       " b'0 13 mm': 263,\n",
       " b'013 music venue': 264,\n",
       " b'0 13 percent': 265,\n",
       " b'0 1 4': 266,\n",
       " b'0 14': 268,\n",
       " b'0 14 0 10 kwh': 269,\n",
       " b'014 acapulcoite': 270,\n",
       " b'0 14 becquerel': 271,\n",
       " b'0 1 4 g m': 272,\n",
       " b'0 14 tbq': 273,\n",
       " b'0 14 to 0 4 a': 274,\n",
       " b'0 14 years': 275,\n",
       " b'0 15': 276,\n",
       " b'0 15 0 3': 277,\n",
       " b'0 150 mg m benzene soluble fraction': 278,\n",
       " b'0 1550 square inches': 279,\n",
       " b'0 15 km': 280,\n",
       " b'0 15 kwh': 281,\n",
       " b'0 15 m min': 282,\n",
       " b'0 15 percent': 283,\n",
       " b'0 16': 284,\n",
       " b'0 16 inches': 286,\n",
       " b'0 16 in diameter': 287,\n",
       " b'0 16 km': 288,\n",
       " b'0 16 mg g mushroom': 289,\n",
       " b'0 16 percent': 290,\n",
       " b'0 16 phosphorus': 291,\n",
       " b'0 16 sq mi': 292,\n",
       " b'017': 293,\n",
       " b'0 17': 294,\n",
       " b'0 17 hectare': 295,\n",
       " b'0 17 jains': 296,\n",
       " b'0 17 native american': 297,\n",
       " b'0 18': 298,\n",
       " b'0 183': 299,\n",
       " b'0 188 kilotons': 300,\n",
       " b'0 18 to 0 59 meters': 301,\n",
       " b'0 19': 303,\n",
       " b'0 1 africa': 304,\n",
       " b'0 1 agnostic': 305,\n",
       " b'0 1 buddhist': 306,\n",
       " b'0 1 c': 307,\n",
       " b'0 1 chinese': 308,\n",
       " b'0 1 chloride': 309,\n",
       " b'0 1 degree': 310,\n",
       " b'0 1 f': 311,\n",
       " b'0 1 fat': 312,\n",
       " b'0 1 g ml': 313,\n",
       " b'0 1 gypsy': 314,\n",
       " b'0 1 hectare': 315,\n",
       " b'0 1 hindu': 316,\n",
       " b'0 1 kilograms': 317,\n",
       " b'0 1 m': 337,\n",
       " b'0 1 mg': 319,\n",
       " b'0 1 mg l': 320,\n",
       " b'0 1 micro sieverts': 321,\n",
       " b'0 1 millimeter': 322,\n",
       " b'0 1 mm': 323,\n",
       " b'0 1 native hawaiian': 324,\n",
       " b'0 1 ng i teq m3 modern incinerators': 325,\n",
       " b'0 1 oceania': 326,\n",
       " b'0 1 pacific islander': 327,\n",
       " b'0 1 percent': 328,\n",
       " b'0 1 percent sikh': 329,\n",
       " b'0 1 rems': 330,\n",
       " b'0 1 republic': 331,\n",
       " b'0 1 sikh': 332,\n",
       " b'0 1 t coal': 333,\n",
       " b'0 1 to 0 4 percent': 334,\n",
       " b'0 1 to 0 8 c': 335,\n",
       " b'0 1 water': 336,\n",
       " b'0 2': 341,\n",
       " b'0 20': 345,\n",
       " b'02 00 local addis ababa time': 346,\n",
       " b'02 03': 347,\n",
       " b'0 2 0 3 mm': 348,\n",
       " b'0 2 0 55': 349,\n",
       " b'0 2 0 55 phosphorus': 350,\n",
       " b'0 20 feet': 351,\n",
       " b'0 20 kwh': 352,\n",
       " b'0 20 native american': 353,\n",
       " b'0 2 1': 354,\n",
       " b'0 21': 355,\n",
       " b'0 2 1 0': 356,\n",
       " b'0 2 1 5 mg kg week': 357,\n",
       " b'0 21 becquerel': 358,\n",
       " b'02 1 joint military exercises': 359,\n",
       " b'0 22': 360,\n",
       " b'0 23': 361,\n",
       " b'0 23 0 15 kwh': 362,\n",
       " b'02 3 46': 363,\n",
       " b'02 35': 364,\n",
       " b'0 236 inch caliber rifle barrels': 365,\n",
       " b'0 236 inches': 366,\n",
       " b'0 23 zn': 367,\n",
       " b'0 24': 369,\n",
       " b'0 244 inches': 370,\n",
       " b'0 247 inches': 371,\n",
       " b'0 24 ma': 372,\n",
       " b'0 24 native american': 373,\n",
       " b'0 24 pacific islander': 374,\n",
       " b'0 24 percent': 375,\n",
       " b'0 24 to 1 2 million cubic miles': 376,\n",
       " b'0 25': 379,\n",
       " b'0 25 0 30': 380,\n",
       " b'0 25 0 30 tonnes': 381,\n",
       " b'0 25 kg': 382,\n",
       " b'0 25 kwh': 383,\n",
       " b'0 25 m': 384,\n",
       " b'0 25 mg m': 385,\n",
       " b'0 25 native american': 386,\n",
       " b'0 25 ppm': 387,\n",
       " b'0 26': 388,\n",
       " b'0 263 g': 389,\n",
       " b'0 26 millisieverts': 390,\n",
       " b'0 27': 391,\n",
       " b'0 27 g': 392,\n",
       " b'0 27 ma': 393,\n",
       " b'0 27 tj': 394,\n",
       " b'0 28': 396,\n",
       " b'0 282': 397,\n",
       " b'0 2826': 398,\n",
       " b'0 29': 399,\n",
       " b'029 22xx xxxx': 400,\n",
       " b'0 29 73 137 speakers': 401,\n",
       " b'0 2 arcsecond sampling': 402,\n",
       " b'0 2 bcf': 403,\n",
       " b'0 2 black caribbean': 404,\n",
       " b'0 2 cadmium': 405,\n",
       " b'0 2 carbon': 406,\n",
       " b'0 2 degrees': 407,\n",
       " b'0 2 f': 408,\n",
       " b'0 2 fat': 409,\n",
       " b'0 2 gram': 410,\n",
       " b'0 2 grams': 412,\n",
       " b'0 2 iron': 413,\n",
       " b'0 2 jews': 414,\n",
       " b'0 2 litre': 415,\n",
       " b'0 2 mg m benzene soluble fraction': 416,\n",
       " b'0 2 mil': 417,\n",
       " b'0 2 milligrams': 418,\n",
       " b'0 2 mm': 419,\n",
       " b'0 2 native american': 420,\n",
       " b'0 2 offset strain': 421,\n",
       " b'0 2 pacific islander': 422,\n",
       " b'0 2 percent': 424,\n",
       " b'0 2 proof stress': 425,\n",
       " b'0 2 square degrees': 426,\n",
       " b'0 2 taoist': 427,\n",
       " b'0 2 to 0 3 grains': 428,\n",
       " b'0 2 g day': 429,\n",
       " b'0 3': 435,\n",
       " b'0 30': 436,\n",
       " b'03 00': 437,\n",
       " b'03 00 05 00': 438,\n",
       " b'03 00 pst': 439,\n",
       " b'0 3024': 440,\n",
       " b'0 30 4 30 north latitude': 441,\n",
       " b'0 3 0 8 nm': 442,\n",
       " b'0 3 0 9 microgram kwh': 443,\n",
       " b'0 30am': 444,\n",
       " b'0 30 pacific islander': 445,\n",
       " b'0 30 sv': 446,\n",
       " b'0 30 us gal': 447,\n",
       " b'0 30 yuan': 448,\n",
       " b'0 3 1': 450,\n",
       " b'0 31': 451,\n",
       " b'0 31 north latitude': 452,\n",
       " b'0 31 usd': 453,\n",
       " b'0 32': 455,\n",
       " b'0 33': 456,\n",
       " b'0 33 g': 457,\n",
       " b'0 33 litre': 458,\n",
       " b'0 33 native american': 459,\n",
       " b'0 33 percent': 460,\n",
       " b'0 34': 461,\n",
       " b'0 34 islam': 462,\n",
       " b'0 34 ma': 463,\n",
       " b'0 34 nm': 464,\n",
       " b'0 35': 465,\n",
       " b'0 35 0 48 ma': 466,\n",
       " b'0 35 francs': 467,\n",
       " b'0 36': 468,\n",
       " b'0 366': 469,\n",
       " b'0 36 ma': 470,\n",
       " b'0 37': 471,\n",
       " b'0 37 percent': 472,\n",
       " b'0 37 to 1 5 ghz': 473,\n",
       " b'0 38': 474,\n",
       " b'0 386 0 467': 475,\n",
       " b'0 38 percent': 476,\n",
       " b'0 39': 477,\n",
       " b'0 3 day': 478,\n",
       " b'0 3 fat': 479,\n",
       " b'0 3 g': 480,\n",
       " b'0 3 km': 481,\n",
       " b'0 3 m': 482,\n",
       " b'0 3 meters': 483,\n",
       " b'0 3 miles': 484,\n",
       " b'0 3 million tons': 485,\n",
       " b'0 3 mixed race': 486,\n",
       " b'0 3 muslims': 487,\n",
       " b'0 3 native american': 488,\n",
       " b'0 3 percent': 489,\n",
       " b'0 3 percent hindu': 490,\n",
       " b'0 3 pipe': 491,\n",
       " b'0 3 scandium': 492,\n",
       " b'0 3 to 0 5 mm': 493,\n",
       " b'0 3 to 0 6 solid fraction': 494,\n",
       " b'0 3 to 0 8': 495,\n",
       " b'0 3 to 1 1 m': 496,\n",
       " b'0 3 to 1 5 micrometres': 497,\n",
       " b'0 3 to 2 9 ppm': 498,\n",
       " b'0 3 to 6 ft': 499,\n",
       " b'0 3 to 8 3 mpa': 500,\n",
       " b'0 3 w m k': 501,\n",
       " b'0 4': 503,\n",
       " b'04': 504,\n",
       " b'04 00 am': 505,\n",
       " b'0 4 0 5 kg': 506,\n",
       " b'0 4 0st bon accord': 507,\n",
       " b'0 4 0st john howe': 508,\n",
       " b'0 41': 509,\n",
       " b'0 410 inches': 510,\n",
       " b'0 41 grams': 511,\n",
       " b'0 41 km': 512,\n",
       " b'0 41 sq m': 513,\n",
       " b'0 41 sucrose': 514,\n",
       " b'042': 515,\n",
       " b'04 26': 516,\n",
       " b'0 42 cdn': 517,\n",
       " b'0 43': 518,\n",
       " b'04 30 pm': 519,\n",
       " b'0 437 megacars': 520,\n",
       " b'0 44': 521,\n",
       " b'0 44 12': 522,\n",
       " b'0 44 msv': 523,\n",
       " b'0 4 4 t dunrobin': 524,\n",
       " b'0 45': 525,\n",
       " b'0 45 1 06': 526,\n",
       " b'0 45 kg paper bag': 527,\n",
       " b'0 45 manganese': 528,\n",
       " b'0 45 mya': 529,\n",
       " b'0 46': 530,\n",
       " b'0 46 0 51': 531,\n",
       " b'0 46 m': 532,\n",
       " b'0 46 ma': 533,\n",
       " b'0 46 mi': 534,\n",
       " b'0 46 mya': 535,\n",
       " b'0 47': 537,\n",
       " b'0 47 metric tons': 538,\n",
       " b'0 47 mtpa': 539,\n",
       " b'0 48': 540,\n",
       " b'0 49': 541,\n",
       " b'0 49 millimeter': 542,\n",
       " b'0 49 to 0 79': 543,\n",
       " b'0 4 bangladeshi': 544,\n",
       " b'0 4 billion cubic feet': 545,\n",
       " b'04e1': 546,\n",
       " b'0 4 fat': 547,\n",
       " b'0 4 g': 548,\n",
       " b'0 4 hectares': 549,\n",
       " b'0 4 hussite': 550,\n",
       " b'0 4 kwh': 551,\n",
       " b'0 4 mcg': 552,\n",
       " b'0 4 million other vehicles': 553,\n",
       " b'0 4 nanograms l': 554,\n",
       " b'0 4 native american': 555,\n",
       " b'0 4 native americans': 556,\n",
       " b'0 4 nmi': 557,\n",
       " b'0 4 percent': 558,\n",
       " b'0 4 percent american indian': 559,\n",
       " b'0 4 slope': 560,\n",
       " b'0 4 sq mile site': 561,\n",
       " b'0 4 to 0 5 hectares': 562,\n",
       " b'0 4 to 3 7 bq kg': 563,\n",
       " b'04 w': 564,\n",
       " b'0 4 white and black african': 565,\n",
       " b'0 4 years': 566,\n",
       " b'0 4 g m': 567,\n",
       " b'0 5': 570,\n",
       " b'0 50': 571,\n",
       " b'05 00 07 00': 572,\n",
       " b'0 5 0 75 inches': 573,\n",
       " b'0 5 0 8 tons acre': 574,\n",
       " b'050 duration': 575,\n",
       " b'0 50 g m': 576,\n",
       " b'050 lift': 577,\n",
       " b'0 50 molybdenum': 578,\n",
       " b'0 50 pounds': 579,\n",
       " b'0 5 1': 580,\n",
       " b'0 51': 581,\n",
       " b'0 517 inch rim': 582,\n",
       " b'0 518 hdi value': 583,\n",
       " b'0 5 2 5': 584,\n",
       " b'0 526': 585,\n",
       " b'0 527': 586,\n",
       " b'0 52 carbon': 587,\n",
       " b'0 52 to 0 22 million year old dacite and andesite flows': 588,\n",
       " b'0 53': 589,\n",
       " b'0 537 medium': 590,\n",
       " b'053 inch': 591,\n",
       " b'0 53 west': 592,\n",
       " b'0 54': 593,\n",
       " b'0 55': 595,\n",
       " b'0555 x': 596,\n",
       " b'0 55 c': 597,\n",
       " b'0 55 gj': 598,\n",
       " b'0 55 ma': 599,\n",
       " b'0 55 m min': 600,\n",
       " b'0 56': 601,\n",
       " b'0 56 5 4': 602,\n",
       " b'0 56 5 4 lauric acid': 603,\n",
       " b'0 56 km': 604,\n",
       " b'0 56 percent': 605,\n",
       " b'0 57': 606,\n",
       " b'0 58': 607,\n",
       " b'0 581': 608,\n",
       " b'0 58 g': 609,\n",
       " b'0 58m 0 97': 610,\n",
       " b'0 59': 612,\n",
       " b'05 99 west longitude': 613,\n",
       " b'0 5 agriculture': 614,\n",
       " b'0 5 alcohol': 615,\n",
       " b'0 5 and 1 0 millimeters': 616,\n",
       " b'0 5 asia': 617,\n",
       " b'0 5 chromium': 618,\n",
       " b'0 5 cm': 619,\n",
       " b'0 5 czech brethren': 620,\n",
       " b'0 5 degrees': 621,\n",
       " b'0 5 g': 622,\n",
       " b'0 5 gram': 623,\n",
       " b'0 5 guar gum': 624,\n",
       " b'0 5 inch diameter': 625,\n",
       " b'05 jpg thumb residence': 626,\n",
       " b'0 5 kg': 627,\n",
       " b'0 5 lbs gallon': 628,\n",
       " b'0 5 litre': 629,\n",
       " b'0 5 m': 632,\n",
       " b'0 5 mg m time weighted average': 633,\n",
       " b'0 5 micrometers': 634,\n",
       " b'0 5 million teu': 635,\n",
       " b'0 5 million tonnes': 636,\n",
       " b'0 5 mixed race': 637,\n",
       " b'0 5 mm': 638,\n",
       " b'0 5 mm cycle': 639,\n",
       " b'0 5 mormon': 640,\n",
       " b'0 5 mya': 641,\n",
       " b'0 5 native american': 642,\n",
       " b'0 5 n fertilizer nitrogen applications': 643,\n",
       " b'0 5 other european countries': 644,\n",
       " b'0 5 ounces': 645,\n",
       " b'0 5 percent': 646,\n",
       " b'0 5 percent accuracy': 647,\n",
       " b'0 5 percent baptist': 648,\n",
       " b'0 5 protein': 649,\n",
       " b'05 ref': 650,\n",
       " b'0 5 repeatability': 651,\n",
       " b'0 5 rmb': 652,\n",
       " b'0 5 sq mi': 653,\n",
       " b'0 5 to 0 75 mm': 654,\n",
       " b'0 5 to 1': 656,\n",
       " b'0 5 to 1 parts': 657,\n",
       " b'0 5 to 21 8 ppm': 658,\n",
       " b'0 5 to 2 7 ppm': 659,\n",
       " b'0 5 to 3': 660,\n",
       " b'0 5v': 661,\n",
       " b'0 5 v': 662,\n",
       " b'0 5 vegan': 663,\n",
       " b'0 5 w': 664,\n",
       " b'0 5 white and black caribbean': 665,\n",
       " b'0 5 white irish': 666,\n",
       " b'0 5 year centimetre': 667,\n",
       " b'0 5 s': 668,\n",
       " b'0 6': 672,\n",
       " b'0 60': 673,\n",
       " b'0 6069': 674,\n",
       " b'0 6069 nm': 675,\n",
       " b'0 6 0 7 ton': 676,\n",
       " b'0 60 grams': 677,\n",
       " b'0 60 silicon': 678,\n",
       " b'0 61': 679,\n",
       " b'0 61 m': 680,\n",
       " b'0 61 mg 100 g': 681,\n",
       " b'0 62': 683,\n",
       " b'0 63': 684,\n",
       " b'0 63 0 66': 685,\n",
       " b'0 630 hdi value': 686,\n",
       " b'0 63 km2': 687,\n",
       " b'0 63 square kilometers': 688,\n",
       " b'0 64': 690,\n",
       " b'0 6 400 240 mpa': 691,\n",
       " b'06 43 monday': 692,\n",
       " b'0 65': 693,\n",
       " b'0 65 euro': 694,\n",
       " b'0 65 g': 695,\n",
       " b'0 65 mining': 696,\n",
       " b'0 65 percent': 697,\n",
       " b'0 66': 698,\n",
       " b'0 66 agriculture': 699,\n",
       " b'0 66 fewer days': 700,\n",
       " b'0 67': 701,\n",
       " b'0 67 g fat 1': 702,\n",
       " b'0 67 native american': 703,\n",
       " b'0 67 to 0 86 mg 100 g': 704,\n",
       " b'0 68': 706,\n",
       " b'0 68 ladin': 707,\n",
       " b'0 69': 708,\n",
       " b'0 69 percent': 709,\n",
       " b'0 6 and 2 feet': 710,\n",
       " b'0 6 and 3 km': 711,\n",
       " b'0 6 bangladeshi': 712,\n",
       " b'0 6 c': 713,\n",
       " b'0 6 copper': 714,\n",
       " b'0 6 degrees': 715,\n",
       " b'0 6 energy supply': 716,\n",
       " b'0 6 growth': 717,\n",
       " b'0 6 ha': 718,\n",
       " b'0 6 hindu': 719,\n",
       " b'0 6 km': 720,\n",
       " b'0 6 l': 721,\n",
       " b'0 6 m': 723,\n",
       " b'0 6 mg m': 724,\n",
       " b'0 6 mile': 725,\n",
       " b'0 6 muslims': 726,\n",
       " b'0 6 others': 727,\n",
       " b'0 6 pacific islander': 728,\n",
       " b'0 6 percent': 729,\n",
       " b'0 6 percent buddhist': 730,\n",
       " b'0 6 practice indigenous religions': 731,\n",
       " b'0 6 republic': 732,\n",
       " b'0 6 rmb': 733,\n",
       " b'0 6 seconds': 734,\n",
       " b'0 6 to 0 8 carbon': 735,\n",
       " b'0 6 tonne': 736,\n",
       " b'0 6 v': 737,\n",
       " b'0 6 years': 738,\n",
       " b'0 7': 744,\n",
       " b'0 70': 745,\n",
       " b'07 00 jst': 746,\n",
       " b'0 71': 747,\n",
       " b'0 7 1 5 hours': 748,\n",
       " b'0 72': 749,\n",
       " b'0 720 silver': 750,\n",
       " b'0 7236 mham': 751,\n",
       " b'0 73': 752,\n",
       " b'07 30 20 00': 753,\n",
       " b'0 73 ma': 754,\n",
       " b'0 74': 755,\n",
       " b'0 74 0 85': 756,\n",
       " b'0 74 0 85 ma': 757,\n",
       " b'0 74 km': 758,\n",
       " b'0 75': 759,\n",
       " b'0 75 3 5': 760,\n",
       " b'0 75 3 5 years': 761,\n",
       " b'0 75 beds': 762,\n",
       " b'0 75 fluid ounces': 763,\n",
       " b'0 75 grams': 764,\n",
       " b'0 75 inches': 765,\n",
       " b'0 75 m': 766,\n",
       " b'0 75 miles': 767,\n",
       " b'0 75 percent': 768,\n",
       " b'0 76': 769,\n",
       " b'0 76 j': 770,\n",
       " b'0 76 volts': 771,\n",
       " b'0 77': 773,\n",
       " b'0 77344 troy oz': 774,\n",
       " b'0 775': 775,\n",
       " b'0 77 carbon': 776,\n",
       " b'077 inches': 777,\n",
       " b'0 78': 778,\n",
       " b'0 79': 779,\n",
       " b'0 796 km2': 780,\n",
       " b'0 79 carbon': 781,\n",
       " b'079 inches': 782,\n",
       " b'0 7 c': 783,\n",
       " b'0 7 filipino': 784,\n",
       " b'0 7 guatemalan': 785,\n",
       " b'07 idar oberstein': 786,\n",
       " b'0 7 km': 787,\n",
       " b'0 7 m': 788,\n",
       " b'0 7 ma': 790,\n",
       " b'0 7 mln m': 791,\n",
       " b'0 7 mm cycle': 792,\n",
       " b'0 7 pacific islander': 793,\n",
       " b'0 7 percent': 795,\n",
       " b'0 7 percent united church': 796,\n",
       " b'0 7 picograms': 797,\n",
       " b'0 7 to 11 parts': 798,\n",
       " b'0 7 to 2 years': 799,\n",
       " b'0 7 to 3 3 fat': 800,\n",
       " b'0 7 uranium 235': 801,\n",
       " b'0 7 white irish': 802,\n",
       " b'0 7 zinc': 803,\n",
       " b'0 8': 807,\n",
       " b'0 80': 809,\n",
       " b'0 80 b': 810,\n",
       " b'0 80 fiber': 811,\n",
       " b'0 80 g m': 812,\n",
       " b'0 81': 813,\n",
       " b'0 8 1 0 g': 814,\n",
       " b'08 16 utc': 815,\n",
       " b'0 81 million tons': 816,\n",
       " b'0 82': 817,\n",
       " b'0 83': 818,\n",
       " b'08 34 am': 819,\n",
       " b'0 83 percent': 820,\n",
       " b'0 84': 821,\n",
       " b'0 85': 822,\n",
       " b'0 85 0 855': 823,\n",
       " b'0 852': 824,\n",
       " b'0 854': 825,\n",
       " b'0 85 fl oz': 826,\n",
       " b'0 86': 827,\n",
       " b'0 86 w': 828,\n",
       " b'0 87': 829,\n",
       " b'0 875 cents ton mile': 830,\n",
       " b'0 88': 831,\n",
       " b'0 8 800 640 mpa': 832,\n",
       " b'0 889 g': 833,\n",
       " b'0 88 pounds': 834,\n",
       " b'0 89': 835,\n",
       " b'0 8 carbon': 836,\n",
       " b'0 8 carbon content': 837,\n",
       " b'0 8 copper': 838,\n",
       " b'0 8 electricity': 839,\n",
       " b'0 8 gj odt': 840,\n",
       " b'0 8 impurities': 841,\n",
       " b'0 8 in height': 842,\n",
       " b'0 8 km': 844,\n",
       " b'0 8 m': 846,\n",
       " b'0 8 meters': 847,\n",
       " b'0 8 mi': 848,\n",
       " b'0 8 million entries': 849,\n",
       " b'0 8 million tonnes': 850,\n",
       " b'0 8 m latex beads': 851,\n",
       " b'0 8 mm': 852,\n",
       " b'0 8 m to 1 2 m a 40 m hole': 853,\n",
       " b'0 8 mw': 854,\n",
       " b'0 8 mya': 855,\n",
       " b'08 n': 856,\n",
       " b'0 8 per 100 000 people': 857,\n",
       " b'0 8 percent': 858,\n",
       " b'0 8 times ultimate tensile strength': 859,\n",
       " b'0 8 to 1 0 kg': 860,\n",
       " b'0 8 to 1 4 cm': 861,\n",
       " b'0 8 to 2 0 tons': 862,\n",
       " b'0 8 tonnes': 863,\n",
       " b'0 8v': 864,\n",
       " b'0 9': 868,\n",
       " b'0 90': 870,\n",
       " b'0 900 fine': 871,\n",
       " b'0 900 fine gold': 872,\n",
       " b'0 900 silver': 873,\n",
       " b'09 00 utc': 874,\n",
       " b'0 90 1 20': 876,\n",
       " b'0 9 0 95 m long': 877,\n",
       " b'0 90 degrees': 878,\n",
       " b'0 90 euro': 879,\n",
       " b'0 91': 881,\n",
       " b'0 9 1 0': 882,\n",
       " b'0 911 g cm': 883,\n",
       " b'0 916 g cm': 884,\n",
       " b'0 917': 885,\n",
       " b'0 918': 886,\n",
       " b'0 91 mi': 887,\n",
       " b'0 91 m tall mechanical wine server': 888,\n",
       " b'0 91 pounds': 889,\n",
       " b'0 92': 890,\n",
       " b'0 92 metal': 891,\n",
       " b'0 92 ounces': 892,\n",
       " b'0 92 utilities': 893,\n",
       " b'0 93': 895,\n",
       " b'0 93 and 1 59 m': 896,\n",
       " b'0 93 gm': 897,\n",
       " b'0 94': 898,\n",
       " b'0 940 pbq': 899,\n",
       " b'0 946 g cm': 900,\n",
       " b'0 9486 kg m': 901,\n",
       " b'0 94 pbq': 902,\n",
       " b'0 95 1': 903,\n",
       " b'0 95 cubic feet': 904,\n",
       " b'0 95 males': 905,\n",
       " b'0 96': 906,\n",
       " b'0 962': 907,\n",
       " b'0 968': 908,\n",
       " b'0 98 percent': 909,\n",
       " b'0 99': 910,\n",
       " b'0 992': 911,\n",
       " b'0 998': 912,\n",
       " b'0 9999 silver coin': 913,\n",
       " b'0 999 fine silver': 914,\n",
       " b'0 9 armenians': 915,\n",
       " b'0 9 jewish 0 7 hindu': 916,\n",
       " b'0 9 lb coal': 917,\n",
       " b'0 9 m': 918,\n",
       " b'0 9 mile': 919,\n",
       " b'0 9 million ounces': 920,\n",
       " b'0 9 mm year': 921,\n",
       " b'0 9 mormons': 922,\n",
       " b'0 9 mya': 923,\n",
       " b'0 9 native american': 924,\n",
       " b'09 00 00 latitude': 925,\n",
       " b'0 9 percent': 926,\n",
       " b'0 9 protein': 927,\n",
       " b'0 9 water supply and waste management': 928,\n",
       " b'0 9 white irish': 929,\n",
       " b'0 ad': 930,\n",
       " b'brother': 312919,\n",
       " b'0 c': 934,\n",
       " b'c': 1539008,\n",
       " b'0 c isoterm': 936,\n",
       " b'cushion': 388028,\n",
       " b'0 degree': 938,\n",
       " b'0 drift': 939,\n",
       " b'0 g': 940,\n",
       " b'0h0': 941,\n",
       " b'inch': 579761,\n",
       " b'inch thick pieces': 944,\n",
       " b'0 kwh': 945,\n",
       " b'0ld mexico city': 946,\n",
       " b'0 longitude': 947,\n",
       " b'mass velocity': 948,\n",
       " b'mile': 731604,\n",
       " b'0 million people': 951,\n",
       " b'sho': 966951,\n",
       " b'0st': 953,\n",
       " b'0t': 954,\n",
       " b'tangka': 1036731,\n",
       " b'0 tax': 956,\n",
       " b'teaspoon': 1039855,\n",
       " b'0th': 958,\n",
       " b'0 to 15': 959,\n",
       " b'0 to 300 meters': 960,\n",
       " b'0 to 43': 961,\n",
       " b'0 travel': 962,\n",
       " b'0 y chromosome dna': 963,\n",
       " b'0 years': 964,\n",
       " b'10': 3604,\n",
       " b'1 0': 987,\n",
       " b'100': 1538920,\n",
       " b'1 00': 996,\n",
       " b'10 0': 997,\n",
       " b'100 0': 1000,\n",
       " b'1000': 1001,\n",
       " b'10 000': 1005,\n",
       " b'100 000 000 square feet': 1006,\n",
       " b'100 000 000 tonnes': 1007,\n",
       " b'10 000 000 metric tons': 1008,\n",
       " b'10 000 000 tonnes': 1009,\n",
       " b'10 000 000 tons': 1010,\n",
       " b'1 000 000 cartridges': 1011,\n",
       " b'1 000 000 cash': 1012,\n",
       " b'1 000 000 cubic metres': 1013,\n",
       " b'1 000 000 m': 1014,\n",
       " b'1 000 000 metric tons': 1015,\n",
       " b'1 000 000 more pieces': 1016,\n",
       " b'1 000 000 oz': 1017,\n",
       " b'1 000 000 people': 1018,\n",
       " b'1 000 000 psi': 1019,\n",
       " b'1 000 000 restaurants': 1020,\n",
       " b'1 000 000 steel tonnes': 1021,\n",
       " b'1 000 000 tonnes': 1022,\n",
       " b'1 000 000 tons': 1023,\n",
       " b'1 000 000 total earnings': 1024,\n",
       " b'1 000 000 towers': 1025,\n",
       " b'100 000 barrels': 1026,\n",
       " b'100 000 bbl': 1027,\n",
       " b'100 000 bc': 1028,\n",
       " b'100 000 bce': 1029,\n",
       " b'100 000 bikers': 1030,\n",
       " b'100 000 births': 1031,\n",
       " b'100 000 blacks': 1032,\n",
       " b'100 000 bottles': 1033,\n",
       " b'100 000 bricks': 1034,\n",
       " b'100 000 british thermal units': 1035,\n",
       " b'100 000 bushels': 1036,\n",
       " b'100 000 canadian dollars': 1037,\n",
       " b'100 000 cells': 1038,\n",
       " b'100 000 children': 1039,\n",
       " b'100 000 christians': 1040,\n",
       " b'100 000 coins': 1041,\n",
       " b'100 000 cords': 1042,\n",
       " b'100 000 cubic meters': 1043,\n",
       " b'100 000 ducats': 1044,\n",
       " b'100 000 dunams': 1045,\n",
       " b'100 000 european civilians': 1046,\n",
       " b'100 000 fc mchp systems': 1047,\n",
       " b'100 000 flight hours': 1048,\n",
       " b'100 000 gallons': 1049,\n",
       " b'100 000 gold florins': 1050,\n",
       " b'100 000 gunpowder arrows': 1051,\n",
       " b'100 000 ha': 1052,\n",
       " b'100 000 hectares': 1053,\n",
       " b'100 000 homes': 1054,\n",
       " b'100 000 individuals': 1055,\n",
       " b'100 000 industrial robots': 1056,\n",
       " b'100 000 inhabitants': 1057,\n",
       " b'100 000 jin': 1058,\n",
       " b'100 000 juan': 1059,\n",
       " b'100 000 kg': 1060,\n",
       " b'100 000 led street lamps': 1061,\n",
       " b'100 000 li': 1062,\n",
       " b'100 000 listeners': 1063,\n",
       " b'100 000 live births': 1064,\n",
       " b'100 000 metric tons': 1066,\n",
       " b'100 000 mulatto': 1067,\n",
       " b'100 000 mulberry trees': 1068,\n",
       " b'100 000 one peso coins': 1069,\n",
       " b'100 000 palestinians': 1070,\n",
       " b'100 000 people': 1072,\n",
       " b'100 000 persons': 1073,\n",
       " b'100 000 pesos': 1074,\n",
       " b'100 000 pieces': 1075,\n",
       " b'100 000 population': 1076,\n",
       " b'100 000 portions': 1077,\n",
       " b'100 000 pound': 1078,\n",
       " b'100 000 residents': 1079,\n",
       " b'100 000 seeds': 1080,\n",
       " b'100 000 sesterces': 1081,\n",
       " b'100 000 sets': 1082,\n",
       " b'100 000 soldiers': 1083,\n",
       " b'100 000 spectators': 1084,\n",
       " b'100 000 square feet': 1085,\n",
       " b'100 000 strings': 1086,\n",
       " b'100 000 taels': 1087,\n",
       " b'100 000 the equivalent': 1088,\n",
       " b'100 000 tickets': 1089,\n",
       " b'100 000 times the normal level': 1090,\n",
       " b'100 000 to 200 000 adults': 1091,\n",
       " b'100 000 to 300 000 followers': 1092,\n",
       " b'100 000 to 300 000 tons': 1093,\n",
       " b'100 000 tonnes': 1094,\n",
       " b'100 000 tons': 1096,\n",
       " b'100 000 trees': 1097,\n",
       " b'100 000 troops': 1098,\n",
       " b'100 000 tuscan florin': 1099,\n",
       " b'100 000 units': 1100,\n",
       " b'100 000 used vehicle tyres': 1101,\n",
       " b'100 000 vehicles': 1102,\n",
       " b'100 000 vietnamese refugees': 1103,\n",
       " b'100 000 volumes': 1104,\n",
       " b'100 000 words': 1105,\n",
       " b'100 000 workers': 1106,\n",
       " b'100 000 yearly earnings': 1107,\n",
       " b'100 000 year old beads': 1108,\n",
       " b'100 000 years': 1109,\n",
       " b'100000 yuan': 1110,\n",
       " b'100 000 zx81 users': 1111,\n",
       " b'10 000 12 000 ybp': 1112,\n",
       " b'10 000 200 000 additional deaths': 1113,\n",
       " b'10 000 40 000 bc': 1115,\n",
       " b'100 005 dollars': 1116,\n",
       " b'10 000 acres': 1117,\n",
       " b'10 000 additional parking spaces': 1118,\n",
       " b'10 000 alligators': 1119,\n",
       " b'10 000 and 15 000 feet': 1120,\n",
       " b'10 000 and 8 000 albanians': 1121,\n",
       " b'10 000 atomic bombs': 1122,\n",
       " b'10 000 automobiles': 1123,\n",
       " b'10 000 bc': 1125,\n",
       " b'10 000 b c': 1127,\n",
       " b'10 000 bce': 1128,\n",
       " b'10 000 becquerels': 1129,\n",
       " b'10 000 b p sunnm re': 1130,\n",
       " b'10 000 bq kg': 1131,\n",
       " b'10 000 bulldozers': 1132,\n",
       " b'10 000 canadians': 1133,\n",
       " b'10 000 centrifuges': 1134,\n",
       " b'10 000 copies': 1135,\n",
       " b'10 000 crores': 1136,\n",
       " b'10 000 cruzados': 1137,\n",
       " b'10 000 denominations': 1138,\n",
       " b'10 000 electric vehicles': 1139,\n",
       " b'10 000 fans': 1140,\n",
       " b'10 000 feet': 1141,\n",
       " b'10 000 followers': 1142,\n",
       " b'10 000 french': 1143,\n",
       " b'10 000 gallons': 1144,\n",
       " b'10 000 gross tons': 1145,\n",
       " b'10 000 guecha warriors': 1146,\n",
       " b'10 000 gwh': 1147,\n",
       " b'10 000 ha': 1148,\n",
       " b'10 000 head': 1149,\n",
       " b'10 000 heated buildings': 1150,\n",
       " b'10 000 highly qualified researchers': 1151,\n",
       " b'10 000 horsemen': 1152,\n",
       " b'10 000 horsepower': 1153,\n",
       " b'10 000 hours': 1154,\n",
       " b'10 000 households': 1155,\n",
       " b'10 000 incentives': 1156,\n",
       " b'10 000 indian americans': 1157,\n",
       " b'10 000 indians': 1158,\n",
       " b'10 000 indirect jobs': 1159,\n",
       " b'10 000 inhabitants': 1160,\n",
       " b'10 000 injuries': 1161,\n",
       " b'10 000 jobs': 1162,\n",
       " b'10 000 juan': 1163,\n",
       " b'10 000 korean musketeers': 1164,\n",
       " b'10 000 lamps': 1165,\n",
       " b'10 000 landings': 1166,\n",
       " b'10 000 letters': 1167,\n",
       " b'10 000 lira': 1168,\n",
       " b'10 000 loaves': 1169,\n",
       " b'10 000 lps': 1170,\n",
       " b'10 000 lux': 1171,\n",
       " b'10 000 maniacs': 1172,\n",
       " b'10 000 m area': 1173,\n",
       " b'10 000 marines': 1174,\n",
       " b'10 000 megawatts': 1175,\n",
       " b'10 000 men': 1176,\n",
       " b'10 000 metres': 1177,\n",
       " b'10 000 metric tons': 1178,\n",
       " b'10 000 mi': 1179,\n",
       " b'10 000 miles': 1180,\n",
       " b'10 000 miners': 1181,\n",
       " b'10 000 monthly royalty': 1182,\n",
       " b'10 000 mw projects': 1183,\n",
       " b'10 000 pairs': 1184,\n",
       " b'10 000 pakistanis': 1185,\n",
       " b'10 000 passenger cars': 1186,\n",
       " b'10 000 passengers': 1187,\n",
       " b'10 000 people': 1188,\n",
       " b'10 000 permutations': 1189,\n",
       " b'10 000 peso denominations': 1190,\n",
       " b'10 000 pesos': 1191,\n",
       " b'10 000 pieces': 1192,\n",
       " b'10 000 plants': 1193,\n",
       " b'10 000 points': 1194,\n",
       " b'10 000 population': 1195,\n",
       " b'10 000 portions': 1196,\n",
       " b'10 000 pots': 1197,\n",
       " b'10 000 pounds': 1198,\n",
       " b'10 000 psi': 1199,\n",
       " b'10 000 public transport vehicles': 1200,\n",
       " b'10 000 radiocarbon years': 1201,\n",
       " b'10 000 red foxes': 1202,\n",
       " b'10 000 residents': 1203,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489929"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  tylgiv\n",
      "Dropping  valtra\n",
      "Dropping  matsika\n",
      "Dropping  frenstrup\n",
      "Dropping  kakkassery\n",
      "Dropping  only martelly\n",
      "Dropping  n700\n",
      "Dropping  mitteldeutschland\n",
      "Dropping  n5348a\n",
      "Dropping  hiramic\n",
      "Dropping  defined fields\n",
      "Dropping  the s j p harvie professor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a terminating binary expansion\n",
      "Dropping  the online canvas design elements\n",
      "Dropping  the instantaneous angular velocity vector\n",
      "Dropping  fitting anorexic illnesses\n",
      "Dropping  a 1920s proposal\n",
      "Dropping  an international non profit and non governmental student society\n",
      "Dropping  william a trimble\n",
      "Dropping  a provincial regiment\n",
      "Dropping  first real studio experience\n",
      "Dropping  a lycoming o 360 a4 m\n",
      "Dropping  other graphics systems\n",
      "Dropping  polish tradition\n",
      "Dropping  a practising teacher\n",
      "Dropping  close diplomatic and economic relationships\n",
      "Dropping  kiley\n",
      "Dropping  original or reconstructed fabric\n",
      "Dropping  scriptural or customary laws\n",
      "Dropping  national economics challenge champions\n",
      "Dropping  a long horizontal jump\n",
      "Dropping  the open bloodstream\n",
      "Dropping  the officer s blooded horses\n",
      "Dropping  classical comedy\n",
      "Dropping  the continental exchanges\n",
      "Dropping  the most frequent uses\n",
      "Dropping  major local developers\n",
      "Dropping  184 restaurants\n",
      "Dropping  maria s young son\n",
      "Dropping  utsu\n",
      "Dropping  archeologist hugo winckler\n",
      "Dropping  zp120\n",
      "Dropping  the early 1950s dubuffet\n",
      "Dropping  merina and betsileo families\n",
      "Dropping  impersonalization\n",
      "Dropping  all necessary activities\n",
      "Dropping  more complex background settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15446:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "Exception in thread Thread-15449:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  periodic recitals\n",
      "Dropping  last weekend s post coup presidential election\n",
      "Dropping  so2 james suh\n",
      "Dropping  silvie iii\n",
      "Dropping  pot au feu\n",
      "Dropping  its operational readiness\n",
      "Dropping  no one reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15472:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  the exterior mirror\n",
      "Dropping  free agent greg holland\n",
      "Dropping  keio university hospital\n",
      "Dropping  negative at skew\n",
      "Dropping  the former coalfield area\n",
      "Dropping  a coherent personality\n",
      "Dropping  intevation\n",
      "Dropping  fgm 148 javelin\n",
      "Dropping  17 august robert ritter von greim s fliegerkorps v\n",
      "Dropping  neither military training\n",
      "Dropping  self service passport control\n",
      "Dropping  sierra s salon\n",
      "Dropping  general no l de castelnau\n",
      "Dropping  debra delee\n",
      "Dropping  davis second term\n",
      "Dropping  the oldest literary account\n",
      "Dropping  each wall inlet\n",
      "Dropping  the people s nomadic heritage\n",
      "Dropping  glasgow academicals\n",
      "Dropping  fine v fib\n",
      "Dropping  flat end facets\n",
      "Dropping  dense grids\n",
      "Dropping  professor dominique martin\n",
      "Dropping  the fastest overall driver\n",
      "Dropping  their sledging rations\n",
      "Dropping  the lambda company\n",
      "Dropping  the additional rail\n",
      "Dropping  maintenance flaws\n",
      "Dropping  a 75 cm long bundle\n",
      "Dropping  179 fs\n",
      "Dropping  military miniatures\n",
      "Dropping  performance and management flexibility\n",
      "Dropping  two state run polytechnic schools\n",
      "Dropping  scriabin s museum\n",
      "Dropping  protestant dublin lawyer theobald wolfe tone\n",
      "Dropping  16 canadians\n",
      "Dropping  the individual coal plots\n",
      "Dropping  i e visemes\n",
      "Dropping  a e36 m3 compact prototype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15713:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  sal n de la paz\n",
      "Dropping  brian williams lustmord project\n",
      "Dropping  an exponential behavior\n",
      "Dropping  this uncommon case\n",
      "Dropping  only 13 more performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15739:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  then a third wrestling team\n",
      "Dropping  an old watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15748:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  a sophisticated propaganda machine\n",
      "Dropping  a successful and effective program\n",
      "Dropping  50 s strongest track\n",
      "Dropping  the yshphh\n",
      "Dropping  the estimated sinking position\n",
      "Dropping  phoenix s citizens\n",
      "Dropping  the cbbb\n",
      "Dropping  re arranged panels\n",
      "Dropping  his 50th birthday celebration\n",
      "Dropping  the male eggs\n",
      "Dropping  montane meadows\n",
      "Dropping  the troops good spirit\n",
      "Dropping  paltrow s performance\n",
      "Dropping  a free demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15808:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/vlead/anaconda3/lib/python3.7/json/encoder.py\", line 356, in _iterencode_dict\n",
      "    for key, value in items:\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping  roxy attempts\n",
      "Dropping  either deletion\n",
      "Dropping  frictional behavior\n",
      "Dropping  four successive popes\n",
      "Dropping  engineering design teams\n",
      "Dropping  felix the cat\n",
      "Dropping  tidal venuses\n",
      "Dropping  dsquared2 duo dean and dan caten\n",
      "Dropping  cooper s most important film\n",
      "Dropping  consistent subtest scores\n",
      "Dropping  frances hegarty\n",
      "Dropping  borland s guitar playing\n",
      "Dropping  ahsura\n",
      "Dropping  an unnamed polish clone\n",
      "Dropping  at least the a credit rating\n",
      "Dropping  a radio based transatlantic telephone service\n",
      "Dropping  carddass exclusive storyline series\n",
      "Dropping  a balance sheet hedge\n",
      "Dropping  bluebush saltbush steppe\n"
     ]
    }
   ],
   "source": [
    "word2id_db_corrected = pickledb.load(prefix + \"w2i_corrected.db\", True)\n",
    "id2word_db_corrected = pickledb.load(prefix + \"i2w_corrected.db\", True)\n",
    "allkeys = list(word2id_db.getall())\n",
    "for key in allkeys:\n",
    "    try:\n",
    "        word2id_db_corrected[preprocess_word(nlp(key))] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = preprocess_word(nlp(key))\n",
    "    except:\n",
    "        print (\"Dropping \", key)\n",
    "        word2id_db_corrected[key] = word2id_db[key]\n",
    "        id2word_db_corrected[word2id_db[key]] = key\n",
    "word2id_db_corrected.dump()\n",
    "id2word_db_corrected.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing, sys, glob,  en_core_web_lg, neuralcoref\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def preprocess_word(noun):\n",
    "    filt_tokens = [\"DET\", \"ADV\", \"PUNCT\", \"CCONJ\"]\n",
    "    start_index = [i for i,token in enumerate(noun) if token.pos_ not in filt_tokens][0]\n",
    "    np_filt = noun[start_index:].text\n",
    "    if \"(\" not in np_filt and \")\" in np_filt:\n",
    "        np_filt = np_filt.replace(\")\", \"\")\n",
    "    elif \"(\" in np_filt and \")\" not in np_filt:\n",
    "        np_filt = np_filt.replace(\"(\", \"\")\n",
    "    return np_filt\n",
    "\n",
    "\n",
    "nlp = en_core_web_lg.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015409708023071289\n",
      "0.0001728534698486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "a = nlp(\"beach on the ocean\")\n",
    "print (time.time()-t)\n",
    "t = time.time()\n",
    "preprocess_word(a)\n",
    "print (time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25139"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "lines = [l.split(\"\\t\")[:3] for l in open(\"../files/dataset/pizza_final.tsv\", \"r\").read().split(\"\\n\")[:-1]]\n",
    "final_lines = []\n",
    "for line in lines:\n",
    "    elem = line\n",
    "    if random.random()>0.5:\n",
    "        label = line[2]\n",
    "        if label == \"hyponym\":\n",
    "            label = \"hypernym\"\n",
    "        elif label == \"hypernym\":\n",
    "            label = \"hyponym\"\n",
    "        elif label == \"concept\":\n",
    "            label = \"instance\"\n",
    "        elif label == \"instance\":\n",
    "            label = \"concept\"\n",
    "        elem = [line[1], line[0], label]\n",
    "    final_lines.append(elem)\n",
    "random.shuffle(final_lines)\n",
    "final_lines_none = [elem for elem in final_lines if elem==\"none\"]\n",
    "final_lines_none_train = final_lines_none[:int(0.9 * len(final_lines_none))]\n",
    "final_lines_none_test = final_lines_none[int(0.9 * len(final_lines_none)):]\n",
    "\n",
    "final_lines_rest = [elem for elem in final_lines if elem!=\"none\"]\n",
    "final_lines_rest_train = final_lines_rest[:int(0.9 * len(final_lines_rest))]\n",
    "final_lines_rest_test = final_lines_rest[int(0.9 * len(final_lines_rest)):]\n",
    "\n",
    "final_lines_train = final_lines_none_train + final_lines_rest_train\n",
    "final_lines_test = final_lines_none_test + final_lines_rest_test\n",
    "\n",
    "open(\"../files/dataset/pizza_train.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines_train]))\n",
    "open(\"../files/dataset/pizza_test.tsv\",\"w+\").write(\"\\n\".join([\"\\t\".join(line) for line in final_lines_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7119, 791)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_lines_train), len(final_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
