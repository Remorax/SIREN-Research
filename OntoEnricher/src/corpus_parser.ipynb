{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsddb3 import btopen\n",
    "import bcolz, pickle, os, sys, shelve, time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "train_file = \"../files/dataset/train.tsv\"\n",
    "test_file = \"../files/dataset/test.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat\"\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "use_embeddings = \"../files/embeddings.pt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "# relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"/data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\"\n",
    "op_file = \"dataset_parsed.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset for  /data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\n",
      "Successful hits:  138959 Failed hits:  63951\n",
      "Parsed /data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_7_10/security\n"
     ]
    }
   ],
   "source": [
    "success, failed = [], []\n",
    "def id_to_entity(db, entity_id):\n",
    "    try:\n",
    "        entity = db[str(entity_id)]    \n",
    "    except:\n",
    "        entity = db[str(entity_id).decode(\"utf-8\")]\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    try:\n",
    "        entity = db[str(entity_id)]\n",
    "    except:\n",
    "        entity = db[str(entity_id).decode(\"utf-8\")]\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    if entity in db:\n",
    "        success.append(entity)\n",
    "        try:\n",
    "            return int(db[entity])\n",
    "        except:\n",
    "            return int(db[entity.decode(\"utf-8\")])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = vectors\n",
    "    except:\n",
    "        embeddings, word2idx = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "\n",
    "\n",
    "def create_embeddings():\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word, vector = line[0], line[1:]\n",
    "            words.append(word)\n",
    "            vectors.append(np.array(vector).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    vectors = vectors.reshape((-1, EMBEDDING_DIM))\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "\n",
    "    return vectors, word2idx\n",
    "\n",
    "try:\n",
    "    word2id_db = shelve.open(prefix + \"_word_to_id_dict.db\", 'r')\n",
    "except:\n",
    "    print (prefix)\n",
    "    raise\n",
    "id2word_db = shelve.open(prefix + \"_id_to_word_dict.db\", \"r\")\n",
    "path2id_db = shelve.open(prefix + \"_path_to_id_dict.db\", \"r\")\n",
    "id2path_db = shelve.open(prefix + \"_id_to_path_dict.db\", \"r\")\n",
    "relations_db = shelve.open(prefix + \"_relations_map.db\", \"r\")\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = entity_to_id(word2id_db, tup[0]), entity_to_id(word2id_db, tup[1])\n",
    "    # paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "    # x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "    # y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "    # path_count_dict = { id_to_path(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    paths_xy = list(extract_paths(relations_db,x,y).items())\n",
    "    paths_yx = list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict = { id_to_path(id2path_db, path) : freq for (path, freq) in paths_xy }\n",
    "    path_count_dict.update({ id_to_path(id2path_db, path).replace(\"X/\", '@@@').replace('Y/', 'X/').replace('@@@', 'Y/') : freq for (path, freq) in paths_yx })\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    print (\"Parsing dataset for \", prefix)\n",
    "\n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    embed_indices = [(emb_indexer.get(x,0), emb_indexer.get(y,0)) for (x,y) in dataset]\n",
    "\n",
    "    return embed_indices, paths\n",
    "\n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys()) + list(test_instances.keys()) + list(test_knocked.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values()) + list(test_instances.values()) + list(test_knocked.values())\n",
    "\n",
    "mappingDict = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "f.close()\n",
    "\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.hidden_dim = HIDDEN_DIM + 2 * EMBEDDING_DIM\n",
    "        self.input_dim = POS_DIM + DEP_DIM + EMBEDDING_DIM + DIR_DIM\n",
    "        self.W = nn.Linear(NUM_RELATIONS, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(len(embeddings), EMBEDDING_DIM)\n",
    "        self.word_embeddings.load_state_dict({'weight': torch.from_numpy(np.array(embeddings))})\n",
    "        self.word_embeddings.require_grad = False\n",
    "        \n",
    "        self.pos_embeddings = nn.Embedding(len(pos_indexer), POS_DIM)\n",
    "        self.dep_embeddings = nn.Embedding(len(dep_indexer), DEP_DIM)\n",
    "        self.dir_embeddings = nn.Embedding(len(dir_indexer), DIR_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, NUM_LAYERS)\n",
    "    \n",
    "    def embed_path(self, elem):\n",
    "        path, count = elem\n",
    "        if path in self.cache:\n",
    "            return cache[path] * count\n",
    "        lstm_inp = []\n",
    "        for edge in path:\n",
    "            inputs = [torch.Tensor([[el]]) for el in edge]\n",
    "            word_embed = self.dropout_layer(self.word_embeddings(inputs[0]))\n",
    "            pos_embed = self.dropout_layer(self.pos_embeddings(inputs[1]))\n",
    "            dep_embed = self.dropout_layer(self.dep_embeddings(inputs[2]))\n",
    "            dir_embed = self.dropout_layer(self.dir_embeddings(inputs[3]))\n",
    "            embeds = np.concatenate((word_embed, pos_embed, dep_embed, dir_embed))\n",
    "            lstm_inp.append(embeds)\n",
    "        output, _ = self.lstm(lstm_inp)\n",
    "        cache[path] = output\n",
    "\n",
    "        return output * count\n",
    "    \n",
    "    def forward(self, data, emb_indexer):\n",
    "        if not data:\n",
    "            data[NULL_PATH] = 1\n",
    "        print (\"Data: \", data)\n",
    "        num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = [np.sum([self.embed_path(path) for path in paths.items()]) for paths in data]\n",
    "        print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        h = [np.concatenate((self.word_embeddings(elem[0]), h[i], self.word_embeddings(elem[1]))) for i,emb in enumerate(emb_indexer)]\n",
    "        return self.softmax(self.W(h))\n",
    "\n",
    "HIDDEN_DIM = 60\n",
    "NUM_LAYERS = 2\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "batch_size = min(batch_size, dataset_size)\n",
    "num_batches = int(ceil(dataset_size/batch_size))\n",
    "\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "lstm = LSTM()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss, epoch_idx = 0, np.random.permutation(dataset_size)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch = epoch_idx[batch_start:batch_end]\n",
    "        \n",
    "        data, labels, embeddings_idx = x_train[batch], y_train[batch], embed_indices_train[batch]\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = lstm(data, embeddings_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= dataset_size\n",
    "    print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch + 1, num_epochs, total_loss))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictedLabels = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = lstm(data)\n",
    "        print (outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictedLabels.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "e = nn.Embedding(3, 3)\n",
    "ls = [[0, 1, 2], [3,4,5], [6,7,8]]\n",
    "ls = np.array([np.array(el) for el in ls])\n",
    "e.load_state_dict({'weight': torch.from_numpy(ls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = np.array([np.sum([self.embed_path(path) for path in paths.items()]) for paths in data])\n",
    "        #print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        print (h.shape)\n",
    "        h = [np.concatenate((self.word_embeddings(emb[0]), h[i], self.word_embeddings(emb[1]))) for i,emb in enumerate(emb_indexer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randn(1,4)\n",
    "torch.Tensor([[1]]).shape\n",
    "# torch.cat((h, t.view(1,-1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputt = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(inputt, target)\n",
    "output.backward()\n",
    "print (output, inputt, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took me 16.031342029571533 seconds to extract USE embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crateva greveana flowers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_word_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wansecure firewall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Values and keys obtained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim=-1000, closest_word=\"\"):\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    closest_word = \"\"\n",
    "    with shelve.open(use_embeddings, 'c') as db:\n",
    "        for i, w in enumerate(word_embeddings):\n",
    "            db[words[i]] = w\n",
    "        closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[closest_word_idx]\n",
    "        del word_embeddings\n",
    "    del db\n",
    "    return closest_word, max_sim\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "    if os.path.isfile(use_embeddings):\n",
    "        with shelve.open(use_embeddings, 'r') as db:\n",
    "            embeds = np.array(list(db.values()))\n",
    "            words = np.array(list(db.keys()))\n",
    "            print (\"Values and keys obtained\", time.time()-a)\n",
    "            sim_mat = awesome_cossim_topn(coo_matrix(embeds, dtype=np.float64), coo_matrix(word_to_compare.T, dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250)\n",
    "            print (\"Sim mat calculated\", time.time()-a)\n",
    "            closest_word_idx = np.argmax(sim_mat)\n",
    "            print (\"idx gotten\", time.time()-a)\n",
    "            closest_word = words[closest_word_idx]\n",
    "    else:\n",
    "        words = list(word2id_db.keys())\n",
    "        print (\"Obtained list of words\")\n",
    "        len_part = 100000\n",
    "        max_sim = -1000\n",
    "        n_parts = ceil(len(words)/len_part)\n",
    "        closest_word = \"\"\n",
    "        for i in range(n_parts):\n",
    "            words_part = words[i*len_part:(i+1)*len_part]\n",
    "            closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "\n",
    "org_names = names['buyer'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "\n",
    "\n",
    "print('All 3-grams in \"Department\":')\n",
    "print(extract_ngrams('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "\n",
    "def extract_ngrams(string, n=3):\n",
    "    string = fix_text(string).encode(\"ascii\", errors=\"ignore\").decode().lower() # fix text\n",
    "    string = string.replace('&', 'and').replace(',', ' ').replace('-', ' ').title()\n",
    "    string = re.sub('[' + re.escape(''.join(chars_to_remove)) + ']', '', string)\n",
    "    string = ' ' + re.sub(' +',' ',string).strip() + ' '\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    ngrams = [''.join(ngram) for ngram in ngrams]\n",
    "    return ngrams\n",
    "\n",
    "word_to_match = \"margherita pizza\"\n",
    "words = list(word2id_db.keys())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(words + [word_to_match])\n",
    "\n",
    "# d = awesome_cossim_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    print (sparserows)\n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "words[np.argmax(d)]\n",
    "print (\"time: \", start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_db_new = shelve.open(prefix + \"_relations_map.db\", \"c\")\n",
    "for k, v in relations_db.items():\n",
    "    relations_db_new[\"###\".join(k.split(\"_\"))] = v\n",
    "relations_db_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"\\t\".join(l.split(\"\\t\")[1:-1]) for l in open(\"../junk/security_dataset.tsv\",\"r\").read().split(\"\\n\")[1:]]\n",
    "open(\"../files/dataset/dataset.tsv\",\"w\").write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open(use_embeddings, 'r') as db:    \n",
    "    allitems = list(db.items())\n",
    "    emb = [el[1] for el in allitems]\n",
    "    wds = [el[0] for el in allitems]\n",
    "    file = open(\"../files/embeddings_list.pkl\", \"wb\")\n",
    "    pickle.dump(allitems, file)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22912765"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab peter wyche (diplomat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab acoma-zuni section\n",
      "Original word not in vocab madan-harini\n",
      "Original word not in vocab trust no one (internet security)\n",
      "Original word not in vocab international tibet independence movement\n",
      "Original word not in vocab isobase\n",
      "Original word not in vocab human computer interaction (security)\n",
      "Original word not in vocab poetas de karaoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab ipa pulmonic consonant chart with audio\n",
      "Original word not in vocab lego clutch powers: bad hair day\n",
      "Original word not in vocab aed (non-profit)\n",
      "Original word not in vocab quilmes airport\n",
      "Original word not in vocab yendegaia airport\n",
      "Original word not in vocab the pack a.d.\n",
      "Original word not in vocab harvie-watt baronets\n",
      "Original word not in vocab sharp actius rd3d notebook\n",
      "Original word not in vocab big beach boutique ii - the movie\n",
      "Original word not in vocab privacy by design\n",
      "Original word not in vocab motorola devour\n",
      "Original word not in vocab piracy act\n",
      "Original word not in vocab starter ring gear\n",
      "Original word not in vocab antonio sánchez (puerto rican host)\n",
      "Original word not in vocab electronic logbook\n",
      "Original word not in vocab greg burke (journalist)\n",
      "Original word not in vocab deaths in november 2013\n",
      "Original word not in vocab hp mini 311\n",
      "Original word not in vocab confederation of indigenous nationalities of the ecuadorian amazon\n",
      "Original word not in vocab url subscription architecture\n",
      "Original word not in vocab snowballers entertainment\n",
      "Original word not in vocab basic strategic arts program\n",
      "Original word not in vocab mars (ticket reservation system)\n",
      "Original word not in vocab matija kristić\n",
      "Original word not in vocab edward graham lee\n",
      "Original word not in vocab rebellion of the three guards\n",
      "Original word not in vocab the recipe for gertrude\n",
      "Original word not in vocab quiet pc\n",
      "Original word not in vocab russian amateur radio union\n",
      "1 done\n",
      "Original word not in vocab core strategy document\n",
      "Original word not in vocab mutukula airport\n",
      "2 done\n",
      "Original word not in vocab magnus l. kpakol\n",
      "1 done\n",
      "Original word not in vocab strangers (malibu comics)\n",
      "2 done\n",
      "3 done\n",
      "Original word not in vocab cary baronets\n",
      "3 done\n",
      "Original word not in vocab andrew wood (diplomat)\n",
      "4 done\n",
      "Original word not in vocab lead petty officer\n",
      "4 done\n",
      "Original word not in vocab corps of military police (india)\n",
      "5 done\n",
      "Original word not in vocab siniša radanović\n",
      "5 done\n",
      "Original word not in vocab oatmeal cookie\n",
      "6 done\n",
      "Original word not in vocab academic research alliance\n",
      "6 done\n",
      "1 done\n",
      "Original word not in vocab joseph e. duncan iii\n",
      "7 done\n",
      "7 done\n",
      "Original word not in vocab lideta army airport\n",
      "2 done\n",
      "8 done\n",
      "8 done\n",
      "3 done\n",
      "9 done\n",
      "9 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "10 done\n",
      "10 done\n",
      "7 done\n",
      "11 done\n",
      "11 done\n",
      "12 done\n",
      "12 done\n",
      "8 done\n",
      "13 done\n",
      "13 done\n",
      "1 done\n",
      "9 done\n",
      "14 done\n",
      "2 done\n",
      "14 done\n",
      "1 done\n",
      "15 done\n",
      "3 done\n",
      "15 done\n",
      "10 done\n",
      "16 done\n",
      "2 done\n",
      "11 done\n",
      "4 done\n",
      "16 done\n",
      "3 done\n",
      "12 done\n",
      "17 done\n",
      "5 done\n",
      "4 done\n",
      "17 done\n",
      "13 done\n",
      "6 done\n",
      "18 done\n",
      "5 done\n",
      "18 done\n",
      "14 done\n",
      "6 done\n",
      "19 done\n",
      "7 done\n",
      "19 done\n",
      "15 done\n",
      "7 done\n",
      "20 done\n",
      "20 done\n",
      "16 done\n",
      "8 done\n",
      "21 done\n",
      "8 done\n",
      "21 done\n",
      "17 done\n",
      "9 done\n",
      "22 done\n",
      "22 done\n",
      "9 done\n",
      "18 done\n",
      "23 done\n",
      "23 done\n",
      "19 done\n",
      "10 done\n",
      "24 done\n",
      "10 done\n",
      "24 done\n",
      "20 done\n",
      "11 done\n",
      "25 done\n",
      "11 done\n",
      "25 done\n",
      "21 done\n",
      "12 done\n",
      "12 done\n",
      "26 done\n",
      "13 done\n",
      "26 done\n",
      "13 done\n",
      "22 done\n",
      "27 done\n",
      "14 done\n",
      "27 done\n",
      "14 done\n",
      "28 done\n",
      "15 done\n",
      "23 done\n",
      "28 done\n",
      "16 done\n",
      "15 done\n",
      "29 done\n",
      "24 done\n",
      "17 done\n",
      "16 done\n",
      "29 done\n",
      "30 done\n",
      "25 done\n",
      "18 done\n",
      "30 done\n",
      "17 done\n",
      "26 done\n",
      "19 done\n",
      "18 done\n",
      "27 done\n",
      "31 done\n",
      "20 done\n",
      "31 done\n",
      "19 done\n",
      "28 done\n",
      "32 done\n",
      "21 done\n",
      "20 done\n",
      "32 done\n",
      "33 done\n",
      "29 done\n",
      "22 done\n",
      "33 done\n",
      "21 done\n",
      "34 done\n",
      "30 done\n",
      "34 done\n",
      "23 done\n",
      "22 done\n",
      "35 done\n",
      "24 done\n",
      "35 done\n",
      "36 done\n",
      "23 done\n",
      "25 done\n",
      "31 done\n",
      "36 done\n",
      "37 done\n",
      "37 done\n",
      "24 done\n",
      "26 done\n",
      "32 done\n",
      "38 done\n",
      "38 done\n",
      "25 done\n",
      "27 done\n",
      "33 done\n",
      "39 done\n",
      "39 done\n",
      "28 done\n",
      "26 done\n",
      "34 done\n",
      "40 done\n",
      "40 done\n",
      "29 done\n",
      "27 done\n",
      "41 done\n",
      "35 done\n",
      "30 done\n",
      "41 done\n",
      "28 done\n",
      "42 done\n",
      "36 done\n",
      "42 done\n",
      "29 done\n",
      "43 done\n",
      "37 done\n",
      "31 done\n",
      "43 done\n",
      "44 done\n",
      "30 done\n",
      "38 done\n",
      "44 done\n",
      "45 done\n",
      "32 done\n",
      "39 done\n",
      "45 done\n",
      "33 done\n",
      "46 done\n",
      "31 done\n",
      "40 done\n",
      "34 done\n",
      "46 done\n",
      "47 done\n",
      "32 done\n",
      "47 done\n",
      "35 done\n",
      "33 done\n",
      "41 done\n",
      "48 done\n",
      "48 done\n",
      "34 done\n",
      "36 done\n",
      "42 done\n",
      "49 done\n",
      "49 done\n",
      "37 done\n",
      "43 done\n",
      "35 done\n",
      "50 done\n",
      "44 done\n",
      "50 done\n",
      "38 done\n",
      "36 done\n",
      "51 done\n",
      "51 done\n",
      "45 done\n",
      "39 done\n",
      "37 done\n",
      "52 done\n",
      "52 done\n",
      "46 done\n",
      "53 done\n",
      "38 done\n",
      "40 done\n",
      "53 done\n",
      "47 done\n",
      "54 done\n",
      "39 done\n",
      "41 done\n",
      "54 done\n",
      "48 done\n",
      "55 done\n",
      "42 done\n",
      "55 done\n",
      "40 done\n",
      "49 done\n",
      "56 done\n",
      "43 done\n",
      "50 done\n",
      "41 done\n",
      "57 done\n",
      "56 done\n",
      "44 done\n",
      "51 done\n",
      "57 done\n",
      "58 done\n",
      "42 done\n",
      "45 done\n",
      "58 done\n",
      "52 done\n",
      "59 done\n",
      "43 done\n",
      "46 done\n",
      "59 done\n",
      "53 done\n",
      "44 done\n",
      "60 done\n",
      "47 done\n",
      "60 done\n",
      "54 done\n",
      "45 done\n",
      "61 done\n",
      "48 done\n",
      "55 done\n",
      "61 done\n",
      "62 done\n",
      "46 done\n",
      "49 done\n",
      "62 done\n",
      "56 done\n",
      "63 done\n",
      "50 done\n",
      "47 done\n",
      "57 done\n",
      "63 done\n",
      "64 done\n",
      "51 done\n",
      "48 done\n",
      "58 done\n",
      "64 done\n",
      "52 done\n",
      "49 done\n",
      "65 done\n",
      "59 done\n",
      "50 done\n",
      "65 done\n",
      "53 done\n",
      "66 done\n",
      "60 done\n",
      "51 done\n",
      "54 done\n",
      "67 done\n",
      "66 done\n",
      "52 done\n",
      "61 done\n",
      "55 done\n",
      "68 done\n",
      "67 done\n",
      "53 done\n",
      "62 done\n",
      "56 done\n",
      "68 done\n",
      "69 done\n",
      "54 done\n",
      "63 done\n",
      "57 done\n",
      "69 done\n",
      "70 done\n",
      "55 done\n",
      "64 done\n",
      "58 done\n",
      "70 done\n",
      "71 done\n",
      "56 done\n",
      "65 done\n",
      "72 done\n",
      "71 done\n",
      "59 done\n",
      "57 done\n",
      "66 done\n",
      "72 done\n",
      "73 done\n",
      "60 done\n",
      "58 done\n",
      "67 done\n",
      "73 done\n",
      "61 done\n",
      "74 done\n",
      "59 done\n",
      "68 done\n",
      "62 done\n",
      "74 done\n",
      "75 done\n",
      "60 done\n",
      "69 done\n",
      "63 done\n",
      "75 done\n",
      "76 done\n",
      "64 done\n",
      "61 done\n",
      "70 done\n",
      "76 done\n",
      "77 done\n",
      "62 done\n",
      "71 done\n",
      "65 done\n",
      "77 done\n",
      "72 done\n",
      "78 done\n",
      "63 done\n",
      "78 done\n",
      "66 done\n",
      "73 done\n",
      "79 done\n",
      "64 done\n",
      "79 done\n",
      "67 done\n",
      "74 done\n",
      "80 done\n",
      "65 done\n",
      "68 done\n",
      "80 done\n",
      "75 done\n",
      "69 done\n",
      "81 done\n",
      "81 done\n",
      "66 done\n",
      "70 done\n",
      "76 done\n",
      "67 done\n",
      "82 done\n",
      "71 done\n",
      "82 done\n",
      "68 done\n",
      "77 done\n",
      "83 done\n",
      "72 done\n",
      "83 done\n",
      "69 done\n",
      "84 done\n",
      "78 done\n",
      "73 done\n",
      "84 done\n",
      "70 done\n",
      "79 done\n",
      "74 done\n",
      "71 done\n",
      "85 done\n",
      "85 done\n",
      "80 done\n",
      "72 done\n",
      "75 done\n",
      "86 done\n",
      "86 done\n",
      "81 done\n",
      "76 done\n",
      "73 done\n",
      "87 done\n",
      "87 done\n",
      "74 done\n",
      "77 done\n",
      "88 done\n",
      "82 done\n",
      "88 done\n",
      "89 done\n",
      "78 done\n",
      "75 done\n",
      "83 done\n",
      "89 done\n",
      "90 done\n",
      "76 done\n",
      "79 done\n",
      "90 done\n",
      "84 done\n",
      "80 done\n",
      "77 done\n",
      "91 done\n",
      "91 done\n",
      "85 done\n",
      "92 done\n",
      "81 done\n",
      "78 done\n",
      "92 done\n",
      "93 done\n",
      "86 done\n",
      "79 done\n",
      "93 done\n",
      "82 done\n",
      "87 done\n",
      "83 done\n",
      "80 done\n",
      "94 done\n",
      "94 done\n",
      "88 done\n",
      "95 done\n",
      "84 done\n",
      "95 done\n",
      "81 done\n",
      "89 done\n",
      "96 done\n",
      "96 done\n",
      "90 done\n",
      "82 done\n",
      "85 done\n",
      "97 done\n",
      "97 done\n",
      "91 done\n",
      "83 done\n",
      "86 done\n",
      "98 done\n",
      "98 done\n",
      "92 done\n",
      "84 done\n",
      "87 done\n",
      "93 done\n",
      "88 done\n",
      "99 done\n",
      "99 done\n",
      "89 done\n",
      "85 done\n",
      "100 done\n",
      "100 done\n",
      "94 done\n",
      "90 done\n",
      "86 done\n",
      "101 done\n",
      "101 done\n",
      "95 done\n",
      "91 done\n",
      "87 done\n",
      "92 done\n",
      "102 done\n",
      "102 done\n",
      "96 done\n",
      "88 done\n",
      "93 done\n",
      "103 done\n",
      "89 done\n",
      "97 done\n",
      "103 done\n",
      "94 done\n",
      "90 done\n",
      "104 done\n",
      "98 done\n",
      "104 done\n",
      "95 done\n",
      "105 done\n",
      "99 done\n",
      "91 done\n",
      "106 done\n",
      "96 done\n",
      "100 done\n",
      "105 done\n",
      "92 done\n",
      "97 done\n",
      "101 done\n",
      "106 done\n",
      "107 done\n",
      "93 done\n",
      "102 done\n",
      "108 done\n",
      "98 done\n",
      "107 done\n",
      "94 done\n",
      "103 done\n",
      "109 done\n",
      "108 done\n",
      "95 done\n",
      "99 done\n",
      "104 done\n",
      "100 done\n",
      "109 done\n",
      "96 done\n",
      "110 done\n",
      "105 done\n",
      "97 done\n",
      "101 done\n",
      "111 done\n",
      "106 done\n",
      "110 done\n",
      "102 done\n",
      "112 done\n",
      "98 done\n",
      "113 done\n",
      "111 done\n",
      "107 done\n",
      "103 done\n",
      "99 done\n",
      "114 done\n",
      "108 done\n",
      "112 done\n",
      "100 done\n",
      "115 done\n",
      "109 done\n",
      "104 done\n",
      "113 done\n",
      "101 done\n",
      "116 done\n",
      "105 done\n",
      "114 done\n",
      "102 done\n",
      "110 done\n",
      "106 done\n",
      "115 done\n",
      "117 done\n",
      "111 done\n",
      "103 done\n",
      "107 done\n",
      "116 done\n",
      "118 done\n",
      "104 done\n",
      "108 done\n",
      "112 done\n",
      "113 done\n",
      "105 done\n",
      "109 done\n",
      "119 done\n",
      "117 done\n",
      "114 done\n",
      "106 done\n",
      "120 done\n",
      "118 done\n",
      "115 done\n",
      "110 done\n",
      "107 done\n",
      "121 done\n",
      "111 done\n",
      "119 done\n",
      "116 done\n",
      "108 done\n",
      "122 done\n",
      "120 done\n",
      "112 done\n",
      "109 done\n",
      "123 done\n",
      "113 done\n",
      "121 done\n",
      "117 done\n",
      "124 done\n",
      "110 done\n",
      "114 done\n",
      "122 done\n",
      "118 done\n",
      "111 done\n",
      "125 done\n",
      "115 done\n",
      "123 done\n",
      "112 done\n",
      "119 done\n",
      "126 done\n",
      "116 done\n",
      "124 done\n",
      "113 done\n",
      "120 done\n",
      "127 done\n",
      "125 done\n",
      "114 done\n",
      "121 done\n",
      "128 done\n",
      "117 done\n",
      "122 done\n",
      "115 done\n",
      "118 done\n",
      "129 done\n",
      "126 done\n",
      "123 done\n",
      "130 done\n",
      "127 done\n",
      "116 done\n",
      "119 done\n",
      "124 done\n",
      "131 done\n",
      "128 done\n",
      "120 done\n",
      "129 done\n",
      "117 done\n",
      "125 done\n",
      "132 done\n",
      "121 done\n",
      "118 done\n",
      "130 done\n",
      "133 done\n",
      "122 done\n",
      "126 done\n",
      "131 done\n",
      "134 done\n",
      "119 done\n",
      "123 done\n",
      "127 done\n",
      "135 done\n",
      "132 done\n",
      "124 done\n",
      "120 done\n",
      "128 done\n",
      "136 done\n",
      "133 done\n",
      "129 done\n",
      "121 done\n",
      "125 done\n",
      "134 done\n",
      "130 done\n",
      "122 done\n",
      "126 done\n",
      "137 done\n",
      "135 done\n",
      "123 done\n",
      "131 done\n",
      "127 done\n",
      "136 done\n",
      "138 done\n",
      "124 done\n",
      "132 done\n",
      "139 done\n",
      "128 done\n",
      "125 done\n",
      "140 done\n",
      "133 done\n",
      "129 done\n",
      "137 done\n",
      "138 done\n",
      "130 done\n",
      "134 done\n",
      "126 done\n",
      "141 done\n",
      "139 done\n",
      "135 done\n",
      "127 done\n",
      "142 done\n",
      "131 done\n",
      "140 done\n",
      "136 done\n",
      "143 done\n",
      "128 done\n",
      "141 done\n",
      "132 done\n",
      "142 done\n",
      "144 done\n",
      "129 done\n",
      "133 done\n",
      "137 done\n",
      "130 done\n",
      "145 done\n",
      "143 done\n",
      "134 done\n",
      "138 done\n",
      "146 done\n",
      "135 done\n",
      "144 done\n",
      "131 done\n",
      "147 done\n",
      "139 done\n",
      "148 done\n",
      "136 done\n",
      "145 done\n",
      "149 done\n",
      "132 done\n",
      "140 done\n",
      "150 done\n",
      "146 done\n",
      "133 done\n",
      "147 done\n",
      "151 done\n",
      "141 done\n",
      "137 done\n",
      "134 done\n",
      "148 done\n",
      "142 done\n",
      "152 done\n",
      "135 done\n",
      "138 done\n",
      "153 done\n",
      "149 done\n",
      "136 done\n",
      "154 done\n",
      "143 done\n",
      "139 done\n",
      "150 done\n",
      "155 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "151 done\n",
      "145 done\n",
      "152 done\n",
      "137 done\n",
      "157 done\n",
      "141 done\n",
      "153 done\n",
      "146 done\n",
      "158 done\n",
      "138 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 done\n",
      "154 done\n",
      "147 done\n",
      "159 done\n",
      "139 done\n",
      "143 done\n",
      "155 done\n",
      "148 done\n",
      "160 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "161 done\n",
      "149 done\n",
      "145 done\n",
      "157 done\n",
      "141 done\n",
      "150 done\n",
      "146 done\n",
      "142 done\n",
      "158 done\n",
      "162 done\n",
      "151 done\n",
      "147 done\n",
      "159 done\n",
      "143 done\n",
      "152 done\n",
      "148 done\n",
      "163 done\n",
      "160 done\n",
      "144 done\n",
      "149 done\n",
      "153 done\n",
      "161 done\n",
      "145 done\n",
      "150 done\n",
      "146 done\n",
      "154 done\n",
      "164 done\n",
      "151 done\n",
      "147 done\n",
      "155 done\n",
      "148 done\n",
      "152 done\n",
      "156 done\n",
      "165 done\n",
      "162 done\n",
      "149 done\n",
      "157 done\n",
      "166 done\n",
      "163 done\n",
      "153 done\n",
      "150 done\n",
      "158 done\n",
      "154 done\n",
      "151 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "164 done\n",
      "152 done\n",
      "168 done\n",
      "156 done\n",
      "160 done\n",
      "165 done\n",
      "153 done\n",
      "169 done\n",
      "161 done\n",
      "157 done\n",
      "166 done\n",
      "170 done\n",
      "154 done\n",
      "158 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "171 done\n",
      "168 done\n",
      "156 done\n",
      "162 done\n",
      "160 done\n",
      "172 done\n",
      "169 done\n",
      "157 done\n",
      "161 done\n",
      "163 done\n",
      "173 done\n",
      "170 done\n",
      "158 done\n",
      "171 done\n",
      "159 done\n",
      "174 done\n",
      "172 done\n",
      "162 done\n",
      "160 done\n",
      "175 done\n",
      "164 done\n",
      "161 done\n",
      "173 done\n",
      "163 done\n",
      "176 done\n",
      "165 done\n",
      "174 done\n",
      "166 done\n",
      "177 done\n",
      "175 done\n",
      "178 done\n",
      "164 done\n",
      "167 done\n",
      "162 done\n",
      "176 done\n",
      "168 done\n",
      "165 done\n",
      "179 done\n",
      "163 done\n",
      "169 done\n",
      "166 done\n",
      "177 done\n",
      "170 done\n",
      "180 done\n",
      "178 done\n",
      "167 done\n",
      "181 done\n",
      "164 done\n",
      "171 done\n",
      "179 done\n",
      "168 done\n",
      "172 done\n",
      "165 done\n",
      "182 done\n",
      "169 done\n",
      "166 done\n",
      "180 done\n",
      "170 done\n",
      "173 done\n",
      "183 done\n",
      "181 done\n",
      "171 done\n",
      "174 done\n",
      "167 done\n",
      "184 done\n",
      "172 done\n",
      "182 done\n",
      "175 done\n",
      "168 done\n",
      "185 done\n",
      "173 done\n",
      "176 done\n",
      "183 done\n",
      "169 done\n",
      "186 done\n",
      "174 done\n",
      "170 done\n",
      "184 done\n",
      "177 done\n",
      "187 done\n",
      "171 done\n",
      "175 done\n",
      "185 done\n",
      "178 done\n",
      "188 done\n",
      "172 done\n",
      "176 done\n",
      "186 done\n",
      "179 done\n",
      "189 done\n",
      "177 done\n",
      "187 done\n",
      "173 done\n",
      "180 done\n",
      "190 done\n",
      "174 done\n",
      "178 done\n",
      "188 done\n",
      "181 done\n",
      "189 done\n",
      "179 done\n",
      "191 done\n",
      "175 done\n",
      "182 done\n",
      "192 done\n",
      "180 done\n",
      "190 done\n",
      "176 done\n",
      "193 done\n",
      "183 done\n",
      "181 done\n",
      "194 done\n",
      "177 done\n",
      "184 done\n",
      "195 done\n",
      "191 done\n",
      "182 done\n",
      "178 done\n",
      "185 done\n",
      "192 done\n",
      "196 done\n",
      "183 done\n",
      "179 done\n",
      "186 done\n",
      "193 done\n",
      "197 done\n",
      "184 done\n",
      "180 done\n",
      "187 done\n",
      "198 done\n",
      "194 done\n",
      "185 done\n",
      "181 done\n",
      "199 done\n",
      "188 done\n",
      "186 done\n",
      "195 done\n",
      "182 done\n",
      "187 done\n",
      "189 done\n",
      "200 done\n",
      "196 done\n",
      "183 done\n",
      "188 done\n",
      "190 done\n",
      "197 done\n",
      "201 done\n",
      "184 done\n",
      "189 done\n",
      "198 done\n",
      "202 done\n",
      "190 done\n",
      "185 done\n",
      "191 done\n",
      "199 done\n",
      "191 done\n",
      "203 done\n",
      "186 done\n",
      "192 done\n",
      "200 done\n",
      "192 done\n",
      "204 done\n",
      "193 done\n",
      "187 done\n",
      "201 done\n",
      "193 done\n",
      "194 done\n",
      "188 done\n",
      "205 done\n",
      "194 done\n",
      "202 done\n",
      "189 done\n",
      "206 done\n",
      "195 done\n",
      "195 done\n",
      "207 done\n",
      "190 done\n",
      "203 done\n",
      "196 done\n",
      "204 done\n",
      "196 done\n",
      "208 done\n",
      "197 done\n",
      "197 done\n",
      "191 done\n",
      "209 done\n",
      "198 done\n",
      "205 done\n",
      "198 done\n",
      "210 done\n",
      "192 done\n",
      "199 done\n",
      "206 done\n",
      "199 done\n",
      "200 done\n",
      "193 done\n",
      "211 done\n",
      "207 done\n",
      "200 done\n",
      "194 done\n",
      "208 done\n",
      "201 done\n",
      "201 done\n",
      "195 done\n",
      "212 done\n",
      "202 done\n",
      "209 done\n",
      "202 done\n",
      "213 done\n",
      "196 done\n",
      "203 done\n",
      "210 done\n",
      "214 done\n",
      "197 done\n",
      "203 done\n",
      "204 done\n",
      "215 done\n",
      "211 done\n",
      "198 done\n",
      "204 done\n",
      "205 done\n",
      "216 done\n",
      "206 done\n",
      "199 done\n",
      "212 done\n",
      "217 done\n",
      "207 done\n",
      "205 done\n",
      "218 done\n",
      "200 done\n",
      "213 done\n",
      "206 done\n",
      "208 done\n",
      "219 done\n",
      "214 done\n",
      "209 done\n",
      "207 done\n",
      "201 done\n",
      "215 done\n",
      "220 done\n",
      "210 done\n",
      "208 done\n",
      "221 done\n",
      "202 done\n",
      "216 done\n",
      "209 done\n",
      "211 done\n",
      "203 done\n",
      "222 done\n",
      "217 done\n",
      "204 done\n",
      "218 done\n",
      "210 done\n",
      "212 done\n",
      "223 done\n",
      "205 done\n",
      "213 done\n",
      "211 done\n",
      "219 done\n",
      "206 done\n",
      "214 done\n",
      "224 done\n",
      "220 done\n",
      "212 done\n",
      "215 done\n",
      "225 done\n",
      "207 done\n",
      "221 done\n",
      "213 done\n",
      "216 done\n",
      "208 done\n",
      "214 done\n",
      "226 done\n",
      "222 done\n",
      "217 done\n",
      "209 done\n",
      "215 done\n",
      "227 done\n",
      "218 done\n",
      "223 done\n",
      "210 done\n",
      "216 done\n",
      "228 done\n",
      "219 done\n",
      "211 done\n",
      "217 done\n",
      "229 done\n",
      "224 done\n",
      "220 done\n",
      "225 done\n",
      "230 done\n",
      "218 done\n",
      "221 done\n",
      "212 done\n",
      "231 done\n",
      "219 done\n",
      "226 done\n",
      "222 done\n",
      "213 done\n",
      "232 done\n",
      "227 done\n",
      "220 done\n",
      "214 done\n",
      "223 done\n",
      "233 done\n",
      "221 done\n",
      "215 done\n",
      "228 done\n",
      "234 done\n",
      "222 done\n",
      "216 done\n",
      "224 done\n",
      "229 done\n",
      "235 done\n",
      "217 done\n",
      "225 done\n",
      "223 done\n",
      "236 done\n",
      "218 done\n",
      "230 done\n",
      "226 done\n",
      "237 done\n",
      "231 done\n",
      "219 done\n",
      "227 done\n",
      "224 done\n",
      "232 done\n",
      "238 done\n",
      "220 done\n",
      "225 done\n",
      "228 done\n",
      "233 done\n",
      "229 done\n",
      "239 done\n",
      "221 done\n",
      "226 done\n",
      "234 done\n",
      "230 done\n",
      "222 done\n",
      "240 done\n",
      "231 done\n",
      "227 done\n",
      "235 done\n",
      "241 done\n",
      "223 done\n",
      "232 done\n",
      "236 done\n",
      "228 done\n",
      "237 done\n",
      "242 done\n",
      "233 done\n",
      "224 done\n",
      "229 done\n",
      "243 done\n",
      "238 done\n",
      "234 done\n",
      "225 done\n",
      "230 done\n",
      "235 done\n",
      "244 done\n",
      "239 done\n",
      "226 done\n",
      "231 done\n",
      "236 done\n",
      "227 done\n",
      "240 done\n",
      "232 done\n",
      "237 done\n",
      "245 done\n",
      "241 done\n",
      "238 done\n",
      "228 done\n",
      "233 done\n",
      "246 done\n",
      "242 done\n",
      "234 done\n",
      "229 done\n",
      "247 done\n",
      "239 done\n",
      "235 done\n",
      "243 done\n",
      "240 done\n",
      "248 done\n",
      "230 done\n",
      "236 done\n",
      "249 done\n",
      "241 done\n",
      "244 done\n",
      "231 done\n",
      "237 done\n",
      "250 done\n",
      "232 done\n",
      "242 done\n",
      "238 done\n",
      "245 done\n",
      "251 done\n",
      "233 done\n",
      "243 done\n",
      "246 done\n",
      "239 done\n",
      "252 done\n",
      "234 done\n",
      "247 done\n",
      "244 done\n",
      "240 done\n",
      "235 done\n",
      "253 done\n",
      "248 done\n",
      "241 done\n",
      "236 done\n",
      "249 done\n",
      "254 done\n",
      "245 done\n",
      "237 done\n",
      "250 done\n",
      "242 done\n",
      "255 done\n",
      "246 done\n",
      "238 done\n",
      "243 done\n",
      "256 done\n",
      "247 done\n",
      "251 done\n",
      "239 done\n",
      "248 done\n",
      "252 done\n",
      "244 done\n",
      "257 done\n",
      "240 done\n",
      "249 done\n",
      "253 done\n",
      "258 done\n",
      "241 done\n",
      "250 done\n",
      "245 done\n",
      "259 done\n",
      "254 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "242 done\n",
      "255 done\n",
      "261 done\n",
      "252 done\n",
      "243 done\n",
      "247 done\n",
      "256 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "244 done\n",
      "257 done\n",
      "258 done\n",
      "249 done\n",
      "263 done\n",
      "254 done\n",
      "264 done\n",
      "250 done\n",
      "259 done\n",
      "245 done\n",
      "255 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "256 done\n",
      "265 done\n",
      "261 done\n",
      "252 done\n",
      "247 done\n",
      "257 done\n",
      "266 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "258 done\n",
      "267 done\n",
      "249 done\n",
      "263 done\n",
      "259 done\n",
      "268 done\n",
      "254 done\n",
      "250 done\n",
      "264 done\n",
      "260 done\n",
      "269 done\n",
      "255 done\n",
      "251 done\n",
      "270 done\n",
      "261 done\n",
      "256 done\n",
      "265 done\n",
      "252 done\n",
      "271 done\n",
      "262 done\n",
      "257 done\n",
      "266 done\n",
      "253 done\n",
      "272 done\n",
      "258 done\n",
      "263 done\n",
      "267 done\n",
      "264 done\n",
      "273 done\n",
      "259 done\n",
      "254 done\n",
      "268 done\n",
      "274 done\n",
      "260 done\n",
      "255 done\n",
      "269 done\n",
      "265 done\n",
      "261 done\n",
      "256 done\n",
      "275 done\n",
      "270 done\n",
      "266 done\n",
      "262 done\n",
      "257 done\n",
      "267 done\n",
      "276 done\n",
      "271 done\n",
      "258 done\n",
      "268 done\n",
      "263 done\n",
      "277 done\n",
      "272 done\n",
      "269 done\n",
      "259 done\n",
      "264 done\n",
      "278 done\n",
      "273 done\n",
      "270 done\n",
      "260 done\n",
      "279 done\n",
      "274 done\n",
      "261 done\n",
      "265 done\n",
      "271 done\n",
      "280 done\n",
      "272 done\n",
      "275 done\n",
      "262 done\n",
      "281 done\n",
      "266 done\n",
      "273 done\n",
      "276 done\n",
      "263 done\n",
      "282 done\n",
      "267 done\n",
      "277 done\n",
      "274 done\n",
      "264 done\n",
      "268 done\n",
      "283 done\n",
      "278 done\n",
      "275 done\n",
      "269 done\n",
      "284 done\n",
      "279 done\n",
      "276 done\n",
      "265 done\n",
      "280 done\n",
      "277 done\n",
      "270 done\n",
      "285 done\n",
      "266 done\n",
      "281 done\n",
      "278 done\n",
      "271 done\n",
      "267 done\n",
      "286 done\n",
      "282 done\n",
      "272 done\n",
      "279 done\n",
      "268 done\n",
      "287 done\n",
      "283 done\n",
      "280 done\n",
      "269 done\n",
      "273 done\n",
      "288 done\n",
      "270 done\n",
      "284 done\n",
      "281 done\n",
      "274 done\n",
      "282 done\n",
      "271 done\n",
      "289 done\n",
      "285 done\n",
      "275 done\n",
      "272 done\n",
      "283 done\n",
      "286 done\n",
      "276 done\n",
      "290 done\n",
      "284 done\n",
      "277 done\n",
      "273 done\n",
      "291 done\n",
      "287 done\n",
      "285 done\n",
      "278 done\n",
      "274 done\n",
      "288 done\n",
      "292 done\n",
      "286 done\n",
      "279 done\n",
      "275 done\n",
      "293 done\n",
      "289 done\n",
      "280 done\n",
      "287 done\n",
      "276 done\n",
      "290 done\n",
      "281 done\n",
      "288 done\n",
      "277 done\n",
      "294 done\n",
      "291 done\n",
      "282 done\n",
      "278 done\n",
      "295 done\n",
      "289 done\n",
      "292 done\n",
      "283 done\n",
      "279 done\n",
      "296 done\n",
      "284 done\n",
      "293 done\n",
      "280 done\n",
      "290 done\n",
      "297 done\n",
      "285 done\n",
      "281 done\n",
      "291 done\n",
      "298 done\n",
      "294 done\n",
      "282 done\n",
      "286 done\n",
      "295 done\n",
      "292 done\n",
      "283 done\n",
      "296 done\n",
      "299 done\n",
      "287 done\n",
      "284 done\n",
      "293 done\n",
      "297 done\n",
      "288 done\n",
      "300 done\n",
      "285 done\n",
      "301 done\n",
      "298 done\n",
      "294 done\n",
      "289 done\n",
      "286 done\n",
      "302 done\n",
      "295 done\n",
      "299 done\n",
      "303 done\n",
      "290 done\n",
      "287 done\n",
      "296 done\n",
      "300 done\n",
      "291 done\n",
      "304 done\n",
      "301 done\n",
      "288 done\n",
      "297 done\n",
      "292 done\n",
      "305 done\n",
      "302 done\n",
      "289 done\n",
      "298 done\n",
      "306 done\n",
      "293 done\n",
      "303 done\n",
      "290 done\n",
      "307 done\n",
      "299 done\n",
      "291 done\n",
      "304 done\n",
      "294 done\n",
      "300 done\n",
      "308 done\n",
      "295 done\n",
      "305 done\n",
      "309 done\n",
      "292 done\n",
      "301 done\n",
      "310 done\n",
      "296 done\n",
      "302 done\n",
      "306 done\n",
      "293 done\n",
      "311 done\n",
      "297 done\n",
      "307 done\n",
      "303 done\n",
      "298 done\n",
      "308 done\n",
      "312 done\n",
      "294 done\n",
      "304 done\n",
      "309 done\n",
      "295 done\n",
      "313 done\n",
      "299 done\n",
      "305 done\n",
      "310 done\n",
      "296 done\n",
      "300 done\n",
      "314 done\n",
      "306 done\n",
      "311 done\n",
      "297 done\n",
      "301 done\n",
      "307 done\n",
      "315 done\n",
      "298 done\n",
      "312 done\n",
      "302 done\n",
      "308 done\n",
      "316 done\n",
      "313 done\n",
      "303 done\n",
      "309 done\n",
      "299 done\n",
      "317 done\n",
      "314 done\n",
      "310 done\n",
      "300 done\n",
      "318 done\n",
      "304 done\n",
      "301 done\n",
      "311 done\n",
      "315 done\n",
      "305 done\n",
      "302 done\n",
      "316 done\n",
      "312 done\n",
      "319 done\n",
      "306 done\n",
      "303 done\n",
      "317 done\n",
      "313 done\n",
      "307 done\n",
      "320 done\n",
      "308 done\n",
      "318 done\n",
      "304 done\n",
      "309 done\n",
      "314 done\n",
      "305 done\n",
      "310 done\n",
      "319 done\n",
      "311 done\n",
      "315 done\n",
      "321 done\n",
      "306 done\n",
      "316 done\n",
      "322 done\n",
      "312 done\n",
      "307 done\n",
      "320 done\n",
      "313 done\n",
      "317 done\n",
      "323 done\n",
      "308 done\n",
      "318 done\n",
      "321 done\n",
      "309 done\n",
      "324 done\n",
      "314 done\n",
      "322 done\n",
      "310 done\n",
      "315 done\n",
      "325 done\n",
      "311 done\n",
      "319 done\n",
      "323 done\n",
      "326 done\n",
      "316 done\n",
      "324 done\n",
      "327 done\n",
      "312 done\n",
      "317 done\n",
      "320 done\n",
      "325 done\n",
      "313 done\n",
      "318 done\n",
      "328 done\n",
      "326 done\n",
      "321 done\n",
      "314 done\n",
      "327 done\n",
      "329 done\n",
      "319 done\n",
      "322 done\n",
      "315 done\n",
      "328 done\n",
      "330 done\n",
      "323 done\n",
      "316 done\n",
      "320 done\n",
      "329 done\n",
      "324 done\n",
      "331 done\n",
      "317 done\n",
      "321 done\n",
      "330 done\n",
      "325 done\n",
      "332 done\n",
      "318 done\n",
      "322 done\n",
      "326 done\n",
      "333 done\n",
      "331 done\n",
      "323 done\n",
      "327 done\n",
      "334 done\n",
      "332 done\n",
      "319 done\n",
      "324 done\n",
      "333 done\n",
      "328 done\n",
      "335 done\n",
      "334 done\n",
      "325 done\n",
      "320 done\n",
      "336 done\n",
      "329 done\n",
      "326 done\n",
      "335 done\n",
      "337 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 done\n",
      "321 done\n",
      "336 done\n",
      "327 done\n",
      "338 done\n",
      "322 done\n",
      "337 done\n",
      "328 done\n",
      "331 done\n",
      "323 done\n",
      "339 done\n",
      "338 done\n",
      "332 done\n",
      "324 done\n",
      "329 done\n",
      "340 done\n",
      "339 done\n",
      "325 done\n",
      "333 done\n",
      "340 done\n",
      "330 done\n",
      "326 done\n",
      "341 done\n",
      "334 done\n",
      "327 done\n",
      "341 done\n",
      "331 done\n",
      "335 done\n",
      "342 done\n",
      "332 done\n",
      "336 done\n",
      "328 done\n",
      "343 done\n",
      "342 done\n",
      "337 done\n",
      "333 done\n",
      "344 done\n",
      "343 done\n",
      "329 done\n",
      "345 done\n",
      "338 done\n",
      "344 done\n",
      "334 done\n",
      "346 done\n",
      "330 done\n",
      "339 done\n",
      "345 done\n",
      "335 done\n",
      "347 done\n",
      "340 done\n",
      "336 done\n",
      "331 done\n",
      "348 done\n",
      "346 done\n",
      "337 done\n",
      "347 done\n",
      "332 done\n",
      "341 done\n",
      "349 done\n",
      "348 done\n",
      "338 done\n",
      "333 done\n",
      "350 done\n",
      "342 done\n",
      "349 done\n",
      "334 done\n",
      "339 done\n",
      "351 done\n",
      "343 done\n",
      "350 done\n",
      "340 done\n",
      "335 done\n",
      "344 done\n",
      "351 done\n",
      "336 done\n",
      "352 done\n",
      "345 done\n",
      "341 done\n",
      "353 done\n",
      "337 done\n",
      "346 done\n",
      "352 done\n",
      "354 done\n",
      "342 done\n",
      "347 done\n",
      "338 done\n",
      "353 done\n",
      "355 done\n",
      "348 done\n",
      "343 done\n",
      "339 done\n",
      "356 done\n",
      "354 done\n",
      "344 done\n",
      "349 done\n",
      "340 done\n",
      "355 done\n",
      "345 done\n",
      "357 done\n",
      "350 done\n",
      "356 done\n",
      "346 done\n",
      "341 done\n",
      "358 done\n",
      "351 done\n",
      "347 done\n",
      "357 done\n",
      "359 done\n",
      "348 done\n",
      "360 done\n",
      "358 done\n",
      "342 done\n",
      "361 done\n",
      "352 done\n",
      "359 done\n",
      "349 done\n",
      "343 done\n",
      "360 done\n",
      "344 done\n",
      "353 done\n",
      "362 done\n",
      "350 done\n",
      "361 done\n",
      "345 done\n",
      "351 done\n",
      "363 done\n",
      "354 done\n",
      "362 done\n",
      "346 done\n",
      "364 done\n",
      "355 done\n",
      "363 done\n",
      "347 done\n",
      "365 done\n",
      "356 done\n",
      "364 done\n",
      "352 done\n",
      "348 done\n",
      "366 done\n",
      "365 done\n",
      "357 done\n",
      "353 done\n",
      "349 done\n",
      "366 done\n",
      "358 done\n",
      "367 done\n",
      "354 done\n",
      "367 done\n",
      "350 done\n",
      "359 done\n",
      "355 done\n",
      "368 done\n",
      "351 done\n",
      "360 done\n",
      "356 done\n",
      "368 done\n",
      "369 done\n",
      "361 done\n",
      "369 done\n",
      "357 done\n",
      "370 done\n",
      "352 done\n",
      "362 done\n",
      "370 done\n",
      "371 done\n",
      "358 done\n",
      "371 done\n",
      "353 done\n",
      "363 done\n",
      "372 done\n",
      "359 done\n",
      "372 done\n",
      "354 done\n",
      "364 done\n",
      "360 done\n",
      "355 done\n",
      "373 done\n",
      "373 done\n",
      "365 done\n",
      "356 done\n",
      "361 done\n",
      "374 done\n",
      "374 done\n",
      "357 done\n",
      "362 done\n",
      "366 done\n",
      "375 done\n",
      "375 done\n",
      "358 done\n",
      "363 done\n",
      "367 done\n",
      "359 done\n",
      "364 done\n",
      "376 done\n",
      "368 done\n",
      "360 done\n",
      "376 done\n",
      "365 done\n",
      "369 done\n",
      "361 done\n",
      "377 done\n",
      "370 done\n",
      "366 done\n",
      "377 done\n",
      "362 done\n",
      "378 done\n",
      "371 done\n",
      "379 done\n",
      "378 done\n",
      "367 done\n",
      "363 done\n",
      "372 done\n",
      "379 done\n",
      "380 done\n",
      "364 done\n",
      "368 done\n",
      "380 done\n",
      "373 done\n",
      "381 done\n",
      "365 done\n",
      "369 done\n",
      "381 done\n",
      "374 done\n",
      "382 done\n",
      "370 done\n",
      "382 done\n",
      "366 done\n",
      "371 done\n",
      "383 done\n",
      "375 done\n",
      "383 done\n",
      "367 done\n",
      "372 done\n",
      "384 done\n",
      "384 done\n",
      "368 done\n",
      "373 done\n",
      "385 done\n",
      "369 done\n",
      "376 done\n",
      "385 done\n",
      "374 done\n",
      "386 done\n",
      "370 done\n",
      "386 done\n",
      "377 done\n",
      "371 done\n",
      "375 done\n",
      "387 done\n",
      "387 done\n",
      "378 done\n",
      "372 done\n",
      "379 done\n",
      "388 done\n",
      "388 done\n",
      "373 done\n",
      "380 done\n",
      "376 done\n",
      "389 done\n",
      "389 done\n",
      "374 done\n",
      "381 done\n",
      "390 done\n",
      "390 done\n",
      "377 done\n",
      "375 done\n",
      "391 done\n",
      "382 done\n",
      "391 done\n",
      "378 done\n",
      "383 done\n",
      "379 done\n",
      "392 done\n",
      "392 done\n",
      "384 done\n"
     ]
    }
   ],
   "source": [
    "# words_sample = [\"pizza hut\", \"burger king\", \"south africa\", \"nasa\"]\n",
    "# del og_dict\n",
    "def calculate_sim(words, word1, max_sim, closest_word):\n",
    "    t = time.time()\n",
    "    i = 0\n",
    "    for word2 in words:\n",
    "        try:\n",
    "            sim = wiki2vec.similarity(\"_\".join(word1.lower().split()), \"_\".join(word2.split()))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                closest_word = word2\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    print (\"Original word: \", word1, \"Closest Word: \", closest_word)\n",
    "    print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-a))\n",
    "    sys.stdout.flush()\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_w2v(word1):\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    if word1 not in wiki2vec.wv.vocab:\n",
    "        print (\"Original word not in vocab\", word1)\n",
    "        return (closest_word, max_sim)\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, word1, max_sim, closest_word)\n",
    "    return word1, closest_word          \n",
    "\n",
    "a = time.time()\n",
    "\n",
    "# closest_word = closest_word_w2v(\"margherita pizza\")\n",
    "\n",
    "# closest_word_w2v(\"nelson mandela\")\n",
    "\n",
    "resolved = dict()\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    for res in executor.map(closest_word_w2v, failed):\n",
    "        resolved[res[0]] = res[1]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# w2v = KeyedVectors.load_word2vec_format(\"~/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(\"/home/vlead/enwiki_20180420_win10_300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "og_dict = deepcopy(wiki2vec.wv.vocab)\n",
    "for k in og_dict:\n",
    "    if \"/\" in k:\n",
    "        wiki2vec.wv.vocab[k.split(\"/\")[1].lower()] = wiki2vec.wv.vocab[k]\n",
    "        del wiki2vec.wv.vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005288124084472656\n",
      "0.00016236305236816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"abrkadabra\" in w2v.wv\n",
    "print (time.time()-a)\n",
    "a = time.time()\n",
    "try:\n",
    "    w2v.similarity(\"margherita_pizza\", \"abrkadabra\")\n",
    "except:    \n",
    "    print (time.time()-a)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2id_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gregory feist',\n",
       " 'a successful and less troubled adaptation',\n",
       " 'toolo',\n",
       " 'a supposed chaos emerald',\n",
       " 'szlachta profits',\n",
       " 'the special intervention',\n",
       " 'different parameter profiles',\n",
       " 'a consumption binge',\n",
       " 'a vaguely sentient malevolence',\n",
       " 'doss aviation',\n",
       " 'sturdier mechanical parts',\n",
       " 'the average financial aid',\n",
       " 'a classical choral group',\n",
       " 'actual authorship',\n",
       " 'a robotic european space agency lander',\n",
       " 'cook s sixth form',\n",
       " 'lahr s papers',\n",
       " 'unfair app store practices',\n",
       " 'both insurance',\n",
       " 'periodontal related diseases',\n",
       " 'the library windows',\n",
       " 'five new stories',\n",
       " 'metal working fluids',\n",
       " 'any medieval knight',\n",
       " 'even mysticism',\n",
       " 'the backscatter signal power',\n",
       " 'posc caesar',\n",
       " 'rodgers street',\n",
       " 'a follow up investigation sheriff s deputies',\n",
       " 'egduf muriel',\n",
       " 'four challengers',\n",
       " 'ramakrishna paramhansa',\n",
       " 'corrective rape cases',\n",
       " 'various internet chat rooms',\n",
       " 'the roc time',\n",
       " 'true detective s third season',\n",
       " 'the rescue leader',\n",
       " 'olden manor',\n",
       " 'f lux s systems',\n",
       " 'steatohepatitis or advanced fibrosis',\n",
       " 'two dfms',\n",
       " 'an x ray generating source',\n",
       " '38 year old stefano robotti',\n",
       " 'the most salient grammatical trait',\n",
       " 'resistance records',\n",
       " 'the old haymarket theatre',\n",
       " 'sustainability and transport policies',\n",
       " 'an indiscriminate aerial bombardment campaign',\n",
       " 'my private attachments',\n",
       " 'world famous venues',\n",
       " 'commercials',\n",
       " 'companion specification',\n",
       " 'the strange and mysterious everglades',\n",
       " 'a slow and awkward process',\n",
       " 'both characters origins',\n",
       " 'bacillus oleraceae',\n",
       " 'virendranath chatterjee',\n",
       " 'ethiopian army strength',\n",
       " 'a steric interaction',\n",
       " 'the first psycho pass series',\n",
       " 'french alexandrine',\n",
       " 'only two and three stars',\n",
       " 'some thirty leagues',\n",
       " 'vavi',\n",
       " 'those notices',\n",
       " 'behavioral health hospitalization',\n",
       " 'an irresistible panache',\n",
       " 'vukotinovi',\n",
       " 'lovecries',\n",
       " 'an eighth consecutive season',\n",
       " 'soviet flanks',\n",
       " 'the oldest written documents',\n",
       " 'serious pests',\n",
       " 'notably coal miners',\n",
       " 'his brother malik',\n",
       " 'both marine corps air facility walnut ridge',\n",
       " 'extreme religious tolerance',\n",
       " 'brickman s numerous critics',\n",
       " 'his old political rival',\n",
       " 'griquas',\n",
       " 'phatboy',\n",
       " 'a cross country movement',\n",
       " 'twenty two baptists',\n",
       " 'doug z goodstein',\n",
       " 'al zubayr ibn al awwam',\n",
       " 'an electronic hydraulic four speed automatic transmission',\n",
       " 'inland fisheries and nearshore marine fisheries',\n",
       " 'the best selling manga volume',\n",
       " 'e g bewl canoe club',\n",
       " 'intricate features',\n",
       " 'its associated instruments',\n",
       " 'chicago mayor ed kelly',\n",
       " 'a possibly infinite graph g',\n",
       " 'a secure third party provider',\n",
       " 'the assistant intelligence officer',\n",
       " 'other rich nations',\n",
       " 'the first two aircraft',\n",
       " 'the 1st commando company s renaming',\n",
       " 'the fourth doctor date',\n",
       " 'her restrained portrayal',\n",
       " 'the prospective licensees',\n",
       " 'the sinha family',\n",
       " 'this act polygamy',\n",
       " 'closely connected democratic societies',\n",
       " '552 squadrons',\n",
       " '170 interviews',\n",
       " '451 338 votes',\n",
       " 'water dredge',\n",
       " 'a modified phased development',\n",
       " 'the existing trident missile warhead',\n",
       " 'all earthly occurrences',\n",
       " 'their relatively temperature independent electrical properties',\n",
       " 'abstraktes',\n",
       " '1gw peak load',\n",
       " 'an organized pro zionist lobby',\n",
       " 'a young filipino girl',\n",
       " 'the weekday call',\n",
       " 'the st augustine',\n",
       " 'various global and regional legal and policy instruments',\n",
       " 'between 50 and 75 nuclear icbms',\n",
       " 'many biological molecules',\n",
       " 'a worked out chalk pit',\n",
       " 'general proceed',\n",
       " 'ppc and type in traffic',\n",
       " 'johann heinrich buttstett',\n",
       " 'surrounding landscaping',\n",
       " 'an eight speed gearbox',\n",
       " 'par activators thrombin',\n",
       " 'their own designated classes',\n",
       " 'demand document lifecycle management',\n",
       " 'the united kingdom pigeon fanciers',\n",
       " 'a different menu',\n",
       " 'strong beings',\n",
       " 'at least four countries',\n",
       " 'krieg s ship',\n",
       " 'their constant attendance',\n",
       " 'its largest patrol ships',\n",
       " 'the ch teaux forts',\n",
       " 'bo s succubus birth mother',\n",
       " 'sportswear international magazine',\n",
       " 'the more unconscious and implicit aspects',\n",
       " 'neither a change',\n",
       " 'nuremberg trial no',\n",
       " 'an american folklorist',\n",
       " 'the e tv s effect',\n",
       " 'a phase delay',\n",
       " 'only 26 hours',\n",
       " '20 ps',\n",
       " 'a clean uncluttered design',\n",
       " 'early social relations',\n",
       " 'karvajat',\n",
       " 'a dirty dancing festival',\n",
       " 'the first three base64 digits',\n",
       " 'american parachutes',\n",
       " 'college aged mormon men',\n",
       " 'prop driven and thruster type',\n",
       " 'the collapsing uyghur khaganate',\n",
       " 'three free schools',\n",
       " 'the first school committee',\n",
       " 'paul a georgescu',\n",
       " 'the sebha municipal council building',\n",
       " 'a mechanician',\n",
       " 'thomas fischer',\n",
       " 'indian cavalry regiments',\n",
       " 'a one month visit',\n",
       " 'an einsatzgruppe',\n",
       " 'so called geeky gamers',\n",
       " 'the t antigen oncogene',\n",
       " 'distinguished service medals',\n",
       " 'said website',\n",
       " 'the secretaria de gobernacion s website',\n",
       " 'his deputy retired and lawson hicks',\n",
       " 'its new orleans backdrop',\n",
       " 'glucose insulin interaction',\n",
       " 'problem reaction solution',\n",
       " 'some music bands',\n",
       " 'five malaysian sailors',\n",
       " 'legal medical subjects',\n",
       " 'one particular treatment',\n",
       " 'pixels resolution',\n",
       " 'the school s department',\n",
       " 'digital microscopy software',\n",
       " 'j p queille',\n",
       " 'the main planetoid',\n",
       " 'strategic blind spots',\n",
       " 'the eastern fashion',\n",
       " '21 year old julia roberts',\n",
       " 'several embarrassing mistakes',\n",
       " 'the final province',\n",
       " 'the first honda ascot',\n",
       " 'sonny s widow',\n",
       " 'the izrael pozna ski palace',\n",
       " 'local and national measures',\n",
       " 'his last us appearance',\n",
       " 'the diet coke',\n",
       " 'the sixth mughal emperor',\n",
       " 'whose first model',\n",
       " 'french gaule',\n",
       " 'gowri tiruselvan',\n",
       " 'sampat pal devi',\n",
       " 'a large empty warehouse building',\n",
       " 'john glas',\n",
       " 'this temperature profile',\n",
       " 'carlo marx',\n",
       " 'the original r100',\n",
       " 'nawab mansur khan',\n",
       " 'his moral precepts',\n",
       " 'nsc terrorism specialist richard a clarke',\n",
       " 'then huddington court',\n",
       " 'to vilnius',\n",
       " 'jamie glover',\n",
       " 'the much larger unceded territory',\n",
       " 'last spark',\n",
       " 'e g a dna molecule',\n",
       " 'only seven places',\n",
       " 'the stark traumatic reality',\n",
       " 'tylgiv',\n",
       " 'u 20 world cup',\n",
       " 'valtra',\n",
       " 'j b kelly',\n",
       " 'valve s managing director',\n",
       " 'the ship s carpenters',\n",
       " 'various climatic regions',\n",
       " 'olga s unthinking comments',\n",
       " 'very strong springs',\n",
       " 'controversial modifications',\n",
       " 'k1 m hauterivian barremian age',\n",
       " 'plaxton s castle logo',\n",
       " 'the national narrative',\n",
       " 'petroleum migration',\n",
       " 'the potential invasion',\n",
       " 'crime lord jabba desilijic tiure s palace',\n",
       " 'forest growth simulation',\n",
       " 'essone',\n",
       " 'antonio cubillo',\n",
       " 'a particularly devastating storm surge',\n",
       " 'the top ten category',\n",
       " 'growing political awareness',\n",
       " 'bentayga',\n",
       " 'cyndi silverman',\n",
       " 'his last 20 innings',\n",
       " 'nucleotide flipping',\n",
       " 'no time and concrete definition',\n",
       " '143 911 km',\n",
       " '1926 january',\n",
       " 'the amalgamators dismay',\n",
       " 'a habitually spoken language',\n",
       " 'always part',\n",
       " 'the k blades',\n",
       " 'the town s fashionable sindlesham mill nightclub',\n",
       " 'gottrockets',\n",
       " 'the scarf',\n",
       " 'free argumentation',\n",
       " 'the power up kit',\n",
       " 'magnus de la gardie s 7 000 men',\n",
       " 'all montgomery public buses',\n",
       " 'thus void clearance',\n",
       " 'fsa performance',\n",
       " 'vere beauclerk',\n",
       " 'kakashi s design',\n",
       " 'brent s rest',\n",
       " 'lecreme',\n",
       " 'other bulk exports',\n",
       " 'regular figures',\n",
       " 'washoe county',\n",
       " 'his reflection coefficients',\n",
       " 'unique item identification',\n",
       " 'the risorgimento museum',\n",
       " 'formerly digidesign',\n",
       " 'husky',\n",
       " 'soundtrack credits',\n",
       " 'the primary strain',\n",
       " 'transmitter pulse repetition frequency',\n",
       " 'a cytosolic nadp dependent isoform',\n",
       " 'around 7 m',\n",
       " 'a secretive and exclusive sisterhood',\n",
       " 'blaine s wariness',\n",
       " 'her first attention',\n",
       " 'earth s geography',\n",
       " 'her successive owners',\n",
       " 'infinite plates',\n",
       " 'otherwise conventional surgery',\n",
       " 'k hex 8 reg resource list',\n",
       " 'a sizable budget surplus',\n",
       " 'old fashioned film serials',\n",
       " 'raf kemble',\n",
       " 'nrel',\n",
       " 'cold and stolid exclusiveness',\n",
       " 'over 300 francs',\n",
       " 'reflexive verbs',\n",
       " 'covered exterior galleries',\n",
       " 'his 1938 book',\n",
       " 'remora remora',\n",
       " 'line ls',\n",
       " 'servicios especiales de vigilancia',\n",
       " 'daimy ukon takayama',\n",
       " '1up com s staff',\n",
       " 'specific festivities',\n",
       " 'the 2008 q awards',\n",
       " 'robert delaunay',\n",
       " 'the alpine brigade alpine brigade orobica',\n",
       " 'remonta',\n",
       " 'matt wittaker',\n",
       " 'the 9 11 attacks plot',\n",
       " 'individual collectors web sites',\n",
       " 'often security',\n",
       " 'tenchteri',\n",
       " 'other such differences',\n",
       " 'non literary fiction',\n",
       " 'onstage props',\n",
       " 'lutherrose',\n",
       " 'eventually publisher',\n",
       " 'historian lewis collins',\n",
       " 'his hungarian relatives',\n",
       " 'hypocras',\n",
       " 'hulu accounts',\n",
       " 'inbound call',\n",
       " 'machakos teachers training college',\n",
       " 'significantly enriched or depleted groups',\n",
       " 'a high school graduate',\n",
       " 'the biggest ever animated opening',\n",
       " 'about 1870 travelers',\n",
       " 'metropolitan governments',\n",
       " 'a shorter fencer',\n",
       " 'the critic irving babbit',\n",
       " 'sarayonu',\n",
       " 'the famous legend',\n",
       " 'project abstracts',\n",
       " 'his trial and felony convictions',\n",
       " 'a stolen moped',\n",
       " 'lex mercatoria',\n",
       " 'a large neck brace',\n",
       " 'the caltech erato team',\n",
       " 'telkom sandhy putra vocational school',\n",
       " 'peruvian wheat',\n",
       " 'the film visibility',\n",
       " 'a post production supervisor',\n",
       " 'the corrupted heptaphonia',\n",
       " 'their original protestant denominations',\n",
       " 'an overwhelming burden',\n",
       " 'the 2007 auto club',\n",
       " 'the lesser understood aspects',\n",
       " 'simon o kelly',\n",
       " 'the least expensive wood bowls',\n",
       " 'the multiwinians',\n",
       " 'esme weatherwax s late maternal grandmother',\n",
       " 'the preamp sections',\n",
       " 'these newly discovered regions',\n",
       " 'a haphazard means',\n",
       " 'the specialist services',\n",
       " '2nd world exhibition',\n",
       " 'a teenage hitchhiker',\n",
       " 'the normal sales channels',\n",
       " 'good and sufficient reasons',\n",
       " 'leap advisory board member',\n",
       " 'the country s first general elections',\n",
       " 'the heavily fortified scheldt estuary',\n",
       " 'the lakeside prags wildbad hotel',\n",
       " 'the adoption procedure',\n",
       " 'lle',\n",
       " 'arte video',\n",
       " 'robin s appearance',\n",
       " 'the longest episode',\n",
       " 'his fifty daughters',\n",
       " 'ddr3 ecc',\n",
       " 'the major artistic figures',\n",
       " '54 46',\n",
       " 'this unusual claim',\n",
       " 'her question',\n",
       " 'mennis office',\n",
       " 'advanced wireless services bands',\n",
       " 'their proposed storyboard',\n",
       " 'valiant tragic self sacrificing heroes',\n",
       " 'piopio',\n",
       " 'ionia',\n",
       " 'the national force',\n",
       " 'ongoing behavioral reinforcement',\n",
       " 'global tree growth',\n",
       " 'corkscrew fashion',\n",
       " 'free software open source software',\n",
       " 'a deciphering section',\n",
       " 'no commercial power',\n",
       " 'travalio',\n",
       " 'the best spark plugs',\n",
       " 'leeds college',\n",
       " 'watchpoint media',\n",
       " 'the 2 58 mounted artillery regiment',\n",
       " 'a six person black americans',\n",
       " 'its box office reception',\n",
       " 'both the whistleblower',\n",
       " 'bearys cultural forum',\n",
       " 'especially the priority',\n",
       " 'the owl s men',\n",
       " 'the coming scientific and educational congress',\n",
       " 'chelms',\n",
       " 'elena anaya',\n",
       " '385 509 people',\n",
       " 'pargolovsky',\n",
       " 'three different album',\n",
       " 'the only dimensions',\n",
       " 'the newly formed cells',\n",
       " 'his polemical approach',\n",
       " 'smithiverse',\n",
       " 'walking eagle',\n",
       " 'projected information',\n",
       " 'a fully dressed man',\n",
       " 'konoha s leadership',\n",
       " 'many display consoles',\n",
       " 'the demanded target bitrate',\n",
       " 'introvigne s and bromley s conclusion',\n",
       " 'the eastfield industrial estate',\n",
       " 'australian graziers',\n",
       " 'their advanced position',\n",
       " 'a mansion block',\n",
       " 'a total re localization',\n",
       " 'a medical missionary dispensary',\n",
       " 'photographic society',\n",
       " 'all charioteers',\n",
       " 'mdib',\n",
       " 'chad s scenes',\n",
       " 'yellow fellow trade paper',\n",
       " 'caralynn lippo',\n",
       " 'jrt arm',\n",
       " 'many middle class white students',\n",
       " 'wayne crawford',\n",
       " 'i e mayor',\n",
       " 'these colonial prejudices',\n",
       " 'a later danish chronicler lists',\n",
       " 'the 2 0 v',\n",
       " 'their own honor',\n",
       " 'the classical ge ez language',\n",
       " 'a general protective chaperone',\n",
       " 'cardinal john',\n",
       " 'saint charbel makhlouf',\n",
       " 'choanomphala',\n",
       " 'two slightly displaced pictures',\n",
       " 'fanciful hopes',\n",
       " 'its implementation weld',\n",
       " 'pre execution request',\n",
       " 'walter jungkind',\n",
       " 'certain data dependent conditions',\n",
       " 'the 2008 beijing olympics opening ceremony',\n",
       " 'around 100 000 daily commuters',\n",
       " 'far other work',\n",
       " 'land ownership patterns',\n",
       " 'a heating and air conditioning equipment manufacturer',\n",
       " 'doctor devon',\n",
       " 'hoepner s mission',\n",
       " 'oregon house district',\n",
       " 'operation chanchera',\n",
       " 'their 63rd boul',\n",
       " 'extremist attacks',\n",
       " 'e ntpdases',\n",
       " 'the hardest period',\n",
       " 'the breaking crest line',\n",
       " 'giora shamis',\n",
       " 'the harps instrument',\n",
       " 'the fewest number',\n",
       " 'other similar questions',\n",
       " 'a navigational buoy',\n",
       " 'unmixed turbofans',\n",
       " 'united states overall imported products',\n",
       " 'the only monolithic structure',\n",
       " 'upwards of 100 different species',\n",
       " 'bulgaria s diplomatic policy',\n",
       " 'gallitas',\n",
       " 'therefore the change',\n",
       " 'the exhibit s wharf section',\n",
       " 'one hypercube node',\n",
       " '1 935 kg',\n",
       " 'agricultural rights advocacy group',\n",
       " 'each foreign key',\n",
       " 'all algeria players',\n",
       " 'one to three men',\n",
       " 'specific theorists',\n",
       " 'industrial control',\n",
       " 'the central crankshaft',\n",
       " 'the mapillas',\n",
       " 'the lower the damping effect',\n",
       " 'modern day loughgall',\n",
       " 'a 2 4 million property',\n",
       " '708 birds',\n",
       " 'luis peluffo',\n",
       " 'the nung minority',\n",
       " 'other ibm technologies',\n",
       " 'the styling distinctiveness',\n",
       " 'ohio state athletic director gene smith',\n",
       " 'all vulnerable machines',\n",
       " 'david landes use',\n",
       " 'eugene haynes',\n",
       " 'purdue identification',\n",
       " 'avicenna s canon',\n",
       " 'a higher impact version',\n",
       " 'mounting fiscal imbalances',\n",
       " 'a more emphatic reprise',\n",
       " 'the so called matitone',\n",
       " 'port wednesday',\n",
       " 'an opulent restaurant',\n",
       " 'the semantic meanings',\n",
       " 'its star bruce campbell',\n",
       " 'jun s first z32',\n",
       " 'the additional 2 mb',\n",
       " '32 cores',\n",
       " 'many monopolistic positions',\n",
       " 'mostly positive critics',\n",
       " 'july 24 virgin galactic astronaut pilots group',\n",
       " 'evolutionary urges',\n",
       " 'a young lena',\n",
       " 'the presentation parties',\n",
       " 'a ritual suicide',\n",
       " 'the irish tv licence',\n",
       " '0k the remaining voltage change',\n",
       " 'balluku',\n",
       " 'christianity relativised',\n",
       " 'his normal employment',\n",
       " 'post mortem privacy rights',\n",
       " 'powered feedwater pumps',\n",
       " 'soon enough harrington',\n",
       " 'a seductive and charismatic facade',\n",
       " 'an upcoming rivalry',\n",
       " 'personal honor',\n",
       " 'film pioneer d w griffith',\n",
       " 'the high profile repeal',\n",
       " 'other important secondary legal sources',\n",
       " 'the asian payment network',\n",
       " 'pictured holding copies',\n",
       " 'its cutting edge it division',\n",
       " 'approximately 76 3',\n",
       " 'germany s die zeit newspaper',\n",
       " 'the first malaysian company',\n",
       " 'kalyankari',\n",
       " 'asset labeling',\n",
       " 'the vivian s brigade',\n",
       " 'the polaris project',\n",
       " 'many famous wrestlers',\n",
       " 'a canoe accident',\n",
       " 'land and water management activities',\n",
       " 'detect',\n",
       " 'glue dots',\n",
       " 'the portrait d un m decin militaire',\n",
       " 'prodrive s share',\n",
       " 'the thermal modification',\n",
       " 'a double height ceiling',\n",
       " 'simanta',\n",
       " 'the ultimate fighter',\n",
       " 'a loan fee',\n",
       " 'the typical energy dispersive spectrometer',\n",
       " 'fire support base coral',\n",
       " 'community developed puzzles',\n",
       " 'the sherrinford staff',\n",
       " 'new third baseman troy glaus',\n",
       " 'my own private river',\n",
       " 'abc s rubik',\n",
       " 'swollen and ulcerated legs',\n",
       " 'the israeli startups',\n",
       " 'mostly male homosexuality',\n",
       " 'the ship s longtime captain',\n",
       " 'many indian film cultures',\n",
       " 'another chengde officer',\n",
       " 'an all black theme',\n",
       " 'advocate sigma huda',\n",
       " 'reiek',\n",
       " 'the grand arcades',\n",
       " 'the verein zur abwehr des antisemitismus',\n",
       " 'in 2019 an addition',\n",
       " 'screenwriter frederic raphael',\n",
       " 'an uncompromising attempt',\n",
       " 'the cheltenham studio audience',\n",
       " 'the trial court judge s views',\n",
       " 'ancient exegetes',\n",
       " 'many experience sexual arousal',\n",
       " 'top tier internet access providers',\n",
       " 'a medieval past',\n",
       " 'questionnaire results',\n",
       " 'richard barnet',\n",
       " 'small frequent exercise periods',\n",
       " '1930 undercut support',\n",
       " 'george troup',\n",
       " 'some pre established rules',\n",
       " 'toyota team principal john howett',\n",
       " 'a different tense',\n",
       " 'the fernpass example',\n",
       " 'hindustan times new delhi',\n",
       " 'more megastores',\n",
       " 'proportional dietary makeup',\n",
       " 'at least the later 15th century',\n",
       " 'some western europe states',\n",
       " 'the admd',\n",
       " 'the correlative right',\n",
       " 'the hi hat the savoy',\n",
       " 'the sigint operators',\n",
       " 'his white horse',\n",
       " 'the 1982 film tron',\n",
       " 'twenty three nations',\n",
       " '2000 pound bombs',\n",
       " 'unmethylated survivin promoters',\n",
       " 'flag hoisting ceremonies',\n",
       " 'a red wolf',\n",
       " 'the sports museum',\n",
       " 'ideal car handling',\n",
       " 'up to 310 000 kw',\n",
       " 'university one',\n",
       " 'uqba ibn',\n",
       " 'bdp 203 205',\n",
       " 'e g four bars',\n",
       " 'various printable ebooks',\n",
       " 'specific target animals',\n",
       " 'chlorophos',\n",
       " 'the western mongol khanates',\n",
       " 'wc 130',\n",
       " 'the academic rules',\n",
       " 'the ioannis metaxas dictatorship',\n",
       " 'the key organizations',\n",
       " 'some 50 000 to 100 000 casualties',\n",
       " 'any classical dance',\n",
       " '7500 m',\n",
       " 'a hanging skeleton',\n",
       " 'about 1337 m',\n",
       " 'ak 41',\n",
       " '8051 machine code',\n",
       " 'forest gilman',\n",
       " 'his seventh deployment',\n",
       " 'i300 t',\n",
       " 'republican style',\n",
       " 'service funding',\n",
       " 'the historical glass harmonica',\n",
       " 'a successful peaceful eastward settlement',\n",
       " 'its comparatively frank depictions',\n",
       " 'the current rates',\n",
       " 'the hinder dogs',\n",
       " 'this supposed roc facility',\n",
       " 'dorothy sargent rosenberg poetry fellows',\n",
       " 'body woman',\n",
       " 'hamilton railway station group',\n",
       " 'the 782 million gross',\n",
       " 'important alumni',\n",
       " 'convicted mafia members',\n",
       " 'sampist t',\n",
       " 'hamilton windsor ironworks co',\n",
       " 'some 2600 people',\n",
       " 'some similar types',\n",
       " 'iqtisadi',\n",
       " 'price s resignation',\n",
       " 'pituitary tumor',\n",
       " 'mild cystinosis',\n",
       " 'strongly brisant hexogen filled warheads',\n",
       " 'a few special units',\n",
       " 'france s first tram train line',\n",
       " 'greek or syrian origin',\n",
       " 'particularly its appearance',\n",
       " 'the brazilian mangrove swamp',\n",
       " 'a restraining injunction',\n",
       " 'the catholic liberals',\n",
       " 'the first all navajo platoon',\n",
       " 'the college s regulations',\n",
       " 'their tribunes',\n",
       " 'the open usability project',\n",
       " 'advertisement requests',\n",
       " 'back donation',\n",
       " 'a stable vehicle',\n",
       " 'a highway pylon',\n",
       " 'citizen s initiative',\n",
       " 'an active patroness',\n",
       " 'diametroi',\n",
       " 'native american enslavement',\n",
       " 'arpita s husband',\n",
       " 'balcescu',\n",
       " 'its own 24 hour internet radio station',\n",
       " 'a sack like volva',\n",
       " 'riverview junior high school',\n",
       " 'tactile reading experiments',\n",
       " 'the 1951 unhcr convention',\n",
       " 'the ghq upper echelons',\n",
       " 'the hungarians ancestors',\n",
       " 'consistent semiclassical quantization condition',\n",
       " 'glider missions',\n",
       " 'his pursued ambiorix',\n",
       " 'the few scot exiles',\n",
       " 'temognatha',\n",
       " 'the machine s limited storage',\n",
       " 'the numeric primitives',\n",
       " 'the pressurized water reactor',\n",
       " 'sinyavino heights',\n",
       " 'the protestant regional theologiate',\n",
       " 'the small engine classes',\n",
       " 'the troms marine hospital',\n",
       " 'stolbizer',\n",
       " 'sutcliffe s only reason',\n",
       " 'their mentor s birthday',\n",
       " 'the most prominent depictions',\n",
       " 'the wahine',\n",
       " 'each customer profitability',\n",
       " 'his zero sluggers',\n",
       " 'its subsequent exoneration',\n",
       " 'river passaic',\n",
       " 'a reward scale',\n",
       " 'rodnoy',\n",
       " 'a local nature reserve',\n",
       " 'dubai al habab road',\n",
       " 'the favorite disaster',\n",
       " 'the imperial weimar and nazi era',\n",
       " 'the free rein',\n",
       " 'nicole de boer',\n",
       " 'radical acquaintances',\n",
       " 'the moulding spring',\n",
       " 'the jagodnja plateau',\n",
       " 'the reform league s popularity',\n",
       " 'already effective programs',\n",
       " '28 f',\n",
       " 'an american intercollegiate organization',\n",
       " 'a low scoring third test',\n",
       " 'captive bred saltwater crocodiles',\n",
       " 'methodological concepts',\n",
       " 'the moesii',\n",
       " 'a distinctive english form',\n",
       " 'underwater research center',\n",
       " 'mmixal',\n",
       " 'rothstein s new moroccan bank account',\n",
       " 'the labor department',\n",
       " '639 ce',\n",
       " 'linn sondek lp12',\n",
       " 'the wilder award',\n",
       " 'welding and component fabrication techniques',\n",
       " 'more than one strange quark',\n",
       " 'the surrounding pedestrian footpaths',\n",
       " 'the u s distribution rights',\n",
       " 'its other project',\n",
       " 'the signed approval',\n",
       " 'physician and hospital services',\n",
       " 'modernised africans',\n",
       " 'all newspaper industry labor disputes',\n",
       " 'c p thakur',\n",
       " 'pionier kompanie',\n",
       " 'the newly developed text based installer',\n",
       " 'the two biggest armies',\n",
       " 'a more carefree soul',\n",
       " 'the most meritorious performance',\n",
       " 'among which the tamil word',\n",
       " 'her father s unexplained disappearance',\n",
       " 'caledonian mercury',\n",
       " 'this new brand statement',\n",
       " 'henry saunders',\n",
       " 'stanstead grey granite',\n",
       " 'other manuals',\n",
       " 'chukrasia tabularis',\n",
       " 'the turkish chief',\n",
       " 'e g collision avoidance',\n",
       " 'developer chris klug',\n",
       " 'present day thamesmead',\n",
       " 'its public policy law center',\n",
       " 'as with ibritumomab rituximab targets',\n",
       " 'anti jewish disturbances',\n",
       " 'unoci s mandate',\n",
       " 'craugastor megalotympanum',\n",
       " 'windows 1 1a',\n",
       " 'allied snipers',\n",
       " 'namely the set theoretic complement',\n",
       " 'naval fleet',\n",
       " 'the term lifestyle',\n",
       " '15 largest financial services providers',\n",
       " 'most female headed households',\n",
       " 'block booking recording studios',\n",
       " 'sangita chudamani',\n",
       " 'the last threshold',\n",
       " 'the country s economic difficulties',\n",
       " 'three dutch lifeboats',\n",
       " 'julia michaels',\n",
       " 'holby city actress tina hobley',\n",
       " 'shahjehan',\n",
       " 'the simplified treaty revision procedure',\n",
       " 'community development honours',\n",
       " 'hodgson s other literary works',\n",
       " 'the former conservative supporter',\n",
       " 'the thynians',\n",
       " 'eritrean origin',\n",
       " 'two collateral agreements',\n",
       " 'ages ago',\n",
       " 'the newly formed u s department',\n",
       " 'an institutional and personal commitment',\n",
       " 'the power shortage',\n",
       " 'dr ludwig ingwer nommensen',\n",
       " 'f i s r',\n",
       " 'r as baixas',\n",
       " 'the stratojet',\n",
       " 'sticks gill east',\n",
       " 'advanced chemistry development',\n",
       " 'the technical default period',\n",
       " 'the dynamic yet sensitive type',\n",
       " 'the welsh women mean business awards',\n",
       " 'austria hungary s naval history',\n",
       " 'the smart cow problem',\n",
       " 'a western negev city',\n",
       " 'both charleston s aristocracy',\n",
       " 'iain moody',\n",
       " 'one s home world',\n",
       " 'grim old place',\n",
       " 'these errands',\n",
       " 'olszewski s tenureship',\n",
       " '540 location',\n",
       " 'relative sovereignty',\n",
       " 'serine protease inhibitors',\n",
       " 'glory statement',\n",
       " 'vdts',\n",
       " 'video library',\n",
       " 'tonsilitis',\n",
       " 'fundamentals concepts',\n",
       " 'the nation s payment system',\n",
       " 'assam independent',\n",
       " 'the injurious misrepresentation',\n",
       " 'certain major new york city newspapers',\n",
       " 'b ogo blissfulness',\n",
       " 'ad hoc student teams',\n",
       " 'aquaculture producers',\n",
       " 'm r zay',\n",
       " 'the read out probe',\n",
       " 'excessive dentine removal',\n",
       " 'b schrieke',\n",
       " 'home governments',\n",
       " 'ali shirazi',\n",
       " 'the longstanding u s proposal',\n",
       " 'the sapience',\n",
       " 'a chemically modified form',\n",
       " 'passiontide',\n",
       " 'such an authentic data source',\n",
       " 'nadiripari',\n",
       " 'japanese lawmakers',\n",
       " 'six graduate schools',\n",
       " 'the local transport department',\n",
       " 'rudd s policies',\n",
       " 'the historic liberty tree',\n",
       " 'another new option',\n",
       " 'football club portsmouth',\n",
       " 'shuxiang',\n",
       " 'the national space society s 2009 space pioneer award',\n",
       " 'solusi',\n",
       " 'a soloviev',\n",
       " 'trading efficiency',\n",
       " 'every outsider',\n",
       " 'their two buildings',\n",
       " 'their vast syllabus',\n",
       " 'three more resorts',\n",
       " 'coastal beaches',\n",
       " 'an ancient and worn leaden tablet',\n",
       " 'its initial package',\n",
       " '10 3 million dollars',\n",
       " 'these programming challenges',\n",
       " 'present weakness',\n",
       " 'a low interest rate',\n",
       " 'martha plimpton',\n",
       " 'other invading european armies',\n",
       " 'the gralsritual',\n",
       " 'centerline and touchdown zone lighting',\n",
       " 'the viscount cranborne estates',\n",
       " 'an interprocess communication technique',\n",
       " 'the eu s fifth largest economy',\n",
       " 'martin s run',\n",
       " 'newly released film',\n",
       " 'staten island s all time basketball team',\n",
       " 'the otis a singletary chair',\n",
       " 'the second generation m5',\n",
       " 'metastable excited oxygen',\n",
       " 'different state court orders',\n",
       " 'a more traditional werewolf',\n",
       " 'pro statehood supporters',\n",
       " 'an emanationist concept',\n",
       " 'a visa arrival',\n",
       " 'superstar drivers',\n",
       " 'the emergent disease state',\n",
       " 'centro escolar university',\n",
       " 'steady constant frequency conditions',\n",
       " 'the braid group',\n",
       " 'a ten year housing plan',\n",
       " 'her husband s mead',\n",
       " 'the t n palayam forest range',\n",
       " 'expert mental models',\n",
       " 'a second new single',\n",
       " 'additional user features',\n",
       " 'big ominous rocks',\n",
       " 'the tercio organizational model',\n",
       " 'edward r brennan',\n",
       " 'their singing telegram',\n",
       " 'quilty s murder',\n",
       " 'reservoir high schools',\n",
       " 'episcopal services',\n",
       " 'cipoletti',\n",
       " 'a science fiction mystery drama',\n",
       " 'hank azaria s reading',\n",
       " 'the nmf apps',\n",
       " 'three conceptual art exhibitions',\n",
       " 'bi directional mappings',\n",
       " 'ragaas',\n",
       " 'however the baltimore afro american s review',\n",
       " '10 field workshop',\n",
       " 'postmans ridge',\n",
       " 'gear reduction mechanism',\n",
       " 'a short medium range interceptor',\n",
       " 'moineddin r multiple maternities',\n",
       " 'the starch reserves',\n",
       " 'bridgewater police chief michael e stewart',\n",
       " 'proprioceptive effects',\n",
       " 'the israeli commander',\n",
       " 'several notable landmarks',\n",
       " 'the connecticut and swift river valleys',\n",
       " 'advisory committee meetings',\n",
       " 'over 30 youngsters',\n",
       " 'the troop increase',\n",
       " 'heterosexual gendered adults',\n",
       " 'kyrholmen',\n",
       " 'the biggest stupidity',\n",
       " 'variant shaped glyph',\n",
       " 'a technological enabler',\n",
       " 'the jabberwock s death fall',\n",
       " 'pasture commons',\n",
       " 'and condition testing statements',\n",
       " 'sansuiken',\n",
       " 'tweak',\n",
       " 'a controlled and concentrated manner',\n",
       " 'the early sydney roads',\n",
       " 'a unique health bar',\n",
       " 'zholy',\n",
       " 'undergraduate educational settings',\n",
       " 'directly generated employment',\n",
       " 'a pink color tests',\n",
       " 'the two particular greek officers',\n",
       " 'the newly departed temple',\n",
       " 'learner ratios',\n",
       " '256 genes',\n",
       " 'polangui pnp',\n",
       " 'verne s works',\n",
       " 'ruddiman s hypothesis',\n",
       " 'other overflights',\n",
       " 'a wandering cocotama',\n",
       " 'spatial features',\n",
       " 'the submillimeter telescope',\n",
       " '14 b 2',\n",
       " 'the track faithless',\n",
       " 'hipwood and school streets',\n",
       " 'lego like building blocks',\n",
       " 'variable montage',\n",
       " 'infamous slums',\n",
       " 'its axis neighbors',\n",
       " 'starro based spores',\n",
       " 'coronotion',\n",
       " 'former interim chancellor j keith motley',\n",
       " 'their own inner abilities',\n",
       " 'a convergent synthetic approach',\n",
       " 'the new electricity industry',\n",
       " 'the few black artists',\n",
       " 'the allard prize the oscars',\n",
       " 'its arboreal habits',\n",
       " 'a homiletic enumeration',\n",
       " 'matthew levitt s book',\n",
       " 'endothelial',\n",
       " 'based crt',\n",
       " 'vincent bollor',\n",
       " 'a medical evacuation arrangement',\n",
       " '100 to 110 aircraft',\n",
       " 'the long leadups',\n",
       " 'the f 35 s high costs',\n",
       " 'the band s march performances',\n",
       " 'major reactor plant and warhead maintenance',\n",
       " 'edincik su',\n",
       " 'the financial services authority indonesia ojk',\n",
       " 'the montane temperate rainforest',\n",
       " 'two ground observing satellites',\n",
       " 'estanislao asencio vel zquez',\n",
       " 'firearms accessories',\n",
       " 'sven bull neck',\n",
       " 'higher educated wealthy citizens',\n",
       " 'the mirror point',\n",
       " 'two to three times earth size',\n",
       " '3 standard',\n",
       " 'most sheriffs',\n",
       " 'a redbox',\n",
       " 'protein repressors',\n",
       " 'china dealer',\n",
       " 'other connecting transit systems',\n",
       " 'aldo leopold s essay',\n",
       " 'their pc platform',\n",
       " 'sz kely heiresses',\n",
       " 'at least hundreds of thousands',\n",
       " 'atsuo',\n",
       " 'brandy producers',\n",
       " 'any meal',\n",
       " 'coll ge notre dame de nazareth',\n",
       " 'a daily site',\n",
       " 'chemiakin',\n",
       " 'oe fugol',\n",
       " 'the 10th bn london regiment',\n",
       " 'the eland s limitations',\n",
       " 'schilderboek',\n",
       " 'the basel civil court',\n",
       " 'a new perk',\n",
       " 'three door hatchbacks',\n",
       " 'parastrephia lucida',\n",
       " 'the open arid grasslands',\n",
       " 'some twenty presidential unit citations',\n",
       " 'va he',\n",
       " 'kay s distinctive capabilities',\n",
       " 'chvr fm',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24768096 0.0012412071228027344\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "b = wiki2vec.similarity(\"sachin_tendulkar\", \"india\")\n",
    "print (b, time.time()-a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
