{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsddb3 import btopen\n",
    "import bcolz, pickle, os, shelve\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "\n",
    "prefix = \"/data/Vivek/Final/SIREN-Research/OntoEnricher/junk/Files/security_threshold_10_10/security\"\n",
    "train_file = \"../files/dataset/train.tsv\"\n",
    "test_file = \"../files/dataset/test.tsv\"\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove.dat\"\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "use_embeddings = \"../files/embeddings.pt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"synonym\", \"none\"]\n",
    "# relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "success = []\n",
    "\n",
    "\n",
    "\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db[str(entity_id)]\n",
    "    if db == id2path_db:\n",
    "        entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    if entity in db:\n",
    "        success.append(entity)\n",
    "        return int(db[entity])\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db[key]\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = vectors\n",
    "    except:\n",
    "        embeddings, word2idx = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "        \n",
    "\n",
    "def create_embeddings():\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = l.split()\n",
    "            word, vector = line[0], line[1:]\n",
    "            words.append(word)\n",
    "            vectors.append(np.array(vector).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    vectors = vectors.reshape((-1, EMBEDDING_DIM))\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "    \n",
    "    return vectors, word2idx\n",
    "\n",
    "word2id_db = shelve.open(prefix + \"_word_to_id_dict.db\", 'r')\n",
    "id2word_db = shelve.open(prefix + \"_id_to_word_dict.db\", \"r\")\n",
    "path2id_db = shelve.open(prefix + \"_path_to_id_dict.db\", \"r\")\n",
    "id2path_db = shelve.open(prefix + \"_id_to_path_dict.db\", \"r\")\n",
    "relations_db = shelve.open(prefix + \"_relations_map.db\", \"r\")\n",
    "\n",
    "embeddings, emb_indexer = load_embeddings_from_disk()\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "def extract_direction(edge):\n",
    "    \n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "    \n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = edge.split(\"/\")\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def extract_all_paths(x,y):\n",
    "    \n",
    "    paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "    x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "    y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "    path_count_dict = { id_to_entity(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    path_count_dict = { parse_path(path) : path_count_dict[path] for path in path_count_dict }\n",
    "\n",
    "    return { path : path_count_dict[path] for path in path_count_dict if path}\n",
    "    \n",
    "def parse_dataset(dataset):\n",
    "    keys = [(entity_to_id(word2id_db, x), entity_to_id(word2id_db, y)) for (x, y) in dataset]\n",
    "    paths = [extract_all_paths(x,y) for (x,y) in keys]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "    print('Pairs without paths:', len(empty), ', all dataset:', len(dataset))\n",
    "    embed_indices = [(emb_indexer.get(x,0), emb_indexer.get(y,0)) for (x,y) in keys]\n",
    "    return embed_indices, paths\n",
    "    \n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values())\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [i for (i,relation) in enumerate(dataset_vals)]\n",
    "\n",
    "embed_indices_train, embed_indices_test = np.array(embed_indices[:len(train_dataset)]), np.array(embed_indices[len(train_dataset):len(train_dataset)+len(test_dataset)])\n",
    "x_train, x_test = np.array(x[:len(train_dataset)]), np.array(x[len(train_dataset):len(train_dataset)+len(test_dataset)])\n",
    "y_train, y_test = np.array(y[:len(train_dataset)]), np.array(y[len(train_dataset):len(train_dataset)+len(test_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.hidden_dim = HIDDEN_DIM + 2 * EMBEDDING_DIM\n",
    "        self.input_dim = POS_DIM + DEP_DIM + EMBEDDING_DIM + DIR_DIM\n",
    "        self.W = nn.Linear(NUM_RELATIONS, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(len(embeddings), EMBEDDING_DIM)\n",
    "        self.word_embeddings.load_state_dict({'weight': torch.from_numpy(np.array(embeddings))})\n",
    "        self.word_embeddings.require_grad = False\n",
    "        \n",
    "        self.pos_embeddings = nn.Embedding(len(pos_indexer), POS_DIM)\n",
    "        self.dep_embeddings = nn.Embedding(len(dep_indexer), DEP_DIM)\n",
    "        self.dir_embeddings = nn.Embedding(len(dir_indexer), DIR_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, NUM_LAYERS)\n",
    "    \n",
    "    def embed_path(self, elem):\n",
    "        path, count = elem\n",
    "        if path in self.cache:\n",
    "            return cache[path] * count\n",
    "        lstm_inp = []\n",
    "        for edge in path:\n",
    "            inputs = [torch.Tensor([[el]]) for el in edge]\n",
    "            word_embed = self.dropout_layer(self.word_embeddings(inputs[0]))\n",
    "            pos_embed = self.dropout_layer(self.pos_embeddings(inputs[1]))\n",
    "            dep_embed = self.dropout_layer(self.dep_embeddings(inputs[2]))\n",
    "            dir_embed = self.dropout_layer(self.dir_embeddings(inputs[3]))\n",
    "            embeds = np.concatenate((word_embed, pos_embed, dep_embed, dir_embed))\n",
    "            lstm_inp.append(embeds)\n",
    "        output, _ = self.lstm(lstm_inp)\n",
    "        cache[path] = output\n",
    "\n",
    "        return output * count\n",
    "    \n",
    "    def forward(self, data, emb_indexer):\n",
    "        if not data:\n",
    "            data[NULL_PATH] = 1\n",
    "        print (\"Data: \", data)\n",
    "        num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = [np.sum([self.embed_path(path) for path in paths.items()]) for paths in data]\n",
    "        print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        h = [np.concatenate((self.word_embeddings(elem[0]), h[i], self.word_embeddings(elem[1]))) for i,emb in enumerate(emb_indexer)]\n",
    "        return self.softmax(self.W(h))\n",
    "\n",
    "HIDDEN_DIM = 60\n",
    "NUM_LAYERS = 2\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "batch_size = min(batch_size, dataset_size)\n",
    "num_batches = int(ceil(dataset_size/batch_size))\n",
    "\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "lstm = LSTM()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss, epoch_idx = 0, np.random.permutation(dataset_size)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch = epoch_idx[batch_start:batch_end]\n",
    "        \n",
    "        data, labels, embeddings_idx = x_train[batch], y_train[batch], embed_indices_train[batch]\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = lstm(data, embeddings_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= dataset_size\n",
    "    print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch + 1, num_epochs, total_loss))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictedLabels = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = lstm(data)\n",
    "        print (outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictedLabels.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3), (5904, 2, 4, 4)): 1,\n",
       "  ((38901, 1, 1, 1), (31, 2, 2, 2), (4045, 1, 3, 3)): 3},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "e = nn.Embedding(3, 3)\n",
    "ls = [[0, 1, 2], [3,4,5], [6,7,8]]\n",
    "ls = np.array([np.array(el) for el in ls])\n",
    "e.load_state_dict({'weight': torch.from_numpy(ls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3., 4.]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = np.array([np.sum([self.embed_path(path) for path in paths.items()]) for paths in data])\n",
    "        #print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        print (h.shape)\n",
    "        h = [np.concatenate((self.word_embeddings(emb[0]), h[i], self.word_embeddings(emb[1]))) for i,emb in enumerate(emb_indexer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = torch.randn(1,4)\n",
    "torch.Tensor([[1]]).shape\n",
    "# torch.cat((h, t.view(1,-1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5777, grad_fn=<NllLossBackward>) tensor([[ 1.1562, -0.1242,  0.1812,  0.3042,  0.9188],\n",
      "        [ 1.0961,  1.5061,  0.4816, -0.1618, -1.0943],\n",
      "        [ 0.8229,  0.5431, -0.4848,  1.8109, -0.8246]], requires_grad=True) tensor([4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputt = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(inputt, target)\n",
    "output.backward()\n",
    "print (output, inputt, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b3d1e24e4ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosestWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wansecure firewall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b3d1e24e4ccb>\u001b[0m in \u001b[0;36mclosestWord\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2id_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mword_to_compare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractUSEEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Took me {} seconds to extract USE embeddings...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1127\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     raise NotImplementedError(\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0;34m\"eval is not supported when eager execution is enabled, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \"is .numpy() what you're looking for?\")\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim=-1000):\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    closest_word = \"\"\n",
    "    with shelve.open(use_embeddings, 'c') as db:\n",
    "        for i, w in enumerate(word_embeddings):\n",
    "            db[words[i]] = \" \".join([str(s) for s in w])\n",
    "        closest_word_idx = np.argmax(awesome_cossim_topn(word_embeddings, word_to_compare.transpose(), 10, 0.85))\n",
    "        closest_word = words[closest_word_idx]\n",
    "        del word_embeddings\n",
    "    del db\n",
    "    return closest_word\n",
    "\n",
    "def closestWord(word, method=\"USE\"):\n",
    "    if method!=\"USE\":\n",
    "        return max([(w, SequenceMatcher(lambda x: x == \" \", word, w).ratio()) for w in list(word2id_db.keys())], key=lambda l:float(l[1]))\n",
    "    a = time.time()\n",
    "    word_to_compare = extractUSEEmbeddings([word]).eval(session=tf.compat.v1.Session())\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "    if os.path.isfile(use_embeddings):\n",
    "        with shelve.open(use_embeddings, 'r') as db:\n",
    "            embeds = [np.array([float(l) for l in e.split()]) for e in db.values()]\n",
    "            words = list(db.keys())\n",
    "            closest_word_idx = np.argmax(awesome_cossim_topn(embeds, word_to_compare.T, 10, 0.85))\n",
    "            closest_word = words[closest_word_idx]\n",
    "    else:\n",
    "        words = list(word2id_db.keys())\n",
    "        print (\"Obtained list of words\")\n",
    "#         len_part = 100000\n",
    "        max_sim = -1000\n",
    "#         n_parts = ceil(len(words)/len_part)\n",
    "#         for i in range(n_parts):\n",
    "#             words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = compare_sim(words, word_to_compare, max_sim)\n",
    "\n",
    "    \n",
    "    return closest_word, max_sim\n",
    "    \n",
    "closest_word = closestWord(\"wansecure firewall\")\n",
    "closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest word: ['neapolitan pizza', 'pan pizza', 'authentic neapolitan pizza'] Calculated in:  5.890500783920288 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "import difflib\n",
    "# closest_word, sim = closestWord(\"neopolitan pizza\", \"sm\")\n",
    "res = difflib.get_close_matches(\"neopolitan pizza\", list(word2id_db.keys()))\n",
    "print (\"closest word:\", res, \"Calculated in: \", time.time() - t, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took me 0.00010180473327636719 seconds to extract USE embeddings...\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3-grams in \"Department\":\n",
      "[' De', 'Dep', 'epa', 'par', 'art', 'rtm', 'tme', 'men', 'ent', 'nt ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "\n",
    "org_names = names['buyer'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "\n",
    "\n",
    "print('All 3-grams in \"Department\":')\n",
    "print(extract_ngrams('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "\n",
    "def extract_ngrams(string, n=3):\n",
    "    string = fix_text(string).encode(\"ascii\", errors=\"ignore\").decode().lower() # fix text\n",
    "    string = string.replace('&', 'and').replace(',', ' ').replace('-', ' ').title()\n",
    "    string = re.sub('[' + re.escape(''.join(chars_to_remove)) + ']', '', string)\n",
    "    string = ' ' + re.sub(' +',' ',string).strip() + ' '\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    ngrams = [''.join(ngram) for ngram in ngrams]\n",
    "    return ngrams\n",
    "\n",
    "word_to_match = \"margherita pizza\"\n",
    "words = list(word2id_db.keys())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(words + [word_to_match])\n",
    "\n",
    "# d = awesome_cossim_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    print (sparserows)\n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  -0.3905460834503174\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "words[np.argmax(d)]\n",
    "print (\"time: \", start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_db_new = shelve.open(prefix + \"_relations_map.db\", \"c\")\n",
    "for k, v in relations_db.items():\n",
    "    relations_db_new[\"###\".join(k.split(\"_\"))] = v\n",
    "relations_db_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801f7abaa96409c98dc3242a4f76114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c1948726584631ad50cc76d8080955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a10b976cf74aa5914d3357b89df31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6a1f8f8fe64c29a7deb93c8c18fbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 253 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/vlead/.local/lib/python3.6/site-packages (from torchvision) (1.18.2)\n",
      "Requirement already satisfied: six in /home/vlead/.local/lib/python3.6/site-packages (from torchvision) (1.14.0)\n",
      "Requirement already satisfied: torch==1.4.0 in /home/vlead/.local/lib/python3.6/site-packages (from torchvision) (1.4.0)\n",
      "Collecting pillow>=4.1.1\n",
      "  Downloading Pillow-7.1.1-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
      "Successfully installed pillow-7.1.1 torchvision-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
