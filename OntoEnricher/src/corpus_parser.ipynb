{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, pickle, os, sys, pickledb, time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from copy import deepcopy\n",
    "\n",
    "train_file = \"/data/Vivek/original/HypeNET/dataset/custom_train_0.0_0.05.tsv\"\n",
    "test_file =  \"/data/Vivek/original/HypeNET/dataset/custom_test_0.0_0.05.tsv\"\n",
    "instances_file = '../files/dataset/test_instances.tsv'\n",
    "knocked_file = '../files/dataset/test_knocked.tsv'\n",
    "output_folder = \"../junk/Output/\"\n",
    "embeddings_folder = \"../junk/Glove_lite.dat/\"\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "embeddings_file = \"/data/Vivek/glove.6B.300d.txt\"\n",
    "use_embeddings = \"../files/embeddings.pt\"\n",
    "\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "EMBEDDING_DIM = 300\n",
    "NULL_PATH = ((0, 0, 0, 0),)\n",
    "relations = [\"hypernym\", \"hyponym\", \"concept\", \"instance\", \"none\"]\n",
    "# relations = [\"True\", \"False\"]\n",
    "NUM_RELATIONS = len(relations)\n",
    "prefix = \"../junk/db_files/\"\n",
    "op_file = \"../junk/dataset_glove_lite.pkl\"\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format(\"/data/Vivek/glove_tmp\")\n",
    "\n",
    "# wiki2vec = KeyedVectors.load_word2vec_format(\"/home/vlead/enwiki_20180420_win10_300d.txt\")\n",
    "\n",
    "\n",
    "# og_dict = deepcopy(wiki2vec.wv.vocab)\n",
    "# for k in og_dict:\n",
    "#     if \"/\" in k:\n",
    "#         wiki2vec.wv.vocab[k.split(\"/\")[1].lower()] = wiki2vec.wv.vocab[k]\n",
    "#         del wiki2vec.wv.vocab[k]\n",
    "# del og_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset for  ../junk/db_files/\n",
      "Successful hits:  22833 Failed hits:  12665\n",
      "Parsed ../junk/db_files/\n"
     ]
    }
   ],
   "source": [
    "success, failed = [], []\n",
    "def id_to_entity(db, entity_id):\n",
    "    entity = db.get(str(entity_id))\n",
    "    if not entity:\n",
    "        print (entity_id)\n",
    "    return entity\n",
    "\n",
    "def id_to_path(db, entity_id):\n",
    "    entity = db.get(str(entity_id))\n",
    "    entity = \"/\".join([\"*##*\".join(e.split(\"_\", 1)) for e in entity.split(\"/\")])\n",
    "    return entity\n",
    "\n",
    "def entity_to_id(db, entity):\n",
    "    global success, faile\n",
    "    entity_id = db.get(entity)\n",
    "    if entity_id:\n",
    "        success.append(entity)\n",
    "        return int(entity_id)\n",
    "#     closest_entity = resolved.get(entity, \"\")\n",
    "#     if closest_entity and closest_entity[0]:\n",
    "#         success.append(entity)\n",
    "#         return int(db.get(closest_entity[0]))\n",
    "    failed.append(entity)\n",
    "    return -1\n",
    "\n",
    "def extract_paths(db, x, y):\n",
    "    key = (str(x) + '###' + str(y))\n",
    "    try:\n",
    "        relation = db.get(key)\n",
    "        return {int(path_count.split(\":\")[0]): int(path_count.split(\":\")[1]) for path_count in relation.split(\",\")}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_embeddings_from_disk():\n",
    "    try:\n",
    "        vectors = bcolz.open(embeddings_folder)[:]\n",
    "        words = pickle.load(open(embeddings_folder + 'words.pkl', 'rb'))\n",
    "        word2idx = pickle.load(open(embeddings_folder + 'words_index.pkl', 'rb'))\n",
    "\n",
    "        embeddings = vectors\n",
    "    except:\n",
    "        embeddings, word2idx = create_embeddings()\n",
    "    return embeddings, word2idx\n",
    "\n",
    "\n",
    "def create_embeddings():\n",
    "#     print (success)\n",
    "    vocab = set([a for a in success if a])\n",
    "    words = ['_unk_']\n",
    "    idx = 1\n",
    "    word2idx = {\"_unk_\": 0}\n",
    "    vectors = bcolz.carray(np.random.uniform(-1, 1, (1, 300)), rootdir=embeddings_folder, mode='w')\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for l in f:\n",
    "            line = [a[::-1] for a in l[::-1].split(\" \", 300)[::-1]]\n",
    "            word, vector = line[0], [float(s) for s in line[1:]]\n",
    "            if len(vector) != 300:\n",
    "                print (len(vector))\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            words.append(word)\n",
    "            vectors.append(np.resize(np.array(vector), (1, 300)).astype(np.float))\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "#     print (vectors.shape)\n",
    "\n",
    "    row_norm = np.sum(np.abs(vectors)**2, axis=-1)**(1./2)\n",
    "    vectors /= row_norm[:, np.newaxis]\n",
    "    vectors = bcolz.carray(vectors, rootdir=embeddings_folder, mode='w')\n",
    "    vectors.flush()\n",
    "\n",
    "    pickle.dump(words, open(embeddings_folder + 'words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(embeddings_folder + 'words_index.pkl', 'wb'))\n",
    "\n",
    "    return vectors, word2idx\n",
    "\n",
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "id2word_db = pickledb.load(prefix + \"i2w.db\", False)\n",
    "path2id_db = pickledb.load(prefix + \"p2i.db\", False)\n",
    "id2path_db = pickledb.load(prefix + \"i2p.db\", False)\n",
    "relations_db = pickledb.load(prefix + \"relations.db\", False)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(train_file).read().split(\"\\n\")}\n",
    "test_dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(test_file).read().split(\"\\n\")}\n",
    "test_instances = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(instances_file).read().split(\"\\n\")}\n",
    "test_knocked = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(knocked_file).read().split(\"\\n\")}\n",
    "\n",
    "arrow_heads = {\">\": \"up\", \"<\":\"down\"}\n",
    "\n",
    "\n",
    "def extract_direction(edge):\n",
    "\n",
    "    if edge[0] == \">\" or edge[0] == \"<\":\n",
    "        direction = \"start_\" + arrow_heads[edge[0]]\n",
    "        edge = edge[1:]\n",
    "    elif edge[-1] == \">\" or edge[-1] == \"<\":\n",
    "        direction = \"end_\" + arrow_heads[edge[-1]]\n",
    "        edge = edge[:-1]\n",
    "    else:\n",
    "        direction = ' '\n",
    "    return direction, edge\n",
    "\n",
    "def parse_path(path):\n",
    "    parsed_path = []\n",
    "    for edge in path.split(\"*##*\"):\n",
    "        direction, edge = extract_direction(edge)\n",
    "        if edge.split(\"/\"):\n",
    "            try:\n",
    "                embedding, pos, dependency = tuple([a[::-1] for a in edge[::-1].split(\"/\",2)][::-1])\n",
    "            except:\n",
    "                print (edge, path)\n",
    "                raise\n",
    "            emb_idx, pos_idx, dep_idx, dir_idx = emb_indexer.get(embedding, 0), pos_indexer[pos], dep_indexer[dependency], dir_indexer[direction]\n",
    "            parsed_path.append(tuple([emb_idx, pos_idx, dep_idx, dir_idx]))\n",
    "        else:\n",
    "            return None\n",
    "    return tuple(parsed_path)\n",
    "\n",
    "def parse_tuple(tup):\n",
    "    x, y = tup\n",
    "#     paths = list(extract_paths(relations_db,x,y).items()) + list(extract_paths(relations_db,y,x).items())\n",
    "#     x_word = id_to_entity(id2word_db, x) if x!=-1 else \"X\"\n",
    "#     y_word = id_to_entity(id2word_db, y) if y!=-1 else \"Y\"\n",
    "#     path_count_dict = { id_to_path(id2path_db, path).replace(\"X/\", x_word+\"/\").replace(\"Y/\", y_word+\"/\") : freq for (path, freq) in paths }\n",
    "    paths_xy = list(extract_paths(relations_db,x,y).items())\n",
    "    paths_yx = list(extract_paths(relations_db,y,x).items())\n",
    "    path_count_dict = { id_to_path(id2path_db, path) : freq for (path, freq) in paths_xy }\n",
    "    path_count_dict.update({ id_to_path(id2path_db, path).replace(\"X/\", '@@@').replace('Y/', 'X/').replace('@@@', 'Y/') : freq for (path, freq) in paths_yx })\n",
    "    return path_count_dict\n",
    "\n",
    "def parse_dataset(dataset):\n",
    "    print (\"Parsing dataset for \", prefix)\n",
    "    \n",
    "    global embeddings, emb_indexer\n",
    "    \n",
    "    dataset = [(entity_to_id(word2id_db, tup[0]), entity_to_id(word2id_db, tup[1])) for tup in dataset]\n",
    "    embeddings, emb_indexer = create_embeddings()\n",
    "    \n",
    "    parsed_dicts = [parse_tuple(tup) for tup in dataset]\n",
    "    parsed_dicts = [{ parse_path(path) : path_count_dict[path] for path in path_count_dict } for path_count_dict in parsed_dicts]\n",
    "    paths = [{ path : path_count_dict[path] for path in path_count_dict if path} for path_count_dict in parsed_dicts]\n",
    "    empty = [list(dataset)[i] for i, path_list in enumerate(paths) if len(list(path_list.keys())) == 0]\n",
    "#     paths = [{NULL_PATH: 1} if not path_list else path_list for i, path_list in enumerate(paths)]\n",
    "    embed_indices = [(emb_indexer.get(x,0), emb_indexer.get(y,0)) for (x,y) in dataset]\n",
    "\n",
    "    return embed_indices, paths\n",
    "\n",
    "pos_indexer, dep_indexer, dir_indexer = defaultdict(count(0).__next__), defaultdict(count(0).__next__), defaultdict(count(0).__next__)\n",
    "unk_pos, unk_dep, unk_dir = pos_indexer[\"#UNKNOWN#\"], dep_indexer[\"#UNKNOWN#\"], dir_indexer[\"#UNKNOWN#\"]\n",
    "\n",
    "dataset_keys = list(train_dataset.keys()) + list(test_dataset.keys()) + list(test_instances.keys()) + list(test_knocked.keys())\n",
    "dataset_vals = list(train_dataset.values()) + list(test_dataset.values()) + list(test_instances.values()) + list(test_knocked.values())\n",
    "\n",
    "embeddings, emb_indexer = None, None\n",
    "\n",
    "mappingDict = {key: idx for (idx,key) in enumerate(relations)}\n",
    "\n",
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "f.close()\n",
    "\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_indices, x = parse_dataset(dataset_keys)\n",
    "y = [mappingDict[relation] for relation in dataset_vals]\n",
    "\n",
    "f = open(op_file, \"wb+\")\n",
    "\n",
    "s1 = len(train_dataset)\n",
    "s2 = len(train_dataset) + len(test_dataset)\n",
    "s3 = len(train_dataset)+len(test_dataset)+len(test_instances)\n",
    "\n",
    "parsed_train = (embed_indices[:s1], x[:s1], y[:s1], dataset_keys[:s1], dataset_vals[:s1])\n",
    "parsed_test = (embed_indices[s1:s2], x[s1:s2], y[s1:s2], dataset_keys[s1:s2], dataset_vals[s1:s2])\n",
    "parsed_instances = (embed_indices[s2:s3], x[s2:s3], y[s2:s3], dataset_keys[s2:s3], dataset_vals[s2:s3])\n",
    "parsed_knocked = (embed_indices[s3:], x[s3:], y[s3:], dataset_keys[s3:], dataset_vals[s3:])\n",
    "pickle.dump([parsed_train, parsed_test, parsed_instances, parsed_knocked, pos_indexer, dep_indexer, dir_indexer], f)\n",
    "print (\"Successful hits: \", len(success), \"Failed hits: \", len(failed))\n",
    "f.close()\n",
    "\n",
    "print (\"Parsed\",prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.hidden_dim = HIDDEN_DIM + 2 * EMBEDDING_DIM\n",
    "        self.input_dim = POS_DIM + DEP_DIM + EMBEDDING_DIM + DIR_DIM\n",
    "        self.W = nn.Linear(NUM_RELATIONS, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(len(embeddings), EMBEDDING_DIM)\n",
    "        self.word_embeddings.load_state_dict({'weight': torch.from_numpy(np.array(embeddings))})\n",
    "        self.word_embeddings.require_grad = False\n",
    "        \n",
    "        self.pos_embeddings = nn.Embedding(len(pos_indexer), POS_DIM)\n",
    "        self.dep_embeddings = nn.Embedding(len(dep_indexer), DEP_DIM)\n",
    "        self.dir_embeddings = nn.Embedding(len(dir_indexer), DIR_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, NUM_LAYERS)\n",
    "    \n",
    "    def embed_path(self, elem):\n",
    "        path, count = elem\n",
    "        if path in self.cache:\n",
    "            return cache[path] * count\n",
    "        lstm_inp = []\n",
    "        for edge in path:\n",
    "            inputs = [torch.Tensor([[el]]) for el in edge]\n",
    "            word_embed = self.dropout_layer(self.word_embeddings(inputs[0]))\n",
    "            pos_embed = self.dropout_layer(self.pos_embeddings(inputs[1]))\n",
    "            dep_embed = self.dropout_layer(self.dep_embeddings(inputs[2]))\n",
    "            dir_embed = self.dropout_layer(self.dir_embeddings(inputs[3]))\n",
    "            embeds = np.concatenate((word_embed, pos_embed, dep_embed, dir_embed))\n",
    "            lstm_inp.append(embeds)\n",
    "        output, _ = self.lstm(lstm_inp)\n",
    "        cache[path] = output\n",
    "\n",
    "        return output * count\n",
    "    \n",
    "    def forward(self, data, emb_indexer):\n",
    "        if not data:\n",
    "            data[NULL_PATH] = 1\n",
    "        print (\"Data: \", data)\n",
    "        num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = [np.sum([self.embed_path(path) for path in paths.items()]) for paths in data]\n",
    "        print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        h = [np.concatenate((self.word_embeddings(elem[0]), h[i], self.word_embeddings(elem[1]))) for i,emb in enumerate(emb_indexer)]\n",
    "        return self.softmax(self.W(h))\n",
    "\n",
    "HIDDEN_DIM = 60\n",
    "NUM_LAYERS = 2\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "batch_size = min(batch_size, dataset_size)\n",
    "num_batches = int(ceil(dataset_size/batch_size))\n",
    "\n",
    "lr = 0.001\n",
    "dropout = 0.3\n",
    "lstm = LSTM()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss, epoch_idx = 0, np.random.permutation(dataset_size)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch = epoch_idx[batch_start:batch_end]\n",
    "        \n",
    "        data, labels, embeddings_idx = x_train[batch], y_train[batch], embed_indices_train[batch]\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = lstm(data, embeddings_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= dataset_size\n",
    "    print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch + 1, num_epochs, total_loss))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    predictedLabels = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = lstm(data)\n",
    "        print (outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictedLabels.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "../junk/Wiki2Vec.dat/meta/siz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "e = nn.Embedding(3, 3)\n",
    "ls = [[0, 1, 2], [3,4,5], [6,7,8]]\n",
    "ls = np.array([np.array(el) for el in ls])\n",
    "e.load_state_dict({'weight': torch.from_numpy(ls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths = [sum(list(paths.values())) for paths in data]\n",
    "        print (\"Number of paths: \", num_paths)\n",
    "        path_embeddings = np.array([np.sum([self.embed_path(path) for path in paths.items()]) for paths in data])\n",
    "        #print (\"Path Embeddings: \", path_embeddings)\n",
    "        \n",
    "        h = np.divide(path_embeddings, num_paths)\n",
    "        print (h.shape)\n",
    "        h = [np.concatenate((self.word_embeddings(emb[0]), h[i], self.word_embeddings(emb[1]))) for i,emb in enumerate(emb_indexer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python (3, 6, 9)\n",
      "numpy 1.18.2\n",
      "bcolz 1.2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "carray((2, 2), float64)\n",
       "  nbytes := 32; cbytes := 16.00 KB; ratio: 0.00\n",
       "  cparams := cparams(clevel=5, shuffle=1, cname='lz4', quantize=0)\n",
       "  chunklen := 1024; chunksize: 16384; blocksize: 0\n",
       "  rootdir := 'x.dat'\n",
       "  mode    := 'r'\n",
       "[[ 0.02012097  0.85212542]\n",
       " [ 0.38955088  0.62628181]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import bcolz\n",
    "\n",
    "print(\"python\", sys.version_info[:3])\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"bcolz\", bcolz.__version__)\n",
    "\n",
    "x = np.random.uniform(size=(2, 2))\n",
    "c = bcolz.carray(x, rootdir='x.dat', mode='w')\n",
    "y = bcolz.open('x.dat', mode='r')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputt = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(inputt, target)\n",
    "output.backward()\n",
    "print (output, inputt, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took me 16.031342029571533 seconds to extract USE embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crateva greveana flowers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_word_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wansecure firewall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mclosest_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3c64f6dc5907>\u001b[0m in \u001b[0;36mclosest_word_USE\u001b[0;34m(word, method)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Values and keys obtained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim=-1000, closest_word=\"\"):\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    closest_word = \"\"\n",
    "    with shelve.open(use_embeddings, 'c') as db:\n",
    "        for i, w in enumerate(word_embeddings):\n",
    "            db[words[i]] = w\n",
    "        closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[closest_word_idx]\n",
    "        del word_embeddings\n",
    "    del db\n",
    "    return closest_word, max_sim\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "    if os.path.isfile(use_embeddings):\n",
    "        with shelve.open(use_embeddings, 'r') as db:\n",
    "            embeds = np.array(list(db.values()))\n",
    "            words = np.array(list(db.keys()))\n",
    "            print (\"Values and keys obtained\", time.time()-a)\n",
    "            sim_mat = awesome_cossim_topn(coo_matrix(embeds, dtype=np.float64), coo_matrix(word_to_compare.T, dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250)\n",
    "            print (\"Sim mat calculated\", time.time()-a)\n",
    "            closest_word_idx = np.argmax(sim_mat)\n",
    "            print (\"idx gotten\", time.time()-a)\n",
    "            closest_word = words[closest_word_idx]\n",
    "    else:\n",
    "        words = list(word2id_db.keys())\n",
    "        print (\"Obtained list of words\")\n",
    "        len_part = 100000\n",
    "        max_sim = -1000\n",
    "        n_parts = ceil(len(words)/len_part)\n",
    "        closest_word = \"\"\n",
    "        for i in range(n_parts):\n",
    "            words_part = words[i*len_part:(i+1)*len_part]\n",
    "            closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_db = pickledb.load(prefix + \"w2i.db\", False)\n",
    "words = list(word2id_db.getall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "\n",
    "org_names = names['buyer'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "\n",
    "\n",
    "print('All 3-grams in \"Department\":')\n",
    "print(extract_ngrams('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016379356384277344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"the\" in words_set\n",
    "time.time() - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "\n",
    "def extract_ngrams(string, n=3):\n",
    "    string = fix_text(string).encode(\"ascii\", errors=\"ignore\").decode().lower() # fix text\n",
    "    string = string.replace('&', 'and').replace(',', ' ').replace('-', ' ').title()\n",
    "    string = re.sub('[' + re.escape(''.join(chars_to_remove)) + ']', '', string)\n",
    "    string = ' ' + re.sub(' +',' ',string).strip() + ' '\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    ngrams = [''.join(ngram) for ngram in ngrams]\n",
    "    return ngrams\n",
    "\n",
    "word_to_match = \"margherita pizza\"\n",
    "words = list(word2id_db.keys())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=extract_ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(words + [word_to_match])\n",
    "\n",
    "# d = awesome_cossim_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    print (sparserows)\n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = awesome_cossim_topn(tf_idf_matrix[:-1], tf_idf_matrix[-1].transpose(), 10, 0.85, use_threads=True, n_jobs=256)\n",
    "words[np.argmax(d)]\n",
    "print (\"time: \", start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_db_new = shelve.open(prefix + \"_relations_map.db\", \"c\")\n",
    "for k, v in relations_db.items():\n",
    "    relations_db_new[\"###\".join(k.split(\"_\"))] = v\n",
    "relations_db_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"\\t\".join(l.split(\"\\t\")[1:-1]) for l in open(\"../junk/security_dataset.tsv\",\"r\").read().split(\"\\n\")[1:]]\n",
    "open(\"../files/dataset/dataset.tsv\",\"w\").write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open(use_embeddings, 'r') as db:    \n",
    "    allitems = list(db.items())\n",
    "    emb = [el[1] for el in allitems]\n",
    "    wds = [el[0] for el in allitems]\n",
    "    file = open(\"../files/embeddings_list.pkl\", \"wb\")\n",
    "    pickle.dump(allitems, file)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "            \n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = compare_sim(words_part, word_to_compare, max_sim, closest_word)\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22912765"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "word = \"margherita pizza\" \n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(args):\n",
    "    words, word_to_compare, max_sim, closest_word = args\n",
    "    t = time.time()\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-t))\n",
    "    sys.stdout.flush()\n",
    "    closest_word_idx = np.argmax(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    sim = np.max(awesome_cossim_topn(coo_matrix(word_embeddings, dtype=np.float64), coo_matrix(word_to_compare.transpose(), dtype=np.float64), 10, 0.85, use_threads=True, n_jobs=250))\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        closest_word = words[closest_word_idx]\n",
    "    del word_embeddings\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(word, method=\"USE\"):\n",
    "\n",
    "    word_to_compare = extractUSEEmbeddings([word])\n",
    "    print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "#     words = list(word2id_db.keys())\n",
    "    print (\"Took me {} seconds to obtain words list...\".format(time.time()-a))\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        t = time.time()\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        sub_arrays = np.array_split(words_part, 2)\n",
    "        args = [(sub_array, word_to_compare, max_sim, closest_word) for sub_array in sub_arrays]\n",
    "        results = []\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            for res in executor.map(compare_sim, args):\n",
    "                results.append(res)\n",
    "        closest_word, max_sim = max(results, key=lambda l:l[-1])\n",
    "        print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-t))\n",
    "\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "a = time.time()\n",
    "closest_word = closest_word_USE(\"wansecure firewall\")\n",
    "print (time.time()-a)\n",
    "closest_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab peter wyche (diplomat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab acoma-zuni section\n",
      "Original word not in vocab madan-harini\n",
      "Original word not in vocab trust no one (internet security)\n",
      "Original word not in vocab international tibet independence movement\n",
      "Original word not in vocab isobase\n",
      "Original word not in vocab human computer interaction (security)\n",
      "Original word not in vocab poetas de karaoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word not in vocab ipa pulmonic consonant chart with audio\n",
      "Original word not in vocab lego clutch powers: bad hair day\n",
      "Original word not in vocab aed (non-profit)\n",
      "Original word not in vocab quilmes airport\n",
      "Original word not in vocab yendegaia airport\n",
      "Original word not in vocab the pack a.d.\n",
      "Original word not in vocab harvie-watt baronets\n",
      "Original word not in vocab sharp actius rd3d notebook\n",
      "Original word not in vocab big beach boutique ii - the movie\n",
      "Original word not in vocab privacy by design\n",
      "Original word not in vocab motorola devour\n",
      "Original word not in vocab piracy act\n",
      "Original word not in vocab starter ring gear\n",
      "Original word not in vocab antonio sánchez (puerto rican host)\n",
      "Original word not in vocab electronic logbook\n",
      "Original word not in vocab greg burke (journalist)\n",
      "Original word not in vocab deaths in november 2013\n",
      "Original word not in vocab hp mini 311\n",
      "Original word not in vocab confederation of indigenous nationalities of the ecuadorian amazon\n",
      "Original word not in vocab url subscription architecture\n",
      "Original word not in vocab snowballers entertainment\n",
      "Original word not in vocab basic strategic arts program\n",
      "Original word not in vocab mars (ticket reservation system)\n",
      "Original word not in vocab matija kristić\n",
      "Original word not in vocab edward graham lee\n",
      "Original word not in vocab rebellion of the three guards\n",
      "Original word not in vocab the recipe for gertrude\n",
      "Original word not in vocab quiet pc\n",
      "Original word not in vocab russian amateur radio union\n",
      "1 done\n",
      "Original word not in vocab core strategy document\n",
      "Original word not in vocab mutukula airport\n",
      "2 done\n",
      "Original word not in vocab magnus l. kpakol\n",
      "1 done\n",
      "Original word not in vocab strangers (malibu comics)\n",
      "2 done\n",
      "3 done\n",
      "Original word not in vocab cary baronets\n",
      "3 done\n",
      "Original word not in vocab andrew wood (diplomat)\n",
      "4 done\n",
      "Original word not in vocab lead petty officer\n",
      "4 done\n",
      "Original word not in vocab corps of military police (india)\n",
      "5 done\n",
      "Original word not in vocab siniša radanović\n",
      "5 done\n",
      "Original word not in vocab oatmeal cookie\n",
      "6 done\n",
      "Original word not in vocab academic research alliance\n",
      "6 done\n",
      "1 done\n",
      "Original word not in vocab joseph e. duncan iii\n",
      "7 done\n",
      "7 done\n",
      "Original word not in vocab lideta army airport\n",
      "2 done\n",
      "8 done\n",
      "8 done\n",
      "3 done\n",
      "9 done\n",
      "9 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "10 done\n",
      "10 done\n",
      "7 done\n",
      "11 done\n",
      "11 done\n",
      "12 done\n",
      "12 done\n",
      "8 done\n",
      "13 done\n",
      "13 done\n",
      "1 done\n",
      "9 done\n",
      "14 done\n",
      "2 done\n",
      "14 done\n",
      "1 done\n",
      "15 done\n",
      "3 done\n",
      "15 done\n",
      "10 done\n",
      "16 done\n",
      "2 done\n",
      "11 done\n",
      "4 done\n",
      "16 done\n",
      "3 done\n",
      "12 done\n",
      "17 done\n",
      "5 done\n",
      "4 done\n",
      "17 done\n",
      "13 done\n",
      "6 done\n",
      "18 done\n",
      "5 done\n",
      "18 done\n",
      "14 done\n",
      "6 done\n",
      "19 done\n",
      "7 done\n",
      "19 done\n",
      "15 done\n",
      "7 done\n",
      "20 done\n",
      "20 done\n",
      "16 done\n",
      "8 done\n",
      "21 done\n",
      "8 done\n",
      "21 done\n",
      "17 done\n",
      "9 done\n",
      "22 done\n",
      "22 done\n",
      "9 done\n",
      "18 done\n",
      "23 done\n",
      "23 done\n",
      "19 done\n",
      "10 done\n",
      "24 done\n",
      "10 done\n",
      "24 done\n",
      "20 done\n",
      "11 done\n",
      "25 done\n",
      "11 done\n",
      "25 done\n",
      "21 done\n",
      "12 done\n",
      "12 done\n",
      "26 done\n",
      "13 done\n",
      "26 done\n",
      "13 done\n",
      "22 done\n",
      "27 done\n",
      "14 done\n",
      "27 done\n",
      "14 done\n",
      "28 done\n",
      "15 done\n",
      "23 done\n",
      "28 done\n",
      "16 done\n",
      "15 done\n",
      "29 done\n",
      "24 done\n",
      "17 done\n",
      "16 done\n",
      "29 done\n",
      "30 done\n",
      "25 done\n",
      "18 done\n",
      "30 done\n",
      "17 done\n",
      "26 done\n",
      "19 done\n",
      "18 done\n",
      "27 done\n",
      "31 done\n",
      "20 done\n",
      "31 done\n",
      "19 done\n",
      "28 done\n",
      "32 done\n",
      "21 done\n",
      "20 done\n",
      "32 done\n",
      "33 done\n",
      "29 done\n",
      "22 done\n",
      "33 done\n",
      "21 done\n",
      "34 done\n",
      "30 done\n",
      "34 done\n",
      "23 done\n",
      "22 done\n",
      "35 done\n",
      "24 done\n",
      "35 done\n",
      "36 done\n",
      "23 done\n",
      "25 done\n",
      "31 done\n",
      "36 done\n",
      "37 done\n",
      "37 done\n",
      "24 done\n",
      "26 done\n",
      "32 done\n",
      "38 done\n",
      "38 done\n",
      "25 done\n",
      "27 done\n",
      "33 done\n",
      "39 done\n",
      "39 done\n",
      "28 done\n",
      "26 done\n",
      "34 done\n",
      "40 done\n",
      "40 done\n",
      "29 done\n",
      "27 done\n",
      "41 done\n",
      "35 done\n",
      "30 done\n",
      "41 done\n",
      "28 done\n",
      "42 done\n",
      "36 done\n",
      "42 done\n",
      "29 done\n",
      "43 done\n",
      "37 done\n",
      "31 done\n",
      "43 done\n",
      "44 done\n",
      "30 done\n",
      "38 done\n",
      "44 done\n",
      "45 done\n",
      "32 done\n",
      "39 done\n",
      "45 done\n",
      "33 done\n",
      "46 done\n",
      "31 done\n",
      "40 done\n",
      "34 done\n",
      "46 done\n",
      "47 done\n",
      "32 done\n",
      "47 done\n",
      "35 done\n",
      "33 done\n",
      "41 done\n",
      "48 done\n",
      "48 done\n",
      "34 done\n",
      "36 done\n",
      "42 done\n",
      "49 done\n",
      "49 done\n",
      "37 done\n",
      "43 done\n",
      "35 done\n",
      "50 done\n",
      "44 done\n",
      "50 done\n",
      "38 done\n",
      "36 done\n",
      "51 done\n",
      "51 done\n",
      "45 done\n",
      "39 done\n",
      "37 done\n",
      "52 done\n",
      "52 done\n",
      "46 done\n",
      "53 done\n",
      "38 done\n",
      "40 done\n",
      "53 done\n",
      "47 done\n",
      "54 done\n",
      "39 done\n",
      "41 done\n",
      "54 done\n",
      "48 done\n",
      "55 done\n",
      "42 done\n",
      "55 done\n",
      "40 done\n",
      "49 done\n",
      "56 done\n",
      "43 done\n",
      "50 done\n",
      "41 done\n",
      "57 done\n",
      "56 done\n",
      "44 done\n",
      "51 done\n",
      "57 done\n",
      "58 done\n",
      "42 done\n",
      "45 done\n",
      "58 done\n",
      "52 done\n",
      "59 done\n",
      "43 done\n",
      "46 done\n",
      "59 done\n",
      "53 done\n",
      "44 done\n",
      "60 done\n",
      "47 done\n",
      "60 done\n",
      "54 done\n",
      "45 done\n",
      "61 done\n",
      "48 done\n",
      "55 done\n",
      "61 done\n",
      "62 done\n",
      "46 done\n",
      "49 done\n",
      "62 done\n",
      "56 done\n",
      "63 done\n",
      "50 done\n",
      "47 done\n",
      "57 done\n",
      "63 done\n",
      "64 done\n",
      "51 done\n",
      "48 done\n",
      "58 done\n",
      "64 done\n",
      "52 done\n",
      "49 done\n",
      "65 done\n",
      "59 done\n",
      "50 done\n",
      "65 done\n",
      "53 done\n",
      "66 done\n",
      "60 done\n",
      "51 done\n",
      "54 done\n",
      "67 done\n",
      "66 done\n",
      "52 done\n",
      "61 done\n",
      "55 done\n",
      "68 done\n",
      "67 done\n",
      "53 done\n",
      "62 done\n",
      "56 done\n",
      "68 done\n",
      "69 done\n",
      "54 done\n",
      "63 done\n",
      "57 done\n",
      "69 done\n",
      "70 done\n",
      "55 done\n",
      "64 done\n",
      "58 done\n",
      "70 done\n",
      "71 done\n",
      "56 done\n",
      "65 done\n",
      "72 done\n",
      "71 done\n",
      "59 done\n",
      "57 done\n",
      "66 done\n",
      "72 done\n",
      "73 done\n",
      "60 done\n",
      "58 done\n",
      "67 done\n",
      "73 done\n",
      "61 done\n",
      "74 done\n",
      "59 done\n",
      "68 done\n",
      "62 done\n",
      "74 done\n",
      "75 done\n",
      "60 done\n",
      "69 done\n",
      "63 done\n",
      "75 done\n",
      "76 done\n",
      "64 done\n",
      "61 done\n",
      "70 done\n",
      "76 done\n",
      "77 done\n",
      "62 done\n",
      "71 done\n",
      "65 done\n",
      "77 done\n",
      "72 done\n",
      "78 done\n",
      "63 done\n",
      "78 done\n",
      "66 done\n",
      "73 done\n",
      "79 done\n",
      "64 done\n",
      "79 done\n",
      "67 done\n",
      "74 done\n",
      "80 done\n",
      "65 done\n",
      "68 done\n",
      "80 done\n",
      "75 done\n",
      "69 done\n",
      "81 done\n",
      "81 done\n",
      "66 done\n",
      "70 done\n",
      "76 done\n",
      "67 done\n",
      "82 done\n",
      "71 done\n",
      "82 done\n",
      "68 done\n",
      "77 done\n",
      "83 done\n",
      "72 done\n",
      "83 done\n",
      "69 done\n",
      "84 done\n",
      "78 done\n",
      "73 done\n",
      "84 done\n",
      "70 done\n",
      "79 done\n",
      "74 done\n",
      "71 done\n",
      "85 done\n",
      "85 done\n",
      "80 done\n",
      "72 done\n",
      "75 done\n",
      "86 done\n",
      "86 done\n",
      "81 done\n",
      "76 done\n",
      "73 done\n",
      "87 done\n",
      "87 done\n",
      "74 done\n",
      "77 done\n",
      "88 done\n",
      "82 done\n",
      "88 done\n",
      "89 done\n",
      "78 done\n",
      "75 done\n",
      "83 done\n",
      "89 done\n",
      "90 done\n",
      "76 done\n",
      "79 done\n",
      "90 done\n",
      "84 done\n",
      "80 done\n",
      "77 done\n",
      "91 done\n",
      "91 done\n",
      "85 done\n",
      "92 done\n",
      "81 done\n",
      "78 done\n",
      "92 done\n",
      "93 done\n",
      "86 done\n",
      "79 done\n",
      "93 done\n",
      "82 done\n",
      "87 done\n",
      "83 done\n",
      "80 done\n",
      "94 done\n",
      "94 done\n",
      "88 done\n",
      "95 done\n",
      "84 done\n",
      "95 done\n",
      "81 done\n",
      "89 done\n",
      "96 done\n",
      "96 done\n",
      "90 done\n",
      "82 done\n",
      "85 done\n",
      "97 done\n",
      "97 done\n",
      "91 done\n",
      "83 done\n",
      "86 done\n",
      "98 done\n",
      "98 done\n",
      "92 done\n",
      "84 done\n",
      "87 done\n",
      "93 done\n",
      "88 done\n",
      "99 done\n",
      "99 done\n",
      "89 done\n",
      "85 done\n",
      "100 done\n",
      "100 done\n",
      "94 done\n",
      "90 done\n",
      "86 done\n",
      "101 done\n",
      "101 done\n",
      "95 done\n",
      "91 done\n",
      "87 done\n",
      "92 done\n",
      "102 done\n",
      "102 done\n",
      "96 done\n",
      "88 done\n",
      "93 done\n",
      "103 done\n",
      "89 done\n",
      "97 done\n",
      "103 done\n",
      "94 done\n",
      "90 done\n",
      "104 done\n",
      "98 done\n",
      "104 done\n",
      "95 done\n",
      "105 done\n",
      "99 done\n",
      "91 done\n",
      "106 done\n",
      "96 done\n",
      "100 done\n",
      "105 done\n",
      "92 done\n",
      "97 done\n",
      "101 done\n",
      "106 done\n",
      "107 done\n",
      "93 done\n",
      "102 done\n",
      "108 done\n",
      "98 done\n",
      "107 done\n",
      "94 done\n",
      "103 done\n",
      "109 done\n",
      "108 done\n",
      "95 done\n",
      "99 done\n",
      "104 done\n",
      "100 done\n",
      "109 done\n",
      "96 done\n",
      "110 done\n",
      "105 done\n",
      "97 done\n",
      "101 done\n",
      "111 done\n",
      "106 done\n",
      "110 done\n",
      "102 done\n",
      "112 done\n",
      "98 done\n",
      "113 done\n",
      "111 done\n",
      "107 done\n",
      "103 done\n",
      "99 done\n",
      "114 done\n",
      "108 done\n",
      "112 done\n",
      "100 done\n",
      "115 done\n",
      "109 done\n",
      "104 done\n",
      "113 done\n",
      "101 done\n",
      "116 done\n",
      "105 done\n",
      "114 done\n",
      "102 done\n",
      "110 done\n",
      "106 done\n",
      "115 done\n",
      "117 done\n",
      "111 done\n",
      "103 done\n",
      "107 done\n",
      "116 done\n",
      "118 done\n",
      "104 done\n",
      "108 done\n",
      "112 done\n",
      "113 done\n",
      "105 done\n",
      "109 done\n",
      "119 done\n",
      "117 done\n",
      "114 done\n",
      "106 done\n",
      "120 done\n",
      "118 done\n",
      "115 done\n",
      "110 done\n",
      "107 done\n",
      "121 done\n",
      "111 done\n",
      "119 done\n",
      "116 done\n",
      "108 done\n",
      "122 done\n",
      "120 done\n",
      "112 done\n",
      "109 done\n",
      "123 done\n",
      "113 done\n",
      "121 done\n",
      "117 done\n",
      "124 done\n",
      "110 done\n",
      "114 done\n",
      "122 done\n",
      "118 done\n",
      "111 done\n",
      "125 done\n",
      "115 done\n",
      "123 done\n",
      "112 done\n",
      "119 done\n",
      "126 done\n",
      "116 done\n",
      "124 done\n",
      "113 done\n",
      "120 done\n",
      "127 done\n",
      "125 done\n",
      "114 done\n",
      "121 done\n",
      "128 done\n",
      "117 done\n",
      "122 done\n",
      "115 done\n",
      "118 done\n",
      "129 done\n",
      "126 done\n",
      "123 done\n",
      "130 done\n",
      "127 done\n",
      "116 done\n",
      "119 done\n",
      "124 done\n",
      "131 done\n",
      "128 done\n",
      "120 done\n",
      "129 done\n",
      "117 done\n",
      "125 done\n",
      "132 done\n",
      "121 done\n",
      "118 done\n",
      "130 done\n",
      "133 done\n",
      "122 done\n",
      "126 done\n",
      "131 done\n",
      "134 done\n",
      "119 done\n",
      "123 done\n",
      "127 done\n",
      "135 done\n",
      "132 done\n",
      "124 done\n",
      "120 done\n",
      "128 done\n",
      "136 done\n",
      "133 done\n",
      "129 done\n",
      "121 done\n",
      "125 done\n",
      "134 done\n",
      "130 done\n",
      "122 done\n",
      "126 done\n",
      "137 done\n",
      "135 done\n",
      "123 done\n",
      "131 done\n",
      "127 done\n",
      "136 done\n",
      "138 done\n",
      "124 done\n",
      "132 done\n",
      "139 done\n",
      "128 done\n",
      "125 done\n",
      "140 done\n",
      "133 done\n",
      "129 done\n",
      "137 done\n",
      "138 done\n",
      "130 done\n",
      "134 done\n",
      "126 done\n",
      "141 done\n",
      "139 done\n",
      "135 done\n",
      "127 done\n",
      "142 done\n",
      "131 done\n",
      "140 done\n",
      "136 done\n",
      "143 done\n",
      "128 done\n",
      "141 done\n",
      "132 done\n",
      "142 done\n",
      "144 done\n",
      "129 done\n",
      "133 done\n",
      "137 done\n",
      "130 done\n",
      "145 done\n",
      "143 done\n",
      "134 done\n",
      "138 done\n",
      "146 done\n",
      "135 done\n",
      "144 done\n",
      "131 done\n",
      "147 done\n",
      "139 done\n",
      "148 done\n",
      "136 done\n",
      "145 done\n",
      "149 done\n",
      "132 done\n",
      "140 done\n",
      "150 done\n",
      "146 done\n",
      "133 done\n",
      "147 done\n",
      "151 done\n",
      "141 done\n",
      "137 done\n",
      "134 done\n",
      "148 done\n",
      "142 done\n",
      "152 done\n",
      "135 done\n",
      "138 done\n",
      "153 done\n",
      "149 done\n",
      "136 done\n",
      "154 done\n",
      "143 done\n",
      "139 done\n",
      "150 done\n",
      "155 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "151 done\n",
      "145 done\n",
      "152 done\n",
      "137 done\n",
      "157 done\n",
      "141 done\n",
      "153 done\n",
      "146 done\n",
      "158 done\n",
      "138 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 done\n",
      "154 done\n",
      "147 done\n",
      "159 done\n",
      "139 done\n",
      "143 done\n",
      "155 done\n",
      "148 done\n",
      "160 done\n",
      "140 done\n",
      "144 done\n",
      "156 done\n",
      "161 done\n",
      "149 done\n",
      "145 done\n",
      "157 done\n",
      "141 done\n",
      "150 done\n",
      "146 done\n",
      "142 done\n",
      "158 done\n",
      "162 done\n",
      "151 done\n",
      "147 done\n",
      "159 done\n",
      "143 done\n",
      "152 done\n",
      "148 done\n",
      "163 done\n",
      "160 done\n",
      "144 done\n",
      "149 done\n",
      "153 done\n",
      "161 done\n",
      "145 done\n",
      "150 done\n",
      "146 done\n",
      "154 done\n",
      "164 done\n",
      "151 done\n",
      "147 done\n",
      "155 done\n",
      "148 done\n",
      "152 done\n",
      "156 done\n",
      "165 done\n",
      "162 done\n",
      "149 done\n",
      "157 done\n",
      "166 done\n",
      "163 done\n",
      "153 done\n",
      "150 done\n",
      "158 done\n",
      "154 done\n",
      "151 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "164 done\n",
      "152 done\n",
      "168 done\n",
      "156 done\n",
      "160 done\n",
      "165 done\n",
      "153 done\n",
      "169 done\n",
      "161 done\n",
      "157 done\n",
      "166 done\n",
      "170 done\n",
      "154 done\n",
      "158 done\n",
      "167 done\n",
      "159 done\n",
      "155 done\n",
      "171 done\n",
      "168 done\n",
      "156 done\n",
      "162 done\n",
      "160 done\n",
      "172 done\n",
      "169 done\n",
      "157 done\n",
      "161 done\n",
      "163 done\n",
      "173 done\n",
      "170 done\n",
      "158 done\n",
      "171 done\n",
      "159 done\n",
      "174 done\n",
      "172 done\n",
      "162 done\n",
      "160 done\n",
      "175 done\n",
      "164 done\n",
      "161 done\n",
      "173 done\n",
      "163 done\n",
      "176 done\n",
      "165 done\n",
      "174 done\n",
      "166 done\n",
      "177 done\n",
      "175 done\n",
      "178 done\n",
      "164 done\n",
      "167 done\n",
      "162 done\n",
      "176 done\n",
      "168 done\n",
      "165 done\n",
      "179 done\n",
      "163 done\n",
      "169 done\n",
      "166 done\n",
      "177 done\n",
      "170 done\n",
      "180 done\n",
      "178 done\n",
      "167 done\n",
      "181 done\n",
      "164 done\n",
      "171 done\n",
      "179 done\n",
      "168 done\n",
      "172 done\n",
      "165 done\n",
      "182 done\n",
      "169 done\n",
      "166 done\n",
      "180 done\n",
      "170 done\n",
      "173 done\n",
      "183 done\n",
      "181 done\n",
      "171 done\n",
      "174 done\n",
      "167 done\n",
      "184 done\n",
      "172 done\n",
      "182 done\n",
      "175 done\n",
      "168 done\n",
      "185 done\n",
      "173 done\n",
      "176 done\n",
      "183 done\n",
      "169 done\n",
      "186 done\n",
      "174 done\n",
      "170 done\n",
      "184 done\n",
      "177 done\n",
      "187 done\n",
      "171 done\n",
      "175 done\n",
      "185 done\n",
      "178 done\n",
      "188 done\n",
      "172 done\n",
      "176 done\n",
      "186 done\n",
      "179 done\n",
      "189 done\n",
      "177 done\n",
      "187 done\n",
      "173 done\n",
      "180 done\n",
      "190 done\n",
      "174 done\n",
      "178 done\n",
      "188 done\n",
      "181 done\n",
      "189 done\n",
      "179 done\n",
      "191 done\n",
      "175 done\n",
      "182 done\n",
      "192 done\n",
      "180 done\n",
      "190 done\n",
      "176 done\n",
      "193 done\n",
      "183 done\n",
      "181 done\n",
      "194 done\n",
      "177 done\n",
      "184 done\n",
      "195 done\n",
      "191 done\n",
      "182 done\n",
      "178 done\n",
      "185 done\n",
      "192 done\n",
      "196 done\n",
      "183 done\n",
      "179 done\n",
      "186 done\n",
      "193 done\n",
      "197 done\n",
      "184 done\n",
      "180 done\n",
      "187 done\n",
      "198 done\n",
      "194 done\n",
      "185 done\n",
      "181 done\n",
      "199 done\n",
      "188 done\n",
      "186 done\n",
      "195 done\n",
      "182 done\n",
      "187 done\n",
      "189 done\n",
      "200 done\n",
      "196 done\n",
      "183 done\n",
      "188 done\n",
      "190 done\n",
      "197 done\n",
      "201 done\n",
      "184 done\n",
      "189 done\n",
      "198 done\n",
      "202 done\n",
      "190 done\n",
      "185 done\n",
      "191 done\n",
      "199 done\n",
      "191 done\n",
      "203 done\n",
      "186 done\n",
      "192 done\n",
      "200 done\n",
      "192 done\n",
      "204 done\n",
      "193 done\n",
      "187 done\n",
      "201 done\n",
      "193 done\n",
      "194 done\n",
      "188 done\n",
      "205 done\n",
      "194 done\n",
      "202 done\n",
      "189 done\n",
      "206 done\n",
      "195 done\n",
      "195 done\n",
      "207 done\n",
      "190 done\n",
      "203 done\n",
      "196 done\n",
      "204 done\n",
      "196 done\n",
      "208 done\n",
      "197 done\n",
      "197 done\n",
      "191 done\n",
      "209 done\n",
      "198 done\n",
      "205 done\n",
      "198 done\n",
      "210 done\n",
      "192 done\n",
      "199 done\n",
      "206 done\n",
      "199 done\n",
      "200 done\n",
      "193 done\n",
      "211 done\n",
      "207 done\n",
      "200 done\n",
      "194 done\n",
      "208 done\n",
      "201 done\n",
      "201 done\n",
      "195 done\n",
      "212 done\n",
      "202 done\n",
      "209 done\n",
      "202 done\n",
      "213 done\n",
      "196 done\n",
      "203 done\n",
      "210 done\n",
      "214 done\n",
      "197 done\n",
      "203 done\n",
      "204 done\n",
      "215 done\n",
      "211 done\n",
      "198 done\n",
      "204 done\n",
      "205 done\n",
      "216 done\n",
      "206 done\n",
      "199 done\n",
      "212 done\n",
      "217 done\n",
      "207 done\n",
      "205 done\n",
      "218 done\n",
      "200 done\n",
      "213 done\n",
      "206 done\n",
      "208 done\n",
      "219 done\n",
      "214 done\n",
      "209 done\n",
      "207 done\n",
      "201 done\n",
      "215 done\n",
      "220 done\n",
      "210 done\n",
      "208 done\n",
      "221 done\n",
      "202 done\n",
      "216 done\n",
      "209 done\n",
      "211 done\n",
      "203 done\n",
      "222 done\n",
      "217 done\n",
      "204 done\n",
      "218 done\n",
      "210 done\n",
      "212 done\n",
      "223 done\n",
      "205 done\n",
      "213 done\n",
      "211 done\n",
      "219 done\n",
      "206 done\n",
      "214 done\n",
      "224 done\n",
      "220 done\n",
      "212 done\n",
      "215 done\n",
      "225 done\n",
      "207 done\n",
      "221 done\n",
      "213 done\n",
      "216 done\n",
      "208 done\n",
      "214 done\n",
      "226 done\n",
      "222 done\n",
      "217 done\n",
      "209 done\n",
      "215 done\n",
      "227 done\n",
      "218 done\n",
      "223 done\n",
      "210 done\n",
      "216 done\n",
      "228 done\n",
      "219 done\n",
      "211 done\n",
      "217 done\n",
      "229 done\n",
      "224 done\n",
      "220 done\n",
      "225 done\n",
      "230 done\n",
      "218 done\n",
      "221 done\n",
      "212 done\n",
      "231 done\n",
      "219 done\n",
      "226 done\n",
      "222 done\n",
      "213 done\n",
      "232 done\n",
      "227 done\n",
      "220 done\n",
      "214 done\n",
      "223 done\n",
      "233 done\n",
      "221 done\n",
      "215 done\n",
      "228 done\n",
      "234 done\n",
      "222 done\n",
      "216 done\n",
      "224 done\n",
      "229 done\n",
      "235 done\n",
      "217 done\n",
      "225 done\n",
      "223 done\n",
      "236 done\n",
      "218 done\n",
      "230 done\n",
      "226 done\n",
      "237 done\n",
      "231 done\n",
      "219 done\n",
      "227 done\n",
      "224 done\n",
      "232 done\n",
      "238 done\n",
      "220 done\n",
      "225 done\n",
      "228 done\n",
      "233 done\n",
      "229 done\n",
      "239 done\n",
      "221 done\n",
      "226 done\n",
      "234 done\n",
      "230 done\n",
      "222 done\n",
      "240 done\n",
      "231 done\n",
      "227 done\n",
      "235 done\n",
      "241 done\n",
      "223 done\n",
      "232 done\n",
      "236 done\n",
      "228 done\n",
      "237 done\n",
      "242 done\n",
      "233 done\n",
      "224 done\n",
      "229 done\n",
      "243 done\n",
      "238 done\n",
      "234 done\n",
      "225 done\n",
      "230 done\n",
      "235 done\n",
      "244 done\n",
      "239 done\n",
      "226 done\n",
      "231 done\n",
      "236 done\n",
      "227 done\n",
      "240 done\n",
      "232 done\n",
      "237 done\n",
      "245 done\n",
      "241 done\n",
      "238 done\n",
      "228 done\n",
      "233 done\n",
      "246 done\n",
      "242 done\n",
      "234 done\n",
      "229 done\n",
      "247 done\n",
      "239 done\n",
      "235 done\n",
      "243 done\n",
      "240 done\n",
      "248 done\n",
      "230 done\n",
      "236 done\n",
      "249 done\n",
      "241 done\n",
      "244 done\n",
      "231 done\n",
      "237 done\n",
      "250 done\n",
      "232 done\n",
      "242 done\n",
      "238 done\n",
      "245 done\n",
      "251 done\n",
      "233 done\n",
      "243 done\n",
      "246 done\n",
      "239 done\n",
      "252 done\n",
      "234 done\n",
      "247 done\n",
      "244 done\n",
      "240 done\n",
      "235 done\n",
      "253 done\n",
      "248 done\n",
      "241 done\n",
      "236 done\n",
      "249 done\n",
      "254 done\n",
      "245 done\n",
      "237 done\n",
      "250 done\n",
      "242 done\n",
      "255 done\n",
      "246 done\n",
      "238 done\n",
      "243 done\n",
      "256 done\n",
      "247 done\n",
      "251 done\n",
      "239 done\n",
      "248 done\n",
      "252 done\n",
      "244 done\n",
      "257 done\n",
      "240 done\n",
      "249 done\n",
      "253 done\n",
      "258 done\n",
      "241 done\n",
      "250 done\n",
      "245 done\n",
      "259 done\n",
      "254 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "242 done\n",
      "255 done\n",
      "261 done\n",
      "252 done\n",
      "243 done\n",
      "247 done\n",
      "256 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "244 done\n",
      "257 done\n",
      "258 done\n",
      "249 done\n",
      "263 done\n",
      "254 done\n",
      "264 done\n",
      "250 done\n",
      "259 done\n",
      "245 done\n",
      "255 done\n",
      "260 done\n",
      "251 done\n",
      "246 done\n",
      "256 done\n",
      "265 done\n",
      "261 done\n",
      "252 done\n",
      "247 done\n",
      "257 done\n",
      "266 done\n",
      "262 done\n",
      "253 done\n",
      "248 done\n",
      "258 done\n",
      "267 done\n",
      "249 done\n",
      "263 done\n",
      "259 done\n",
      "268 done\n",
      "254 done\n",
      "250 done\n",
      "264 done\n",
      "260 done\n",
      "269 done\n",
      "255 done\n",
      "251 done\n",
      "270 done\n",
      "261 done\n",
      "256 done\n",
      "265 done\n",
      "252 done\n",
      "271 done\n",
      "262 done\n",
      "257 done\n",
      "266 done\n",
      "253 done\n",
      "272 done\n",
      "258 done\n",
      "263 done\n",
      "267 done\n",
      "264 done\n",
      "273 done\n",
      "259 done\n",
      "254 done\n",
      "268 done\n",
      "274 done\n",
      "260 done\n",
      "255 done\n",
      "269 done\n",
      "265 done\n",
      "261 done\n",
      "256 done\n",
      "275 done\n",
      "270 done\n",
      "266 done\n",
      "262 done\n",
      "257 done\n",
      "267 done\n",
      "276 done\n",
      "271 done\n",
      "258 done\n",
      "268 done\n",
      "263 done\n",
      "277 done\n",
      "272 done\n",
      "269 done\n",
      "259 done\n",
      "264 done\n",
      "278 done\n",
      "273 done\n",
      "270 done\n",
      "260 done\n",
      "279 done\n",
      "274 done\n",
      "261 done\n",
      "265 done\n",
      "271 done\n",
      "280 done\n",
      "272 done\n",
      "275 done\n",
      "262 done\n",
      "281 done\n",
      "266 done\n",
      "273 done\n",
      "276 done\n",
      "263 done\n",
      "282 done\n",
      "267 done\n",
      "277 done\n",
      "274 done\n",
      "264 done\n",
      "268 done\n",
      "283 done\n",
      "278 done\n",
      "275 done\n",
      "269 done\n",
      "284 done\n",
      "279 done\n",
      "276 done\n",
      "265 done\n",
      "280 done\n",
      "277 done\n",
      "270 done\n",
      "285 done\n",
      "266 done\n",
      "281 done\n",
      "278 done\n",
      "271 done\n",
      "267 done\n",
      "286 done\n",
      "282 done\n",
      "272 done\n",
      "279 done\n",
      "268 done\n",
      "287 done\n",
      "283 done\n",
      "280 done\n",
      "269 done\n",
      "273 done\n",
      "288 done\n",
      "270 done\n",
      "284 done\n",
      "281 done\n",
      "274 done\n",
      "282 done\n",
      "271 done\n",
      "289 done\n",
      "285 done\n",
      "275 done\n",
      "272 done\n",
      "283 done\n",
      "286 done\n",
      "276 done\n",
      "290 done\n",
      "284 done\n",
      "277 done\n",
      "273 done\n",
      "291 done\n",
      "287 done\n",
      "285 done\n",
      "278 done\n",
      "274 done\n",
      "288 done\n",
      "292 done\n",
      "286 done\n",
      "279 done\n",
      "275 done\n",
      "293 done\n",
      "289 done\n",
      "280 done\n",
      "287 done\n",
      "276 done\n",
      "290 done\n",
      "281 done\n",
      "288 done\n",
      "277 done\n",
      "294 done\n",
      "291 done\n",
      "282 done\n",
      "278 done\n",
      "295 done\n",
      "289 done\n",
      "292 done\n",
      "283 done\n",
      "279 done\n",
      "296 done\n",
      "284 done\n",
      "293 done\n",
      "280 done\n",
      "290 done\n",
      "297 done\n",
      "285 done\n",
      "281 done\n",
      "291 done\n",
      "298 done\n",
      "294 done\n",
      "282 done\n",
      "286 done\n",
      "295 done\n",
      "292 done\n",
      "283 done\n",
      "296 done\n",
      "299 done\n",
      "287 done\n",
      "284 done\n",
      "293 done\n",
      "297 done\n",
      "288 done\n",
      "300 done\n",
      "285 done\n",
      "301 done\n",
      "298 done\n",
      "294 done\n",
      "289 done\n",
      "286 done\n",
      "302 done\n",
      "295 done\n",
      "299 done\n",
      "303 done\n",
      "290 done\n",
      "287 done\n",
      "296 done\n",
      "300 done\n",
      "291 done\n",
      "304 done\n",
      "301 done\n",
      "288 done\n",
      "297 done\n",
      "292 done\n",
      "305 done\n",
      "302 done\n",
      "289 done\n",
      "298 done\n",
      "306 done\n",
      "293 done\n",
      "303 done\n",
      "290 done\n",
      "307 done\n",
      "299 done\n",
      "291 done\n",
      "304 done\n",
      "294 done\n",
      "300 done\n",
      "308 done\n",
      "295 done\n",
      "305 done\n",
      "309 done\n",
      "292 done\n",
      "301 done\n",
      "310 done\n",
      "296 done\n",
      "302 done\n",
      "306 done\n",
      "293 done\n",
      "311 done\n",
      "297 done\n",
      "307 done\n",
      "303 done\n",
      "298 done\n",
      "308 done\n",
      "312 done\n",
      "294 done\n",
      "304 done\n",
      "309 done\n",
      "295 done\n",
      "313 done\n",
      "299 done\n",
      "305 done\n",
      "310 done\n",
      "296 done\n",
      "300 done\n",
      "314 done\n",
      "306 done\n",
      "311 done\n",
      "297 done\n",
      "301 done\n",
      "307 done\n",
      "315 done\n",
      "298 done\n",
      "312 done\n",
      "302 done\n",
      "308 done\n",
      "316 done\n",
      "313 done\n",
      "303 done\n",
      "309 done\n",
      "299 done\n",
      "317 done\n",
      "314 done\n",
      "310 done\n",
      "300 done\n",
      "318 done\n",
      "304 done\n",
      "301 done\n",
      "311 done\n",
      "315 done\n",
      "305 done\n",
      "302 done\n",
      "316 done\n",
      "312 done\n",
      "319 done\n",
      "306 done\n",
      "303 done\n",
      "317 done\n",
      "313 done\n",
      "307 done\n",
      "320 done\n",
      "308 done\n",
      "318 done\n",
      "304 done\n",
      "309 done\n",
      "314 done\n",
      "305 done\n",
      "310 done\n",
      "319 done\n",
      "311 done\n",
      "315 done\n",
      "321 done\n",
      "306 done\n",
      "316 done\n",
      "322 done\n",
      "312 done\n",
      "307 done\n",
      "320 done\n",
      "313 done\n",
      "317 done\n",
      "323 done\n",
      "308 done\n",
      "318 done\n",
      "321 done\n",
      "309 done\n",
      "324 done\n",
      "314 done\n",
      "322 done\n",
      "310 done\n",
      "315 done\n",
      "325 done\n",
      "311 done\n",
      "319 done\n",
      "323 done\n",
      "326 done\n",
      "316 done\n",
      "324 done\n",
      "327 done\n",
      "312 done\n",
      "317 done\n",
      "320 done\n",
      "325 done\n",
      "313 done\n",
      "318 done\n",
      "328 done\n",
      "326 done\n",
      "321 done\n",
      "314 done\n",
      "327 done\n",
      "329 done\n",
      "319 done\n",
      "322 done\n",
      "315 done\n",
      "328 done\n",
      "330 done\n",
      "323 done\n",
      "316 done\n",
      "320 done\n",
      "329 done\n",
      "324 done\n",
      "331 done\n",
      "317 done\n",
      "321 done\n",
      "330 done\n",
      "325 done\n",
      "332 done\n",
      "318 done\n",
      "322 done\n",
      "326 done\n",
      "333 done\n",
      "331 done\n",
      "323 done\n",
      "327 done\n",
      "334 done\n",
      "332 done\n",
      "319 done\n",
      "324 done\n",
      "333 done\n",
      "328 done\n",
      "335 done\n",
      "334 done\n",
      "325 done\n",
      "320 done\n",
      "336 done\n",
      "329 done\n",
      "326 done\n",
      "335 done\n",
      "337 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 done\n",
      "321 done\n",
      "336 done\n",
      "327 done\n",
      "338 done\n",
      "322 done\n",
      "337 done\n",
      "328 done\n",
      "331 done\n",
      "323 done\n",
      "339 done\n",
      "338 done\n",
      "332 done\n",
      "324 done\n",
      "329 done\n",
      "340 done\n",
      "339 done\n",
      "325 done\n",
      "333 done\n",
      "340 done\n",
      "330 done\n",
      "326 done\n",
      "341 done\n",
      "334 done\n",
      "327 done\n",
      "341 done\n",
      "331 done\n",
      "335 done\n",
      "342 done\n",
      "332 done\n",
      "336 done\n",
      "328 done\n",
      "343 done\n",
      "342 done\n",
      "337 done\n",
      "333 done\n",
      "344 done\n",
      "343 done\n",
      "329 done\n",
      "345 done\n",
      "338 done\n",
      "344 done\n",
      "334 done\n",
      "346 done\n",
      "330 done\n",
      "339 done\n",
      "345 done\n",
      "335 done\n",
      "347 done\n",
      "340 done\n",
      "336 done\n",
      "331 done\n",
      "348 done\n",
      "346 done\n",
      "337 done\n",
      "347 done\n",
      "332 done\n",
      "341 done\n",
      "349 done\n",
      "348 done\n",
      "338 done\n",
      "333 done\n",
      "350 done\n",
      "342 done\n",
      "349 done\n",
      "334 done\n",
      "339 done\n",
      "351 done\n",
      "343 done\n",
      "350 done\n",
      "340 done\n",
      "335 done\n",
      "344 done\n",
      "351 done\n",
      "336 done\n",
      "352 done\n",
      "345 done\n",
      "341 done\n",
      "353 done\n",
      "337 done\n",
      "346 done\n",
      "352 done\n",
      "354 done\n",
      "342 done\n",
      "347 done\n",
      "338 done\n",
      "353 done\n",
      "355 done\n",
      "348 done\n",
      "343 done\n",
      "339 done\n",
      "356 done\n",
      "354 done\n",
      "344 done\n",
      "349 done\n",
      "340 done\n",
      "355 done\n",
      "345 done\n",
      "357 done\n",
      "350 done\n",
      "356 done\n",
      "346 done\n",
      "341 done\n",
      "358 done\n",
      "351 done\n",
      "347 done\n",
      "357 done\n",
      "359 done\n",
      "348 done\n",
      "360 done\n",
      "358 done\n",
      "342 done\n",
      "361 done\n",
      "352 done\n",
      "359 done\n",
      "349 done\n",
      "343 done\n",
      "360 done\n",
      "344 done\n",
      "353 done\n",
      "362 done\n",
      "350 done\n",
      "361 done\n",
      "345 done\n",
      "351 done\n",
      "363 done\n",
      "354 done\n",
      "362 done\n",
      "346 done\n",
      "364 done\n",
      "355 done\n",
      "363 done\n",
      "347 done\n",
      "365 done\n",
      "356 done\n",
      "364 done\n",
      "352 done\n",
      "348 done\n",
      "366 done\n",
      "365 done\n",
      "357 done\n",
      "353 done\n",
      "349 done\n",
      "366 done\n",
      "358 done\n",
      "367 done\n",
      "354 done\n",
      "367 done\n",
      "350 done\n",
      "359 done\n",
      "355 done\n",
      "368 done\n",
      "351 done\n",
      "360 done\n",
      "356 done\n",
      "368 done\n",
      "369 done\n",
      "361 done\n",
      "369 done\n",
      "357 done\n",
      "370 done\n",
      "352 done\n",
      "362 done\n",
      "370 done\n",
      "371 done\n",
      "358 done\n",
      "371 done\n",
      "353 done\n",
      "363 done\n",
      "372 done\n",
      "359 done\n",
      "372 done\n",
      "354 done\n",
      "364 done\n",
      "360 done\n",
      "355 done\n",
      "373 done\n",
      "373 done\n",
      "365 done\n",
      "356 done\n",
      "361 done\n",
      "374 done\n",
      "374 done\n",
      "357 done\n",
      "362 done\n",
      "366 done\n",
      "375 done\n",
      "375 done\n",
      "358 done\n",
      "363 done\n",
      "367 done\n",
      "359 done\n",
      "364 done\n",
      "376 done\n",
      "368 done\n",
      "360 done\n",
      "376 done\n",
      "365 done\n",
      "369 done\n",
      "361 done\n",
      "377 done\n",
      "370 done\n",
      "366 done\n",
      "377 done\n",
      "362 done\n",
      "378 done\n",
      "371 done\n",
      "379 done\n",
      "378 done\n",
      "367 done\n",
      "363 done\n",
      "372 done\n",
      "379 done\n",
      "380 done\n",
      "364 done\n",
      "368 done\n",
      "380 done\n",
      "373 done\n",
      "381 done\n",
      "365 done\n",
      "369 done\n",
      "381 done\n",
      "374 done\n",
      "382 done\n",
      "370 done\n",
      "382 done\n",
      "366 done\n",
      "371 done\n",
      "383 done\n",
      "375 done\n",
      "383 done\n",
      "367 done\n",
      "372 done\n",
      "384 done\n",
      "384 done\n",
      "368 done\n",
      "373 done\n",
      "385 done\n",
      "369 done\n",
      "376 done\n",
      "385 done\n",
      "374 done\n",
      "386 done\n",
      "370 done\n",
      "386 done\n",
      "377 done\n",
      "371 done\n",
      "375 done\n",
      "387 done\n",
      "387 done\n",
      "378 done\n",
      "372 done\n",
      "379 done\n",
      "388 done\n",
      "388 done\n",
      "373 done\n",
      "380 done\n",
      "376 done\n",
      "389 done\n",
      "389 done\n",
      "374 done\n",
      "381 done\n",
      "390 done\n",
      "390 done\n",
      "377 done\n",
      "375 done\n",
      "391 done\n",
      "382 done\n",
      "391 done\n",
      "378 done\n",
      "383 done\n",
      "379 done\n",
      "392 done\n",
      "392 done\n",
      "384 done\n"
     ]
    }
   ],
   "source": [
    "# words_sample = [\"pizza hut\", \"burger king\", \"south africa\", \"nasa\"]\n",
    "# del og_dict\n",
    "def calculate_sim(words, word1, max_sim, closest_word):\n",
    "    t = time.time()\n",
    "    i = 0\n",
    "    for word2 in words:\n",
    "        try:\n",
    "            sim = wiki2vec.similarity(\"_\".join(word1.lower().split()), \"_\".join(word2.split()))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                closest_word = word2\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    print (\"Original word: \", word1, \"Closest Word: \", closest_word)\n",
    "    print (\"Took me {} seconds to iteration of sim compare...\".format(time.time()-a))\n",
    "    sys.stdout.flush()\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_w2v(word1):\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    if word1 not in wiki2vec.wv.vocab:\n",
    "        print (\"Original word not in vocab\", word1)\n",
    "        return (closest_word, max_sim)\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, word1, max_sim, closest_word)\n",
    "    return word1, closest_word          \n",
    "\n",
    "a = time.time()\n",
    "\n",
    "# closest_word = closest_word_w2v(\"margherita pizza\")\n",
    "\n",
    "# closest_word_w2v(\"nelson mandela\")\n",
    "\n",
    "resolved = dict()\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    for res in executor.map(closest_word_w2v, failed):\n",
    "        resolved[res[0]] = res[1]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, pickle\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "\n",
    "f = open(\"../junk/failed_words\", \"rb\") \n",
    "failed, words = pickle.load(f)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def compare_sim(words, word_to_compare, max_sim, closest_word):\n",
    "\n",
    "    word_embeddings = extractUSEEmbeddings(words)\n",
    "    \n",
    "    for i,w in enumerate(word_embeddings):\n",
    "        sim = np.dot(word_to_compare, w)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            closest_word = words[i]\n",
    "\n",
    "    return (closest_word, max_sim)\n",
    "\n",
    "def closest_word_USE(argument):\n",
    "    \n",
    "    word, embed = compare\n",
    "    len_part = 100000\n",
    "    max_sim = -1000\n",
    "    n_parts = ceil(len(words)/len_part)\n",
    "    closest_word = \"\"\n",
    "    for i in range(n_parts):\n",
    "        words_part = words[i*len_part:(i+1)*len_part]\n",
    "        closest_word, max_sim = calculate_sim(words_part, embed, max_sim, closest_word)\n",
    "    with counter.get_lock():\n",
    "        counter.value += 1\n",
    "    print (\"Original word: \", word, \"Closest Word: \", closest_word, \"Sim: \", max_sim)\n",
    "    print (\"Percentage done: \", float(counter.value*100/len(failed)))\n",
    "    return word1, closest_word, max_sim\n",
    "\n",
    "\n",
    "resolved = dict()\n",
    "print (\"Working on it...\")\n",
    "counter = Value('i', 0)\n",
    "a = time.time()\n",
    "failed_embeddings = extractUSEEmbeddings(failed)\n",
    "print (\"Took me {} seconds to extract USE embeddings...\".format(time.time()-a))\n",
    "with concurrent.futures.ProcessPoolExecutor(=) as executor:\n",
    "    for res in executor.map(closest_word_w2v, zip(failed, failed_embeddings)):\n",
    "        resolved[res[0]] = (res[1], res[2])\n",
    "\n",
    "f = open(\"resolved\", \"wb\")\n",
    "pickle.dump(resolved, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'approach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-484d7c044890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresolved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"approach\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'approach'"
     ]
    }
   ],
   "source": [
    "resolved[\"approach\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005288124084472656\n",
      "0.00016236305236816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "\"abrkadabra\" in w2v.wv\n",
    "print (time.time()-a)\n",
    "a = time.time()\n",
    "try:\n",
    "    w2v.similarity(\"margherita_pizza\", \"abrkadabra\")\n",
    "except:    \n",
    "    print (time.time()-a)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2id_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0884,  0.2092, -0.1895, -0.1527, -0.0978,  0.0378, -0.1611,\n",
       "        0.0245,  0.0549, -0.2892,  0.0931, -0.3243, -0.2276, -0.0727,\n",
       "        0.0521, -0.2883, -0.0754, -0.0059, -0.0705, -0.3562, -0.1019,\n",
       "        0.0847,  0.111 ,  0.0049, -0.3304, -0.2235,  0.1369, -0.1037,\n",
       "       -0.0751, -0.3887,  0.1092, -0.1504,  0.0167,  0.0217,  0.0204,\n",
       "        0.064 , -0.2647,  0.3114, -0.0973,  0.1509, -0.2116, -0.0882,\n",
       "        0.1436, -0.2557,  0.23  ,  0.1662,  0.04  , -0.1121,  0.0426,\n",
       "       -0.179 , -0.0356, -0.1443, -0.2153, -0.1841, -0.2113, -0.1561,\n",
       "        0.258 , -0.0593, -0.1704, -0.0394, -0.0992, -0.1615,  0.0623,\n",
       "       -0.1708, -0.1204,  0.2041,  0.173 , -0.3095, -0.0589, -0.0366,\n",
       "        0.0084, -0.2201, -0.3896, -0.2086,  0.323 , -0.0779, -0.1028,\n",
       "        0.0626,  0.2596,  0.0631,  0.18  ,  0.1857,  0.3112,  0.0103,\n",
       "        0.2184, -0.102 ,  0.0504,  0.0907,  0.2355,  0.2216,  0.0125,\n",
       "        0.0075,  0.0846, -0.1534,  0.4137,  0.0309, -0.2167, -0.0785,\n",
       "       -0.0552,  0.1009,  0.2382,  0.0789, -0.0333, -0.2412, -0.1341,\n",
       "        0.0201,  0.2794,  0.0011,  0.0298, -0.1577, -0.1338, -0.2247,\n",
       "        0.0086, -0.1434,  0.1252,  0.1057, -0.0273, -0.1806,  0.07  ,\n",
       "       -0.0414, -0.2173, -0.1507, -0.1246, -0.006 , -0.3306,  0.2111,\n",
       "       -0.1423,  0.0107, -0.1104, -0.1613,  0.4693,  0.0907, -0.1883,\n",
       "        0.0796, -0.0664, -0.0051, -0.2936, -0.146 ,  0.3968,  0.126 ,\n",
       "       -0.003 ,  0.0855,  0.2327,  0.0266, -0.1693, -0.1261,  0.1542,\n",
       "       -0.062 ,  0.0307,  0.0056, -0.274 ,  0.1674, -0.0414,  0.2391,\n",
       "       -0.2105, -0.1144,  0.0929, -0.12  ,  0.1924, -0.0865, -0.0856,\n",
       "        0.1252,  0.1665, -0.2329, -0.0251, -0.0148,  0.0345, -0.188 ,\n",
       "       -0.0538,  0.0334,  0.1629, -0.1095, -0.1309, -0.0365, -0.2053,\n",
       "        0.0962,  0.5739, -0.1491,  0.1356,  0.1067,  0.2074,  0.1196,\n",
       "        0.0801,  0.2219, -0.1299,  0.0929, -0.2259, -0.1049, -0.3055,\n",
       "       -0.1043, -0.0748,  0.1801, -0.35  ,  0.0795,  0.2982, -0.049 ,\n",
       "        0.1819, -0.3701,  0.1377, -0.0465,  0.1129, -0.2537,  0.1466,\n",
       "       -0.032 , -0.005 , -0.0516,  0.0883, -0.1519, -0.1497, -0.0941,\n",
       "       -0.1741,  0.1991, -0.1138, -0.0219, -0.122 , -0.0123, -0.0136,\n",
       "        0.2865, -0.3002, -0.0241,  0.0726,  0.0654, -0.235 , -0.1227,\n",
       "       -0.2656, -0.2147, -0.1193,  0.011 , -0.3328,  0.3237,  0.0375,\n",
       "        0.1942,  0.0471, -0.0605,  0.0011,  0.0636, -0.03  , -0.1148,\n",
       "       -0.0559, -0.1026, -0.3166,  0.3044,  0.1773,  0.2265,  0.1125,\n",
       "       -0.114 ,  0.1113,  0.0789, -0.2937, -0.2036, -0.2807,  0.0188,\n",
       "        0.0573,  0.0679,  0.1155,  0.1796, -0.1041,  0.1025, -0.0483,\n",
       "        0.0043,  0.1314,  0.0846,  0.1881,  0.1721,  0.463 , -0.1782,\n",
       "       -0.1547,  0.2518,  0.0634,  0.2856,  0.6526, -0.3789,  0.1362,\n",
       "        0.1545,  0.0719, -0.1429, -0.1224,  0.0836,  0.0851, -0.0349,\n",
       "       -0.2552, -0.2929, -0.183 ,  0.2112, -0.0847, -0.148 , -0.1365,\n",
       "       -0.1245,  0.3218,  0.0563,  0.245 ,  0.0132, -0.1204,  0.0391,\n",
       "       -0.1132, -0.0707,  0.2167, -0.0322, -0.1085,  0.0054],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2vec[\"january\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"../resolved_wiki2vec.pkl\", \"rb\")\n",
    "resolved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../junk/failed_words\", \"wb+\") \n",
    "pickle.dump([failed, words], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = resolved.keys()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "embed = extractUSEEmbeddings([\"orange\", \"apple\", \"elon musk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "glove_file = datapath('/data/Vivek/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"/data/Vivek/glove_tmp\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(success)\n",
    "words = ['_unk_']\n",
    "idx = 1\n",
    "word2idx = {\"_unk_\": 0}\n",
    "vectors = bcolz.carray(np.random.random(300), rootdir=embeddings_folder, mode='w')\n",
    "with open(embeddings_file, 'r') as f:\n",
    "    for l in f:\n",
    "        line = [a[::-1] for a in l[::-1].split(\" \", 300)[::-1]]\n",
    "        word, vector = line[0], [float(s) for s in line[1:]]\n",
    "        if len(vector) != 300:\n",
    "            print (len(vector))\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        words.append(word)\n",
    "        vectors.append(np.array(vector).astype(np.float))\n",
    "        word2idx[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = bcolz.carray(np.random.rand(1, 300), rootdir=embeddings_folder, mode='w')\n",
    "v.append(np.resize(np.array(vector), (1, 300)).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89821\n"
     ]
    }
   ],
   "source": [
    "true_file = open(\"../files/dataset/dataset_t.tsv\", \"r\").read().split(\"\\n\")\n",
    "false_file = open(\"../files/dataset/dataset_f.tsv\", \"r\").read().split(\"\\n\")\n",
    "print (len(false_file))\n",
    "false_file = false_file[:int(len(false_file) * float(dbpedia_neg/100))]\n",
    "\n",
    "custom_train = true_file[:int(0.9*len(true_file))] + false_file[:int(0.9*len(false_file))]\n",
    "custom_test = true_file[int(0.9*len(true_file)):] + false_file[int(0.9*len(false_file)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18969, 2109, 13473)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_train), len(custom_test), len(false_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_neg = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
