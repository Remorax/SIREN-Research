{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing\n",
    "from spacy.tokens.token import Token\n",
    "\n",
    "MAX_PATH_LEN = 6\n",
    "\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if MAX_PATH_LEN >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            paths.extend([stringifyFilterPath(path) for path in appendingElem])\n",
    "\n",
    "    return paths\n",
    "\n",
    "def splitFile (file, n):\n",
    "    inputfile = open(file, 'r')\n",
    "    output = None\n",
    "    suffix = 0\n",
    "    for (i, line) in enumerate(inputfile):\n",
    "        if i % n == 0:\n",
    "            if output:\n",
    "                output.close()\n",
    "            output = open(file + \"_split_\" + str(suffix) + '.txt', 'w+')\n",
    "            suffix += 1\n",
    "        output.write(line)\n",
    "    output.close()\n",
    "    return suffix\n",
    "\n",
    "def parseText(idx):\n",
    "    global file\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "    fileName = file + \"_split_\" + str(idx) + \".txt\"\n",
    "    op = file + \"_parsed_\" + str(idx)\n",
    "\n",
    "    with open(fileName, \"r\") as inp:\n",
    "        with open(op, \"w+\") as out:\n",
    "            for i,para in enumerate(inp):\n",
    "                if not para.strip(): continue\n",
    "                nounChunks = list(nlp(para).nounChunks).copy()\n",
    "                sentences = nlp(para.strip()).sents\n",
    "                for sentence in sentences:\n",
    "                    if \"<doc id=\" in sentence.text or \"</doc>\" in sentence.text:\n",
    "                        continue\n",
    "                    sentenceNounChunks = [n for n in nounChunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "                    dependencies = getDependencyPaths(sentence, nlp, sentenceNounChunks)\n",
    "                    if dependencies:\n",
    "                        allpaths = [\"\\t\".join(path) for path in dependencies if path]\n",
    "                        out.write(\"\\n\".join(allpaths))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    file = \"../junk/temp\"\n",
    "    countlines = \"wc -l \" + file \n",
    "    output, _ = subprocess.Popen(countlines.split(), stdout=subprocess.PIPE).communicate()\n",
    "    n = int(output.decode(\"utf-8\").strip().split(\" \")[0]) + 1\n",
    "    m = int(n/20)\n",
    "    suffix = splitFile(file, m)\n",
    "    processes = []\n",
    "    for i in range(20):\n",
    "        p = multiprocessing.Process(target=parseText, args=(i,))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    mergeParsedOutput = \"cat \" + file + \"_parsed_* > parsed_paths\"  \n",
    "    output, _ = subprocess.Popen(mergeParsedOutput.split(), shell=True).communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitFileName = \"file_split_01\".split(\"_\")\n",
    "file = \"_\".join(splitFileName[:-1]) + \"_\" + (\"0\" + splitFileName[-1] if len(splitFileName[-1]) == 1 else  splitFileName[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "DocoptExit",
     "evalue": "Usage:\n        create_resource_from_corpus_2.py <triplet_file> <resource_prefix>",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mDocoptExit\u001b[0m\u001b[0;31m:\u001b[0m Usage:\n        create_resource_from_corpus_2.py <triplet_file> <resource_prefix>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l2r = btopen(paths_folder + \"/\" + prefix + '_l2r.db', 'c')\n",
    "\n",
    "with open(file) as inp:\n",
    "    for line in inp:\n",
    "        x, y, path, count = line.strip().split('\\t')\n",
    "\n",
    "        key = str(x) + '_' + str(y)\n",
    "        current = path + \":\" + count\n",
    "\n",
    "        if key in l2r:\n",
    "            pastkeys = l2r[key].decode('utf-8')\n",
    "            current =  pastkeys + current\n",
    "        \n",
    "        current = current.encode(\"utf-8\")\n",
    "        key = key.encode(\"utf-8\")\n",
    "        \n",
    "        l2r[key] = current\n",
    "\n",
    "l2r.sync()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " defaultdict(<method-wrapper '__next__' of itertools.count object at 0x10db93f48>,\n",
       "             {'ok': 0}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "def somefn(pos_index):\n",
    "    randomstr = \"ok\"\n",
    "    return pos_index[randomstr]\n",
    "\n",
    "pos_index = defaultdict(count(0).__next__)\n",
    "somefn(pos_index), pos_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_prefix = \"../junk/Files/temp_threshold_3_4/temp\"\n",
    "\n",
    "import bsddb3\n",
    "\n",
    "def vectorize_path(path, lemma_index, pos_index, dep_index, dir_index):\n",
    "    \"\"\"\n",
    "    Return a vector representation of the path\n",
    "    :param path:\n",
    "    :param lemma_index:\n",
    "    :param pos_index:\n",
    "    :param dep_index:\n",
    "    :param dir_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    path_edges = [vectorize_edge(edge, lemma_index, pos_index, dep_index, dir_index) for edge in path.split('_')]\n",
    "    if None in path_edges:\n",
    "        return None\n",
    "    else:\n",
    "        return tuple(path_edges)\n",
    "\n",
    "\n",
    "def vectorize_edge(edge, lemma_index, pos_index, dep_index, dir_index):\n",
    "    \"\"\"\n",
    "    Return a vector representation of the edge: concatenate lemma/pos/dep and add direction symbols\n",
    "    :param edge:\n",
    "    :param lemma_index:\n",
    "    :param pos_index:\n",
    "    :param dep_index:\n",
    "    :param dir_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    direction = ' '\n",
    "\n",
    "    # Get the direction\n",
    "    if edge.startswith('<') or edge.startswith('>'):\n",
    "        direction = 's' + edge[0]\n",
    "        edge = edge[1:]\n",
    "    elif edge.endswith('<') or edge.endswith('>'):\n",
    "        direction = 'e' + edge[-1]\n",
    "        edge = edge[:-1]\n",
    "\n",
    "    try:\n",
    "        lemma, pos, dep = edge.split('/')\n",
    "        print (lemma)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return tuple([lemma_index.get(lemma, 0), pos_index[pos], dep_index[dep], dir_index[direction]])\n",
    "\n",
    "def get_paths(corpus, x, y):\n",
    "    \"\"\"\n",
    "    Get the paths that connect x and y in the corpus\n",
    "    :param corpus: the corpus' resource object\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x_to_y_paths = corpus.get_relations(x, y)\n",
    "    y_to_x_paths = corpus.get_relations(y, x)\n",
    "    x_term = corpus.get_term_by_id(x)\n",
    "    y_term = corpus.get_term_by_id(y)\n",
    "#     print ([type(corpus.get_path_by_id(path)) for (path, count) in x_to_y_paths.items()], type(x_term), type(y_term))\n",
    "    paths = { corpus.get_path_by_id(path).replace(\"X/\", x_term+\"/\").replace(\"Y/\", y_term+\"/\") : count for (path, count) in x_to_y_paths.items() }\n",
    "    paths.update({ corpus.get_path_by_id(path).replace(\"X/\", y_term+\"/\").replace(\"Y/\", x_term+\"/\") : count\n",
    "                   for (path, count) in y_to_x_paths.items() })\n",
    "    return paths\n",
    "\n",
    "class KnowledgeResource:\n",
    "    \"\"\"\n",
    "    Holds the resource graph data\n",
    "    \"\"\"\n",
    "    def __init__(self, resource_prefix):\n",
    "        \"\"\"\n",
    "        Init the knowledge resource\n",
    "        :param resource_prefix - the resource directory and file prefix\n",
    "        \"\"\"\n",
    "        self.term_to_id = bsddb3.btopen(resource_prefix + '_word_to_id.db', 'r')\n",
    "        self.id_to_term = bsddb3.btopen(resource_prefix + '_id_to_word.db', 'r')\n",
    "        self.path_to_id = bsddb3.btopen(resource_prefix + '_path_to_id.db', 'r')\n",
    "        self.id_to_path = bsddb3.btopen(resource_prefix + '_id_to_path.db', 'r')\n",
    "        self.l2r_edges = bsddb3.btopen(resource_prefix + '_word_occurence_map.db', 'r')\n",
    "\n",
    "    def get_term_by_id(self, id):\n",
    "        return self.id_to_term[str(id).encode(\"utf-8\")].decode(\"utf-8\")\n",
    "\n",
    "    def get_path_by_id(self, id):\n",
    "        return self.id_to_path[str(id).encode(\"utf-8\")].decode(\"utf-8\")\n",
    "\n",
    "    def get_id_by_term(self, term):\n",
    "        return int(self.term_to_id[term.encode(\"utf-8\")]) if self.term_to_id.has_key(term.encode(\"utf-8\")) else -1\n",
    "\n",
    "    def get_id_by_path(self, path):\n",
    "        return int(self.path_to_id[path.encode(\"utf-8\")]) if self.path_to_id.has_key(path.encode(\"utf-8\")) else -1\n",
    "\n",
    "    def get_relations(self, x, y):\n",
    "        \"\"\"\n",
    "        Returns the relations from x to y\n",
    "        \"\"\"\n",
    "        path_dict = {}\n",
    "        key = str(x) + '_' + str(y)\n",
    "        path_str = self.l2r_edges[key.encode(\"utf-8\")].decode(\"utf-8\") if self.l2r_edges.has_key(key.encode(\"utf-8\")) else ''\n",
    "\n",
    "        if len(path_str) > 0:\n",
    "#             print (path_str)\n",
    "#             print ([p for p in path_str.split(',')])\n",
    "            paths = [tuple([int(x) for x in p.split(':')]) for p in path_str.split(',')]\n",
    "            path_dict = { path : count for (path, count) in paths }\n",
    "\n",
    "        return path_dict\n",
    "\n",
    "    \n",
    "corpus = KnowledgeResource(corpus_prefix)\n",
    "dataset = {tuple(l.split(\"\\t\")[:2]): l.split(\"\\t\")[2] for l in open(\"../junk/temp_dataset.tsv\").read().split(\"\\n\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n",
      "be\n",
      "philosophy\n",
      "reject\n",
      "anarchism\n",
      "be\n",
      "philosophy\n"
     ]
    }
   ],
   "source": [
    "pos_index = defaultdict(count(0).__next__)\n",
    "dep_index = defaultdict(count(0).__next__)\n",
    "dir_index = defaultdict(count(0).__next__)\n",
    "lemma_index = defaultdict(count(0).__next__)\n",
    "\n",
    "keys = [(corpus.get_id_by_term(str(x)), corpus.get_id_by_term(str(y))) for (x, y) in dataset]\n",
    "paths_x_to_y = [{ vectorize_path(path, lemma_index, pos_index, dep_index, dir_index) : count\n",
    "                      for path, count in get_paths(corpus, x_id, y_id).items() }\n",
    "                    for (x_id, y_id) in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{((0, 0, 0, 0), (0, 1, 1, 1), (0, 0, 2, 2), (0, 1, 3, 3)): 1,\n",
       "  ((0, 0, 0, 0), (0, 1, 1, 1), (0, 0, 2, 2)): 3},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_x_to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{((0, 0, 0, 0), (0, 1, 1, 1), (0, 0, 2, 2), (0, 1, 3, 3)): 1,\n",
       "  ((0, 0, 0, 0), (0, 1, 1, 1), (0, 0, 2, 2)): 3},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_x_to_y = [ { p : c for p, c in paths_x_to_y[i].items() if p is not None } for i in range(len(keys)) ]\n",
    "paths_x_to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "HIDDEN_DIM = 60\n",
    "LEMMA_DIM = 300\n",
    "POS_DIM = 4\n",
    "DEP_DIM = 5\n",
    "DIR_DIM = 1\n",
    "\n",
    "from dynet import *\n",
    "model = Model()\n",
    "network_input = HIDDEN_DIM\n",
    "\n",
    "builder = LSTMBuilder(NUM_LAYERS, LEMMA_DIM + POS_DIM + DEP_DIM + DIR_DIM, network_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_dynet.ParameterCollection"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters = {}\n",
    "model.add_parameters((4, 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
