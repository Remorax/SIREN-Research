{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import spacy, subprocess, itertools, multiprocessing\n",
    "from spacy.tokens.token import Token\n",
    "\n",
    "MAX_PATH_LEN = 6\n",
    "\n",
    "\n",
    "def stringifyEdge(word, root=True):\n",
    "    try:\n",
    "        w = word.root\n",
    "    except:\n",
    "        w = word\n",
    "\n",
    "    if isinstance(word, Token):\n",
    "        word = word.lemma_.strip().lower()\n",
    "    else:\n",
    "        word = ' '.join([wd.string.strip().lower() for wd in word])\n",
    "    pos, deps = w.pos_, w.dep_\n",
    "    path = '/'.join([word, pos, deps if deps and root else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def stringifyArg(word, edge):\n",
    "    try:\n",
    "        word = word.root\n",
    "    except:\n",
    "        pass\n",
    "    pos, deps = word.pos_, word.dep_\n",
    "    path = '/'.join([edge, pos, deps if deps else 'ROOT'])\n",
    "    return path\n",
    "\n",
    "def filterPaths(function, lowestCommonHead, paths):\n",
    "    path1 = [lowestCommonHead]\n",
    "    path1.extend(paths[:-1])\n",
    "    path2 = paths\n",
    "    return any(node not in function(path) for path, node in list(zip(path1, path2)))\n",
    "\n",
    "def notPunct(arr):\n",
    "    firstWord = arr[0]\n",
    "    return firstWord.tag_ != 'PUNCT' and len(firstWord.string.strip()) > 1\n",
    "\n",
    "def notEqual(x, y):\n",
    "    try:\n",
    "        return x!=y\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def checkHead(token, lowestCommonHead):\n",
    "    return isinstance(token, Token) and lowestCommonHead == token\n",
    "\n",
    "def getPathFromRoot(phrase):\n",
    "    paths = []\n",
    "    head = phrase.head\n",
    "    while phrase != head:\n",
    "        phrase = phrase.head\n",
    "        paths.append(phrase)\n",
    "        head = phrase.head\n",
    "    paths = paths[::-1]\n",
    "    return paths\n",
    "\n",
    "def breakCompoundWords(elem):\n",
    "    try:\n",
    "        root = elem.root\n",
    "        return root\n",
    "    except:\n",
    "        return elem\n",
    "\n",
    "def findMinLength(x, y):\n",
    "    if len(x) < len(y):\n",
    "        return (len(x), x)\n",
    "    return (len(y), y)\n",
    "\n",
    "def findLowestCommonHead(pathX, pathY, minLength, minArray):\n",
    "    lowestCommonHead = None\n",
    "    if minLength:        \n",
    "        uncommon = [i for i in range(minLength) if pathX[i] != pathY[i]]\n",
    "        if uncommon:\n",
    "            idx = uncommon[0] - 1\n",
    "        else:\n",
    "            idx = minLength - 1\n",
    "        lowestCommonHead = minArray[idx]\n",
    "    else:\n",
    "        idx = 0\n",
    "        if pathX:\n",
    "            lowestCommonHead = pathX[0]\n",
    "        elif pathY:\n",
    "            lowestCommonHead = pathY[0]\n",
    "        else:\n",
    "            lowestCommonHead = None\n",
    "    \n",
    "    return idx, lowestCommonHead\n",
    "\n",
    "def getShortestPath(tup):\n",
    "\n",
    "    xinit, yinit = tup[0], tup[1]\n",
    "\n",
    "    x, y = breakCompoundWords(xinit), breakCompoundWords(yinit)\n",
    "    \n",
    "    pathX, pathY = getPathFromRoot(x), getPathFromRoot(y)\n",
    "    \n",
    "    minLength, minArray = findMinLength(pathX, pathY)\n",
    "    \n",
    "    idx, lowestCommonHead = findLowestCommonHead(pathX, pathY, minLength, minArray)\n",
    "    \n",
    "    try:\n",
    "        pathX = pathX[idx+1:]\n",
    "        pathY = pathY[idx+1:]\n",
    "        checkLeft, checkRight = lambda h: h.lefts, lambda h: h.rights\n",
    "        if lowestCommonHead and (filterPaths(checkLeft, lowestCommonHead, pathX) or filterPaths(checkRight, lowestCommonHead, pathY)):\n",
    "            return None\n",
    "        pathX = pathX[::-1]\n",
    "\n",
    "        paths = [(None, xinit, pathX, lowestCommonHead, pathY, yinit, None)]\n",
    "        lefts, rights = list(xinit.lefts), list(yinit.rights)\n",
    "\n",
    "        if lefts and notPunct(lefts):\n",
    "            paths.append((lefts[0], xinit, pathX, lowestCommonHead, pathY, yinit, None))\n",
    "\n",
    "        if rights and notPunct(rights):\n",
    "            paths.append((None, xinit, pathX, lowestCommonHead, pathY, yinit, rights[0]))\n",
    "        \n",
    "        return paths\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return None\n",
    "\n",
    "def stringifyFilterPath(path):\n",
    "\n",
    "    lowestCommonHeads = []\n",
    "    (leftX, x, pathX, lowestCommonHead, pathY, y, rightY) = path\n",
    "\n",
    "    isXHead, isYHead = checkHead(x, lowestCommonHead), checkHead(y, lowestCommonHead)\n",
    "    signX = '' if isXHead else '>'\n",
    "    leftXPath  = []\n",
    "    if leftX:\n",
    "        edge_str = stringifyEdge(leftX)\n",
    "        leftXPath.append(edge_str + \"<\")\n",
    "\n",
    "    signY = '' if isYHead else '<'\n",
    "    rightYPath = []\n",
    "    if rightY:\n",
    "        edge_str = stringifyEdge(rightY)\n",
    "        rightYPath.append(\">\" + edge_str)\n",
    "\n",
    "    lowestCommonHeads = [[stringifyEdge(lowestCommonHead, False)] if lowestCommonHead and not (isYHead or isXHead) else []][0]\n",
    "    \n",
    "    if MAX_PATH_LEN >= len(pathX + leftXPath + pathY + rightYPath + lowestCommonHeads):\n",
    "        \n",
    "        if isinstance(x, Token):\n",
    "            stringifiedX = x.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedX = ' '.join([x_wd.string.strip().lower() for x_wd in x])\n",
    "        \n",
    "        if isinstance(y, Token):\n",
    "            stringifiedY = y.string.strip().lower()\n",
    "        else:\n",
    "            stringifiedY = ' '.join([y_wd.string.strip().lower() for y_wd in y])\n",
    "\n",
    "        stringifiedPathX, stringifiedPathY = [stringifyEdge(word) + \">\" for word in pathX], [\"<\" + stringifyEdge(word) for word in pathY]\n",
    "        stringifiedArgX, stringifiedArgY = [stringifyArg(x, 'X') + signX], [signY + stringifyArg(y, 'Y')]\n",
    "        \n",
    "        stringifiedPath = '_'.join(leftXPath + stringifiedArgX + stringifiedPathX + lowestCommonHeads + stringifiedPathY + stringifiedArgY + rightYPath)\n",
    "\n",
    "        return (stringifiedX, stringifiedY, stringifiedPath)\n",
    "\n",
    "    return None\n",
    "\n",
    "def getDependencyPaths(sentence, nlp, sentenceNounChunks):\n",
    "\n",
    "    nps = [(n, n.start, n.end) for n in sentenceNounChunks]\n",
    "    nps.extend([(word, pos, pos) for (pos, word) in enumerate(sentence) if word.tag_[:2] == 'NN' and len(word.string.strip()) > 2])\n",
    "    ls = list(itertools.product(nps, nps))\n",
    "    pairedConcepts = [(el[0][0], el[1][0]) for el in itertools.product(nps, nps) if el[1][1] > el[0][2] and notEqual(el[0], el[1])]\n",
    "    pairedConcepts = list(dict.fromkeys(pairedConcepts))\n",
    "    \n",
    "    paths = []\n",
    "    for pair in pairedConcepts:\n",
    "        appendingElem = getShortestPath(pair)\n",
    "        if appendingElem:\n",
    "            paths.extend([stringifyFilterPath(path) for path in appendingElem])\n",
    "\n",
    "    return paths\n",
    "\n",
    "def splitFile (file, n):\n",
    "    inputfile = open(file, 'r')\n",
    "    output = None\n",
    "    suffix = 0\n",
    "    for (i, line) in enumerate(inputfile):\n",
    "        if i % n == 0:\n",
    "            if output:\n",
    "                output.close()\n",
    "            output = open(file + \"_split_\" + str(suffix) + '.txt', 'w+')\n",
    "            suffix += 1\n",
    "        output.write(line)\n",
    "    output.close()\n",
    "    return suffix\n",
    "\n",
    "def parseText(idx):\n",
    "    global file\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "    fileName = file + \"_split_\" + str(idx) + \".txt\"\n",
    "    op = file + \"_parsed_\" + str(idx)\n",
    "\n",
    "    with open(fileName, \"r\") as inp:\n",
    "        with open(op, \"w+\") as out:\n",
    "            for i,para in enumerate(inp):\n",
    "                if not para.strip(): continue\n",
    "                nounChunks = list(nlp(para).nounChunks).copy()\n",
    "                sentences = nlp(para.strip()).sents\n",
    "                for sentence in sentences:\n",
    "                    if \"<doc id=\" in sentence.text or \"</doc>\" in sentence.text:\n",
    "                        continue\n",
    "                    sentenceNounChunks = [n for n in nounChunks if sentence.start <= n.start < n.end - 1 < sentence.end]\n",
    "                    dependencies = getDependencyPaths(sentence, nlp, sentenceNounChunks)\n",
    "                    if dependencies:\n",
    "                        allpaths = [\"\\t\".join(path) for path in dependencies if path]\n",
    "                        out.write(\"\\n\".join(allpaths))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    file = \"../junk/temp\"\n",
    "    countlines = \"wc -l \" + file \n",
    "    output, _ = subprocess.Popen(countlines.split(), stdout=subprocess.PIPE).communicate()\n",
    "    n = int(output.decode(\"utf-8\").strip().split(\" \")[0]) + 1\n",
    "    m = int(n/20)\n",
    "    suffix = splitFile(file, m)\n",
    "    processes = []\n",
    "    for i in range(20):\n",
    "        p = multiprocessing.Process(target=parseText, args=(i,))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machines"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nlp(\"International Business Machines has world-class research facilities.\"))[1].head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
