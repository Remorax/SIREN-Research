{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, urllib, itertools\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, Iterable\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import KeyedVectors\n",
    "from anytree import Node\n",
    "from nltk.tree import Tree\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def checkAppropriateURL(url):\n",
    "    parsedURL = urlparse(url)   \n",
    "\n",
    "    extn = parsedURL.path.split(\".\")[-1]\n",
    "    site = parsedURL.netloc.split(\".\")\n",
    "    path = parsedURL.path.split(\"/\")[-1]\n",
    "\n",
    "    if extn in [\"pdf\",\"doc\",\"docx\",\"xls\",\"xlsx\",\"ppt\",\"pptx\",\"odt\"]:\n",
    "        return False\n",
    "    if any(i in site for i in [\"pinterest\"]):\n",
    "        return False\n",
    "    try:\n",
    "        r = dict(requests.head(url).headers)[\"Content-Type\"].split(\";\")[0]\n",
    "        if(not r==\"text/html\"):\n",
    "            print (\"Failed\",url,r)\n",
    "            return False\n",
    "    except:\n",
    "        pass\n",
    "    return True\n",
    "\n",
    "def scrapeGoogleForAbstracts(query, count):\n",
    "    words = query.split()\n",
    "    q = (\"+\").join(words)\n",
    "#     print (q)\n",
    "    query_url_goog = \"https://www.google.co.in/search?q=\\\"\" + q + \"\\\"\"\n",
    "    data = requests.get(query_url_goog).text\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    h3Rows = soup.find_all(\"h3\", {\"class\":\"r\"})\n",
    "    spanRows = soup.find_all(\"span\", {\"class\":\"st\"})\n",
    "    zippedRows = zip(h3Rows,spanRows)\n",
    "    urls = []\n",
    "#     print (list(zippedRows))\n",
    "    for zippedRow in zippedRows:\n",
    "        try:\n",
    "            h3Row = zippedRow[0]\n",
    "            url = h3Row.find(\"a\")['href']\n",
    "            if(url.split(\"?\")[0]==\"/search\"):\n",
    "                continue\n",
    "        except (GeneratorExit, KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "        par = urllib.parse.parse_qs(urlparse(url).query)\n",
    "        try:\n",
    "            appendingURL = par['q'][0]\n",
    "        except:\n",
    "            continue\n",
    "        if(not checkAppropriateURL(appendingURL)):\n",
    "            continue\n",
    "        matchedQuery = query\n",
    "        allbElems = zippedRow[1].findAll('b')\n",
    "        for bElem in allbElems:\n",
    "            if bElem.string[0].isalnum():\n",
    "                matchedQuery = bElem.string\n",
    "        urls.append((matchedQuery,appendingURL))\n",
    "    i = 1\n",
    "    same = 0\n",
    "    prev = 0\n",
    "    while(len(urls)<count):\n",
    "#         print (\"Query: \",query)\n",
    "        tempurl = query_url_goog + \"&start=\" +str(i*10)\n",
    "        i+=1\n",
    "        data = requests.get(tempurl).text\n",
    "        soup = BeautifulSoup(data, \"lxml\")\n",
    "        h3Rows = soup.find_all(\"h3\", {\"class\":\"r\"})\n",
    "        spanRows = soup.find_all(\"span\", {\"class\":\"st\"})\n",
    "        zippedRows = zip(h3Rows,spanRows)\n",
    "        for (j,zippedRow) in enumerate(zippedRows):\n",
    "            try:\n",
    "                h3Row = zippedRow[0]\n",
    "                url = h3Row.find(\"a\")['href']\n",
    "            except (GeneratorExit, KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except:\n",
    "                continue\n",
    "            par = urllib.parse.parse_qs(urlparse(url).query)\n",
    "            try:\n",
    "                appendingURL = par['q'][0]\n",
    "                if(not checkAppropriateURL(appendingURL)):\n",
    "                    continue\n",
    "                matchedQuery = query\n",
    "                allbElems = zippedRow[1].findAll('b')\n",
    "                for bElem in allbElems:\n",
    "                    if bElem.string[0].isalnum():\n",
    "                        matchedQuery = bElem.string\n",
    "                urls.append((matchedQuery, appendingURL))\n",
    "            except (GeneratorExit, KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except:\n",
    "                continue\n",
    "        if prev == len(urls):\n",
    "            same +=1 \n",
    "            if same>=10:\n",
    "                same = 0\n",
    "                break\n",
    "        prev = len(urls)\n",
    "    return urls[:count]\n",
    "\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    finalList = [a.strip() for a in list(visible_texts) if a.strip()]\n",
    "    return \"\\n\".join(finalList)\n",
    "\n",
    "def getTextFromURL(url):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "   'Accept-Encoding': 'none',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'Connection': 'keep-alive'}\n",
    "    html = urllib.request.urlopen(urllib.request.Request(url, headers=hdr))\n",
    "    inputText = text_from_html(html)\n",
    "#     print (inputText)\n",
    "    return inputText\n",
    "\n",
    "def getAbstracts(query, count):\n",
    "    urls = scrapeGoogleForAbstracts(query, count)\n",
    "#     print (urls)\n",
    "#     print (\"lenght\",len(urls))\n",
    "    texts = []\n",
    "    for (query, url) in urls:\n",
    "#         print (url)\n",
    "        try:\n",
    "            text = getTextFromURL(url)\n",
    "        except (GeneratorExit, KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "        texts.append((query,url,text))\n",
    "    return texts\n",
    "\n",
    "def getHearstPatterns(word):\n",
    "    # Pattern, Instances after/before, One/many instances\n",
    "    patterns = []\n",
    "    patterns.append((word + \" such as\", True, True))\n",
    "    patterns.append((\"such \" + word + \" as\", True, True))\n",
    "    patterns.append((\"and other \" + word, False, True))\n",
    "    patterns.append((\"or other \" + word, False, True))\n",
    "    patterns.append((word + \", including\", True, True))\n",
    "    patterns.append((word + \", especially\", True, True))\n",
    "    return patterns\n",
    "\n",
    "finalList = []\n",
    "for word in [\"pizzas\", \"pizza toppings\"]:\n",
    "    instanceList = []\n",
    "    queriesList = getHearstPatterns(word)\n",
    "    for query in queriesList:\n",
    "        allAbstracts = getAbstracts(query[0], 50)\n",
    "        for abstractTuple in allAbstracts:\n",
    "            abstract = abstractTuple[2]\n",
    "            instanceList.append(abstract)\n",
    "    finalList.append(instanceList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed https://digitalcommons.liberty.edu/cgi/viewcontent.cgi?article=1016&context=paper_02_03 application/pdf\n",
      "Failed https://www.tripadvisor.co.za/ShowUserReviews-g261650-d1911240-r539623742-Pasini_s_Cafe-Bicheno_Tasmania.html text/plain\n",
      "Failed https://www.tripadvisor.ie/ShowUserReviews-g1725258-d2434985-r551248194-Ai_Tre_Archi-City_of_Venice_Veneto.html text/plain\n",
      "Failed https://biz.dominos.com/documents/1697006/6681236/DOM-227_SmartSliceBrochure_Trifold_8_5x11_Final_LoRes.pdf/2ba35601-2bcc-4313-7ac9-2c909be1838e application/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(finalList, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51227206, -1.2847223 ,  3.490544  , -0.93762606,  1.7334481 ,\n",
       "        0.27087292,  0.66152835, -0.84823054, -0.17715542,  0.07977004,\n",
       "        0.1507177 , -0.9045556 , -2.8559575 , -1.2382495 , -0.03531567,\n",
       "       -0.31199107,  1.3665978 , -2.2502768 , -1.5593722 , -0.17395428,\n",
       "        1.719916  ,  1.2555044 , -2.2604537 ,  0.7508089 , -1.1353773 ,\n",
       "        2.9665873 ,  0.9214447 , -0.68822056,  0.14849168,  1.8077168 ,\n",
       "        0.37879112, -1.2270753 , -0.38158682,  0.29182464,  0.03694698,\n",
       "        2.3279104 , -0.23605627, -0.38577148,  1.5197443 , -0.24042459,\n",
       "       -0.19521521, -0.5266814 ,  1.5598212 , -0.32768145,  1.0994464 ,\n",
       "        1.4933176 ,  0.3170104 ,  1.7047279 , -3.6475973 ,  0.86272013,\n",
       "       -0.03298703,  0.133931  , -1.2155799 ,  1.493747  , -1.050826  ,\n",
       "       -0.7165058 , -0.51827544,  0.08226824,  1.421184  ,  1.6986766 ,\n",
       "       -2.9063528 , -0.63867164,  0.00454799, -1.015708  ,  1.4364771 ,\n",
       "        0.7939562 ,  2.4334238 , -1.9164641 , -1.5604006 ,  0.4765302 ,\n",
       "       -0.2707858 ,  2.3362339 , -1.8825202 , -0.5216542 ,  1.4002205 ,\n",
       "       -1.3919283 ,  0.8743938 ,  1.2997327 , -1.9721962 ,  0.73033476,\n",
       "       -0.5663839 ,  0.27404532, -0.5061173 , -0.0311007 , -0.30793062,\n",
       "        0.30436447,  1.375715  , -2.4181888 ,  0.16469415,  1.0114619 ,\n",
       "       -0.711473  , -0.3124103 , -0.9443813 , -0.2698664 , -1.2679411 ,\n",
       "        0.20651078,  0.19547515, -0.67307824,  2.7420108 ,  1.1589596 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['toppings']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
