{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "engine = create_engine('sqlite:///onto.db', echo = False)\n",
    "c = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:20:25,575 INFO sqlalchemy.engine.base.Engine DELETE FROM final_node_decisions;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:DELETE FROM final_node_decisions;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:20:25,583 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:20:25,587 INFO sqlalchemy.engine.base.Engine COMMIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:COMMIT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x14229bfd0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = engine.connect()\n",
    "c.execution_options(autocommit=True).execute(\"DELETE FROM final_node_decisions;\")\n",
    "# results.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,622 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,632 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,640 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,644 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,648 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,666 INFO sqlalchemy.engine.base.Engine SELECT * FROM node_decisions INNER JOIN nodes ON node_decisions.node_id =nodes.id \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT * FROM node_decisions INNER JOIN nodes ON node_decisions.node_id =nodes.id \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 01:11:19,683 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 4, 1, 1324455346191020032, 4, 1, \"https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Domino'SPizza\"),\n",
       " (2, 3, 0, 1324455346191020032, 3, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Peri-Peri'),\n",
       " (3, 2, 1, 1324455346191020032, 2, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Marinara'),\n",
       " (4, 1, 1, 1324455346191020032, 1, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#TandooriPizza'),\n",
       " (5, 5, 1, 1324455346191020032, 5, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#IndianPizza'),\n",
       " (6, 5, 1, 1324449088826343424, 5, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#IndianPizza'),\n",
       " (7, 1, 1, 1324449088826343424, 1, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#TandooriPizza'),\n",
       " (8, 3, 1, 1324449088826343424, 3, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Peri-Peri'),\n",
       " (9, 2, 1, 1324449088826343424, 2, 1, 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Marinara'),\n",
       " (10, 4, 1, 1324449088826343424, 4, 1, \"https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Domino'SPizza\")]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('sqlite:///onto.db', echo = True)\n",
    "    # conn = sqlite3.connect('onto.db')\n",
    "    # c = conn.cursor()\n",
    "c = engine.connect()\n",
    "trans = c.begin()\n",
    "query = \"\"\"SELECT * FROM node_decisions INNER JOIN nodes ON node_decisions.node_id =nodes.id \"\"\"\n",
    "result = c.execute(query)\n",
    "result.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,230 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,233 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,241 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,244 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,249 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,256 INFO sqlalchemy.engine.base.Engine SELECT domain, range, property FROM class_relations \n",
      "        INNER JOIN final_class_decisions ON class_relations.id = final_class_decisions.relation_id\n",
      "        INNER JOIN ontologies ON class_relations.onto_id = ontologies.id\n",
      "        WHERE ontologies.name = :name AND final_class_decisions.approved != 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:SELECT domain, range, property FROM class_relations \n",
      "        INNER JOIN final_class_decisions ON class_relations.id = final_class_decisions.relation_id\n",
      "        INNER JOIN ontologies ON class_relations.onto_id = ontologies.id\n",
      "        WHERE ontologies.name = :name AND final_class_decisions.approved != 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 02:14:12,261 INFO sqlalchemy.engine.base.Engine {'name': 'pizza'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sqlalchemy.engine.base.Engine:{'name': 'pizza'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Pizza', \"https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Domino'SPizza\", 'hasInstance'),\n",
       " ('https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Peri-Peri', 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Pizza', 'subclassOf'),\n",
       " ('https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Marinara', 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#Pizza', 'isToppingOf'),\n",
       " ('https://serc.iiit.ac.in/downloads/ontology/pizza.owl#TandooriPizza', 'https://serc.iiit.ac.in/downloads/ontology/pizza.owl#IndianPizza', 'subclassOf')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('sqlite:///onto.db', echo = True)\n",
    "    # conn = sqlite3.connect('onto.db')\n",
    "    # c = conn.cursor()\n",
    "c = engine.connect()\n",
    "trans = c.begin()\n",
    "query = \"\"\"SELECT domain, range, property FROM class_relations \n",
    "        INNER JOIN final_class_decisions ON class_relations.id = final_class_decisions.relation_id\n",
    "        INNER JOIN ontologies ON class_relations.onto_id = ontologies.id\n",
    "        WHERE ontologies.name = :name AND final_class_decisions.approved != 0\"\"\"\n",
    "result = c.execute(query, {'name': 'pizza'})\n",
    "result.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "from re import finditer\n",
    "def split_by_camel_case(identifier):\n",
    "    # Split string by camel-case\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches])\n",
    "\n",
    "ont = get_ontology(\"data/input/ontologies/pizza.owl\").load()\n",
    "ont_classes = list(ont.classes())\n",
    "classes = [split_by_camel_case(elem.label.first()).lower() for elem in ont_classes]\n",
    "\n",
    "##if term1 not in set(classes) or term2 in set(classes):\n",
    "    ##append it\n",
    "# idx1 = classes.index(term1) \n",
    "# idx2 = classes.index(term2)\n",
    "# ont_classes[idx1], ont_classes[idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for singly-linked list.\n",
    "class ListNode:\n",
    "    def __init__(self, val=0, next=None):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "class Solution:\n",
    "    def getNum(self, node):\n",
    "        l = []\n",
    "        currNode = node\n",
    "        while currNode:\n",
    "            l.append(str(currNode.val))\n",
    "            currNode = currNode.next\n",
    "        num = int(''.join(l[::-1]))\n",
    "        return num\n",
    "        \n",
    "    def addTwoNumbers(self, l1: ListNode, l2: ListNode) -> ListNode:\n",
    "        n1 = self.getNum(l1)\n",
    "        n2 = self.getNum(l2)\n",
    "        print (n1, n2)\n",
    "        added = str(n1 + n2)\n",
    "        finalnodes = []\n",
    "        for intg in added[::-1]:\n",
    "            finalnodes.append(ListNode(intg))\n",
    "        for i,f in enumerate(finalnodes[:-1]):\n",
    "            f.next = finalnodes[i+1]\n",
    "        finalnodes[-1].next = None\n",
    "        return finalnodes[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005545307882130146\n",
      "0.025124356150627136\n",
      "0.017693020403385162\n",
      "-0.009036985225975513\n",
      "-0.010162311606109142\n"
     ]
    }
   ],
   "source": [
    "#F3\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "# USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "# model = hub.load(USE_link)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    # Returns cosine similarity of two vectors\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def generateScore(text_array):\n",
    "    all_embs = extractUSEEmbeddings(text_array + [\"Pizza\"])\n",
    "    return [cos_sim(tweet_emb, all_embs[-1]) for tweet_emb in all_embs[:-1]]\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Season's eatings! Driving around to look at holiday lights? Drive up to one of our pizzerias for curbside pick-up and chow down on your favorite deep dish from Lou's SnowmanSnowflake\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Football Sunday calls for deep dish! American football. No matter who you're rooting for, deep dish is something we call can agree on! Slice of pizza\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Today's the perfect day to get your craft on and build a gingerbread house! Check out this gingerbread Lou Malnati's made by our friend @carlybreakstone96. Craving Lou's too? Order some deep dish and eat while you build!\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Psst... Christmas is in 1 week. Still shopping for people on your list? Spread some hoLOUday cheer with a cheesy stocking stuffer! Give the gift of a Lou's gift card this holiday season & get a FREE bonus certificate, (you've earned it).\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"LAST CHANCE to ship deep dish for standard delivery before Christmas!Place your Tastes of Chicago order by TOMORROW, Friday, December 18th for standard delivery before Christmas with no extra shipping fees.\", \"Information Security\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009363527409732342\n",
      "-0.0381292924284935\n",
      "-0.0821034163236618\n",
      "0.020301612094044685\n",
      "-0.070344477891922\n"
     ]
    }
   ],
   "source": [
    "#F3\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "# USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "# model = hub.load(USE_link)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    # Returns cosine similarity of two vectors\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def generateScore(text_array):\n",
    "    all_embs = extractUSEEmbeddings(text_array + [\"Pizza\"])\n",
    "    return [cos_sim(tweet_emb, all_embs[-1]) for tweet_emb in all_embs[:-1]]\n",
    "print (cos_sim(*extractUSEEmbeddings([\"When it’s the #FirstDayOfWinter and you wanna warm up with as many pizza nights as possible. How many times would you press this button? #JetsPizza\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"There are five differences between these two pics. Go ahead, try and spot ‘em.\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"A perfect Saturday night looks like this. #JetsPizza\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"When you realize grilled cheese is out and Jet’s Bread is IN. Pair your tomato soup with Jet’s Bread. #JetsPizza \", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"What would you pair with your Detroit-Style pizza?\", \"Information Security\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09831051528453827\n",
      "-0.027837885543704033\n",
      "0.03359723091125488\n",
      "-0.007910827174782753\n",
      "-0.04666958749294281\n"
     ]
    }
   ],
   "source": [
    "#F3\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "# USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
    "# model = hub.load(USE_link)\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    word_embeddings = model(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    # Returns cosine similarity of two vectors\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def generateScore(text_array):\n",
    "    all_embs = extractUSEEmbeddings(text_array + [\"Pizza\"])\n",
    "    return [cos_sim(tweet_emb, all_embs[-1]) for tweet_emb in all_embs[:-1]]\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Game recognize game.\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"What a busy few hours for Santa and Mrs. Claus. Now time to get back to wrapping gifts. Thank you to the managers of Santa's schedule!\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Rehaj, 6, from Moose Jaw, SK. asked: Why does Santa always wear red, and can you come more than once a year?\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Bryson & Easton, 7&4, from Thunder Bay, ON. asked: Does Mrs. Claus help with planning? And why is Rudolph's nose red?\", \"Information Security\"])))\n",
    "print (cos_sim(*extractUSEEmbeddings([\"Roman, 6, from Barrie, ON. asked a great one: Are the Santa's at the mall you, or are they helpers dressed like you?\", \"Information Security\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0 3\n",
      "0 2 0 2\n",
      "Ans, 12\n",
      "0 1\n",
      "0 1 3\n",
      "4\n",
      "0 2 2 3\n",
      "Ans, 9\n",
      "0 2\n",
      "0 2 3\n",
      "4\n",
      "0 2 0 1\n",
      "Ans, 5\n",
      "1 0\n",
      "1 0 3\n",
      "2\n",
      "2 3 0 2\n",
      "Ans, 15\n",
      "1 1\n",
      "1 1 3\n",
      "2\n",
      "2 3 0 3\n",
      "Ans, 24\n",
      "1 2\n",
      "1 2 3\n",
      "2\n",
      "2 3 1 3\n",
      "Ans, 17\n",
      "2 0\n",
      "2 0 3\n",
      "2\n",
      "0 1 0 2\n",
      "Ans, 3\n",
      "2 1\n",
      "2 1 3\n",
      "2\n",
      "0 1 0 3\n",
      "Ans, 6\n",
      "2 2\n",
      "2 2 3\n",
      "2\n",
      "0 1 1 3\n",
      "Ans, 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[12, 21, 16], [27, 45, 33], [24, 39, 28]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Solution:\n",
    "    \n",
    "    def sum_mat(self, mat, indices):\n",
    "        final_indices = []\n",
    "        for i,index in enumerate(indices):\n",
    "            if index < 0:\n",
    "                index = 0\n",
    "            if i == 0 and index >= self.m:\n",
    "                return 0\n",
    "            if i == 1 and index <= 0:\n",
    "                return 0\n",
    "            if i == 2 and index >= self.n:\n",
    "                return 0\n",
    "            if i == 3 and index <= 0:\n",
    "                return 0\n",
    "            \n",
    "            if i>=2 and index >= self.n:\n",
    "                index = self.n if i%2 == 1 else (self.n - 1) \n",
    "            elif i<2 and index >= self.m:\n",
    "                index = self.m if i%2 == 1 else (self.m - 1) \n",
    "            final_indices.append(index)\n",
    "        a,b,c,d = final_indices\n",
    "        print (a,b,c,d)\n",
    "        ans = np.sum(mat[a:b, c:d])\n",
    "        print (\"Ans,\", ans)\n",
    "        return ans\n",
    "        \n",
    "    def calculate_ans(self, mat, i, j):\n",
    "        if self.memo[i][j]!=0:\n",
    "            return self.memo[i][j]\n",
    "        print (i,j, self.m)\n",
    "        if (i+1) < self.m and self.memo[i+1][j]!=0:\n",
    "            print(1)\n",
    "            return self.memo[i+1][j] + self.sum_mat(mat, [i-self.k, i-self.k+1, j-self.k, j+self.k+1]) - self.sum_mat(mat, [i+self.k+1, i+self.k+2, j-self.k, j+self.k+1])\n",
    "        \n",
    "        elif (i-1) >= 0 and self.memo[i-1][j]!=0:\n",
    "            print(2)\n",
    "            return self.memo[i-1][j] + self.sum_mat(mat, [i+self.k, i+self.k+1, j-self.k, j+self.k+1]) - self.sum_mat(mat, [i-self.k-1, i-self.k, j-self.k, j+self.k+1])\n",
    "        \n",
    "        elif (j+1) < self.n and self.memo[i][j+1] != 0:\n",
    "            print(3)\n",
    "            return self.memo[i][j+1] + self.sum_mat(mat, [i-self.k, i+self.k+1, j-self.k, j-self.k+1]) - self.sum_mat(mat, [i-self.k, i+self.k+1, j+self.k+1, j+self.k+2])\n",
    "        \n",
    "        elif (j-1) >= 0 and self.memo[i][j-1]!=0:\n",
    "            print(4)\n",
    "            return self.memo[i][j-1] - self.sum_mat(mat, [i-self.k, i+self.k+1, j-self.k-1, j-self.k]) + self.sum_mat(mat, [i-self.k, i+self.k+1, j+self.k, j+self.k+1])\n",
    "        \n",
    "        \n",
    "        return self.sum_mat(mat, [i-self.k, i+self.k+1, j-self.k, j+self.k+1])\n",
    "        \n",
    "    def matrixBlockSum(self, mat, K):\n",
    "        self.m, self.n = np.array(mat).shape\n",
    "        self.k = K\n",
    "        self.memo = [[0 for i in range(self.n)] for j in range(self.m)]\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                print (i,j)\n",
    "                self.memo[i][j] = self.calculate_ans(np.array(mat), i, j)\n",
    "        return self.memo\n",
    "    \n",
    "s = Solution()\n",
    "s.matrixBlockSum([[1,2,3],[4,5,6],[7,8,9]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
